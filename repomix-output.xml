This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where comments have been removed, content has been compressed (code blocks are separated by â‹®---- delimiter).

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Content has been compressed - code blocks are separated by â‹®---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
common/
  domain/
    __init__.py
  factories/
    __init__.py
    adapter_factory.py
    preprocessor_factory.py
  utils/
    __init__.py
    date_utils.py
    logging_wrappers.py
  __init__.py
  README.md
config/
  __init__.py
  state.py
examples/
  __init__.py
  ccxt_binance_demo.py
  ccxt_bybit_demo.py
  ccxt_gateio_demo.py
  ccxt_huobi_demo.py
infrastructure/
  checkpoint/
    __init__.py
    coordinator.py
    path_builder.py
    store.py
  config/
    __init__.py
    settings.py
  database/
    __init__.py
    ports.py
  impls/
    __init__.py
    system.py
  minio/
    init_bronze.sh
  observability/
    __init__.py
    logging.py
  ports/
    __init__.py
    system.py
  __init__.py
ingestion/
  adapters/
    ccxt_plugin/
      __init__.py
      builders.py
      client_factory.py
      ohlcv_adapter.py
      open_interest_adapter.py
    coinalyze_plugin/
      __init__.py
      backfill_adapter.py
      base.py
      client.py
      dependency_container.py
      error_handlers.py
      error_mapper.py
      exceptions.py
      formatters.py
      impls.py
      key_rotator.py
      mappers.py
      ohlcv_adapter.py
      open_interest_adapter.py
      response_validator.py
      retry_handler.py
      strategies.py
      symbol_registry.py
    coinmetrics_plugin/
      __init__.py
      base.py
      client.py
      onchain_adapter.py
    __init__.py
    base.py
  backfill/
    __init__.py
    checkpoint_manager.py
    chunk_generator.py
    chunk_processor.py
    coordinator.py
    dependency_container.py
    rate_limiter.py
    reporter.py
  config/
    value_objects.py
  connectors/
    __init__.py
    aiohttp_client.py
    rest.py
  factories/
    __init__.py
    adapter_factory.py
    preprocessor_factory.py
  models/
    __init__.py
    enums.py
  orchestration/
    backfill/
      __init__.py
      checkpoint_manager.py
      chunk_generator.py
      chunk_processor.py
      coordinator.py
      dependency_container.py
      rate_limiter.py
      reporter.py
    data_fetcher.py
  pipelines/
    __init__.py
  ports/
    __init__.py
    backfill.py
    data_ports.py
    http.py
    validators.py
  preprocessing/
    base.py
    providers.py
  symbol_resolution/
    resolver.py
  __init__.py
  dependency_container.py
  service.py
legacy/
  asset_coverage.py
  ccxt_exchanges.py
  coinalyze.py
  database.py
  enums.py
  orchestrator.py
orchestration/
  operators/
    __init__.py
    base.py
    checkpoint_operators.py
    fetch_operators.py
    write_operators.py
  workflows/
    __init__.py
    backfill_workflow.py
    base.py
    incremental_workflow.py
  __init__.py
  ports.py
  README.md
shared/
  enums/
    __init__.py
  models/
    __init__.py
    enums.py
    instruments.py
  __init__.py
storage/
  adapters/
    __init__.py
    postgres.py
  migrations/
    README.md
  repositories/
    __init__.py
    checkpoint.py
    metadata.py
    ohlcv.py
    open_interest.py
  schemas/
    __init__.py
    relational.py
    time_series.py
  __init__.py
transformation/
  adapters/
    __init__.py
    ccxt.py
    coinalyze.py
    coinmetrics.py
  normalizers/
    __init__.py
  pipelines/
    __init__.py
    base.py
    ohlcv.py
    open_interest.py
    trades.py
  validators/
    __init__.py
  __init__.py
  normalizers.py
  ports.py
  validators.py
__init__.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="common/domain/__init__.py">
__all__ = [
</file>

<file path="common/factories/__init__.py">
__all__ = [
</file>

<file path="common/factories/adapter_factory.py">
AdapterBuilder = Callable[..., BaseAdapter]
â‹®----
class AdapterFactory
â‹®----
def __init__(self) -> None
â‹®----
def register(self, provider: DataProvider, builder: AdapterBuilder) -> None
â‹®----
def create(self, provider: DataProvider, *args: Any, **kwargs: Any) -> BaseAdapter
â‹®----
def available_providers(self) -> list[DataProvider]
</file>

<file path="common/factories/preprocessor_factory.py">
PreprocessorBuilder = Callable[..., DataPreprocessor]
â‹®----
class PreprocessorFactory
â‹®----
def __init__(self) -> None
â‹®----
def register(self, provider: DataProvider, builder: PreprocessorBuilder) -> None
â‹®----
def available_providers(self) -> list[DataProvider]
</file>

<file path="common/utils/__init__.py">
__all__ = [
</file>

<file path="common/utils/date_utils.py">
def to_unix_ms(dt: datetime) -> int
â‹®----
dt = dt.replace(tzinfo=UTC)
â‹®----
def from_unix_ms(timestamp_ms: int) -> datetime
â‹®----
def utc_now() -> datetime
â‹®----
def utc_today() -> datetime
â‹®----
now = utc_now()
â‹®----
def truncate_to_day(dt: datetime) -> datetime
â‹®----
def truncate_to_hour(dt: datetime) -> datetime
â‹®----
def add_days(dt: datetime, days: int) -> datetime
â‹®----
def add_hours(dt: datetime, hours: int) -> datetime
â‹®----
def date_range(start: datetime, end: datetime, days: int = 1) -> list[datetime]
â‹®----
result = []
current = truncate_to_day(start)
end = truncate_to_day(end)
â‹®----
current = add_days(current, days)
</file>

<file path="common/utils/logging_wrappers.py">
def get_logger(name: str, level: int = logging.INFO) -> logging.Logger
â‹®----
logger = logging.getLogger(name)
â‹®----
handler = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter(
â‹®----
root_logger = logging.getLogger(name)
â‹®----
console_handler = logging.StreamHandler(sys.stdout)
â‹®----
file_handler = logging.FileHandler(log_file)
</file>

<file path="common/__init__.py">
__all__ = [
â‹®----
__version__ = "1.0.0"
</file>

<file path="common/README.md">
# Common Layer - Shared Utilities and Factories

## Overview

The common layer provides **reusable components** used across multiple layers (ingestion, storage, transformation, orchestration):

- **factories/**: Adapter and preprocessor factories
- **domain/**: Re-exported shared models and enums
- **utils/**: Utility functions for dates, logging, etc.

## Directory Structure

```
common/
â”œâ”€â”€ __init__.py              # Layer exports
â”œâ”€â”€ README.md                # This file
â”‚
â”œâ”€â”€ factories/               # Moved from ingestion/factories/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ adapter_factory.py   # AdapterFactory: creates adapters
â”‚   â””â”€â”€ preprocessor_factory.py  # PreprocessorFactory: creates preprocessors
â”‚
â”œâ”€â”€ domain/                  # Re-exports from shared/
â”‚   â””â”€â”€ __init__.py          # Exports models and enums
â”‚
â””â”€â”€ utils/                   # Shared utility functions
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ date_utils.py        # Unix timestamp, timezone handling
    â””â”€â”€ logging_wrappers.py  # Logger setup and configuration
```

## Module Purposes

### factories/

**AdapterFactory** - Registry-driven factory for creating data adapters:

```python
from quant_framework.common.factories import AdapterFactory
from quant_framework.ingestion.models.enums import DataProvider

factory = AdapterFactory()
factory.register(DataProvider.CCXT, CCXTAdapter)

adapter = factory.create(DataProvider.CCXT, **kwargs)
```

**PreprocessorFactory** - Registry-driven factory for creating preprocessors:

```python
from quant_framework.common.factories import PreprocessorFactory

factory = PreprocessorFactory()
factory.register(DataProvider.COINALYZE, CoinalyzePreprocessor)

preprocessor = factory.create(DataProvider.COINALYZE)
```

### domain/

Re-exports core domain models:

```python
from quant_framework.common.domain import (
    Instrument,
    MarketType,
    AssetClass,
    DataVenue,
)
```

### utils/

**date_utils.py** - Timestamp and timezone utilities:

```python
from quant_framework.common.utils import (
    to_unix_ms,        # datetime â†’ Unix ms
    from_unix_ms,      # Unix ms â†’ datetime
    utc_now,           # Current UTC time
    utc_today,         # Today at midnight UTC
    truncate_to_day,   # Truncate to start of day
    add_days,          # Add days to datetime
    date_range,        # Generate date sequence
)
```

**logging_wrappers.py** - Logging setup utilities:

```python
from quant_framework.common.utils import (
    get_logger,       # Get configured logger
    setup_logging,    # Initialize application logging
)

logger = get_logger(__name__)
```

## Migration Notes

### Factories

**Old imports (deprecated but still work):**
```python
from quant_framework.ingestion.factories import AdapterFactory
```

**New imports (preferred):**
```python
from quant_framework.common.factories import AdapterFactory
```

The old location still works via backward compatibility shims with deprecation warnings.

### Domain

No migration needed - domain/ is new and complements shared/:

```python
# These are equivalent
from quant_framework.shared.models.enums import Instrument
from quant_framework.common.domain import Instrument
```

Use whichever is more convenient.

## Design Principles

1. **Single Responsibility**: Each module has one clear purpose
2. **No Circular Dependencies**: common/ only imports from shared/ and ingestion/models/
3. **Backward Compatibility**: Old imports still work with deprecation warnings
4. **Type Safety**: 100% type-hinted functions
5. **Async-Ready**: All utilities support async operations

## Testing

All utilities are tested in `tests/common/`:

```bash
pytest tests/common/factories/  # Factory tests
pytest tests/common/utils/      # Utility tests
```

## Future Additions

Potential utilities to add:
- `validation/`: Data validation helpers
- `constants/`: Shared constants
- `exceptions/`: Domain exceptions
- `decorators/`: Common decorators (retry, cache, etc.)

---

**Phase 6**: Orchestration Layer Cleanup - Common utilities consolidated here.
</file>

<file path="config/state.py">
logger = logging.getLogger(__name__)
â‹®----
class AssetConfig(BaseModel)
â‹®----
full_universe: list[str] = Field(default_factory=lambda: ["BTC", "ETH"])
timeframes: list[str] = Field(
â‹®----
class Config
â‹®----
extra = "allow"
â‹®----
class AssetsConfig(BaseModel)
â‹®----
crypto: AssetConfig = Field(default_factory=AssetConfig)
â‹®----
class DatabaseConfig(BaseModel)
â‹®----
url: str = Field(default="postgresql://quant_app:password@timescaledb:5432/quant")
pool_size: int = Field(default=20, ge=1, le=500)
max_overflow: int = Field(default=10, ge=0, le=200)
pool_timeout: int = Field(default=30, ge=1, le=300)
statement_timeout: int = Field(default=30000, ge=1000)
batch_size: int = Field(default=500, ge=1)
â‹®----
@field_validator("url")
@classmethod
    def validate_url(cls, v: str) -> str
â‹®----
class CoinalyzeConfig(BaseModel)
â‹®----
base_url: str = Field(default="https://api.coinalyze.net/v1")
api_keys: list[str] = Field(default_factory=list)
rate_limit: int = Field(default=60, ge=1)
request_interval: float = Field(default=1.0, ge=0.1)
â‹®----
class RedisConfig(BaseModel)
â‹®----
redis_url: str = Field(default="redis://redis:6379/0")
â‹®----
class EndpointSupportConfig(BaseModel)
â‹®----
ohlc: bool = Field(default=True)
open_interest: bool = Field(default=True)
funding_rate: bool = Field(default=False)
liquidation: bool = Field(default=False)
â‹®----
class BatchLimitConfig(BaseModel)
â‹®----
ohlc: int = Field(default=1000, ge=1)
open_interest: int = Field(default=500, ge=1)
â‹®----
class ValidationConfig(BaseModel)
â‹®----
require_open_interest: bool = Field(default=False)
require_funding_rate: bool = Field(default=False)
price_precision: int = Field(default=8, ge=0)
volume_precision: int = Field(default=8, ge=0)
â‹®----
class MarketTypeConfig(BaseModel)
â‹®----
market_type: str
settlement_currency: str | None = Field(default=None)
instrument_type: str = Field(default="spot")
margin_type: str | None = Field(default=None)
contract_size: float = Field(default=1.0)
â‹®----
supported_data_types: list[str] = Field(default_factory=list)
validation: ValidationConfig = Field(default_factory=ValidationConfig)
endpoint_support: EndpointSupportConfig = Field(
â‹®----
class ExchangeMarketConfig(BaseModel)
â‹®----
exchange: str
â‹®----
api_base_url: str
ws_base_url: str | None = Field(default=None)
ccxt_class: str
â‹®----
rest_rate_limit: int = Field(default=1200, ge=1)
batch_limits: BatchLimitConfig = Field(default_factory=BatchLimitConfig)
supported_timeframes: list[str] = Field(
supported_oi_timeframes: list[str] | None = Field(default_factory=list)
â‹®----
retry_max: int = Field(default=5, ge=1, le=50)
retry_delay: float = Field(default=2.0, ge=0.1)
â‹®----
ws_max_connections: int | None = Field(default=None)
ws_max_streams_per_connection: int | None = Field(default=None)
â‹®----
class LoggingConfig(BaseModel)
â‹®----
level: str = Field(default="INFO")
format: str = Field(default="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
â‹®----
class ConfigState(BaseModel)
â‹®----
assets: AssetsConfig = Field(default_factory=AssetsConfig)
database: DatabaseConfig = Field(default_factory=DatabaseConfig)
coinalyze: CoinalyzeConfig = Field(default_factory=CoinalyzeConfig)
redis: RedisConfig = Field(default_factory=RedisConfig)
logging: LoggingConfig = Field(default_factory=LoggingConfig)
â‹®----
exchanges: dict[str, ExchangeMarketConfig] = Field(default_factory=dict)
â‹®----
env: str = Field(default="dev")
config_dir: str = Field(default="/app/config")
â‹®----
class ConfigLoader
â‹®----
def __init__(self, config_dir: str = "/app/config")
â‹®----
def _load_yaml(self, path: Path) -> dict[str, Any]
â‹®----
data = yaml.safe_load(f) or {}
â‹®----
def _apply_env_overrides(self, config: dict[str, Any]) -> dict[str, Any]
â‹®----
"""Apply environment variable overrides to config."""
# Database URL
â‹®----
api_keys_str = os.getenv("COINALYZE_API_KEYS") or os.getenv("COINALYZE_API_KEY")
â‹®----
api_keys = [key.strip() for key in api_keys_str.split(",") if key.strip()]
â‹®----
def _merge_dicts(self, base: dict, override: dict) -> dict
â‹®----
result = base.copy()
â‹®----
def load(self) -> ConfigState
â‹®----
# Load config files in order of specificity
config: dict[str, Any] = {}
â‹®----
# 1. Load top-level YAML files
â‹®----
file_config = self._load_yaml(self.config_dir / config_file)
config = self._merge_dicts(config, file_config)
â‹®----
env_config = self._load_yaml(self.config_dir / "env" / f"{self.env}.yaml")
config = self._merge_dicts(config, env_config)
â‹®----
config = self._apply_env_overrides(config)
â‹®----
state = ConfigState(env=self.env, config_dir=str(self.config_dir), **config)
â‹®----
"""
        Load configuration for specific exchange and market type.

        Merges:
          1. Global CCXT defaults
          2. Market type template
          3. Exchange-specific config
          4. Exchange+market type specific config
        """
exchange_lower = exchange.lower()
market_type_lower = market_type.lower()
â‹®----
result_config: dict[str, Any] = {}
â‹®----
# 1. Global defaults
â‹®----
result_config = defaults.model_dump(exclude_unset=True)
â‹®----
# 2. Market type template
market_type_config = self._load_yaml(
â‹®----
result_config = self._merge_dicts(result_config, market_type_config)
â‹®----
exchange_config_path = (
â‹®----
exchange_file_config = self._load_yaml(exchange_config_path)
â‹®----
market_config = exchange_file_config["market_types"].get(
â‹®----
base_exchange_config = {
result_config = self._merge_dicts(result_config, base_exchange_config)
result_config = self._merge_dicts(result_config, market_config)
â‹®----
result_config = self._merge_dicts(result_config, exchange_file_config)
â‹®----
# =============================================================================
# GLOBAL INSTANCE
â‹®----
def get_config(config_dir: str | None = None) -> ConfigState
â‹®----
"""
    Load and return the global configuration state.

    Args:
        config_dir: Override config directory. Defaults to /app/config or ./config

    Returns:
        ConfigState: Validated configuration object
    """
â‹®----
config_dir = os.getenv("MNEMO_CONFIG_DIR", "/app/config")
â‹®----
config_dir = "./config"
â‹®----
loader = ConfigLoader(config_dir=config_dir)
â‹®----
__all__ = [
</file>

<file path="examples/__init__.py">

</file>

<file path="examples/ccxt_binance_demo.py">
logger = logging.getLogger(__name__)
â‹®----
async def demo_binance_usdm_ohlcv()
â‹®----
instrument = Instrument(
â‹®----
# Fetch OHLCV
start = datetime.now(UTC) - timedelta(hours=2)
end = datetime.now(UTC)
â‹®----
raw_ohlcv = await ohlcv_adapter.fetch_ohlcv(
â‹®----
preprocessor = BinancePreprocessor()
preprocessed = list(preprocessor.preprocess_ohlcv(instrument, raw_ohlcv))
â‹®----
async def demo_binance_spot()
â‹®----
async def demo_binance_coinm_open_interest()
â‹®----
# Fetch Open Interest
â‹®----
raw_oi = await oi_adapter.fetch_open_interest(
â‹®----
preprocessed = list(preprocessor.preprocess_open_interest(instrument, raw_oi))
â‹®----
async def main()
</file>

<file path="examples/ccxt_bybit_demo.py">
logger = logging.getLogger(__name__)
â‹®----
async def demo_bybit_linear_ohlcv()
â‹®----
instrument = Instrument(
â‹®----
# Fetch OHLCV
start = datetime.now(UTC) - timedelta(hours=2)
end = datetime.now(UTC)
â‹®----
raw_ohlcv = await ohlcv_adapter.fetch_ohlcv(
â‹®----
preprocessor = BybitPreprocessor()
preprocessed = list(preprocessor.preprocess_ohlcv(instrument, raw_ohlcv))
â‹®----
async def demo_bybit_open_interest()
â‹®----
# Fetch Open Interest
â‹®----
raw_oi = await oi_adapter.fetch_open_interest(
â‹®----
preprocessed = list(preprocessor.preprocess_open_interest(instrument, raw_oi))
â‹®----
async def demo_bybit_inverse()
â‹®----
async def main()
</file>

<file path="examples/ccxt_gateio_demo.py">
logger = logging.getLogger(__name__)
â‹®----
async def demo_gateio_linear_ohlcv()
â‹®----
instrument = Instrument(
â‹®----
# Fetch OHLCV
start = datetime.now(UTC) - timedelta(hours=2)
end = datetime.now(UTC)
â‹®----
raw_ohlcv = await ohlcv_adapter.fetch_ohlcv(
â‹®----
preprocessor = GateIOPreprocessor()
preprocessed = list(preprocessor.preprocess_ohlcv(instrument, raw_ohlcv))
â‹®----
async def demo_gateio_open_interest()
â‹®----
# Fetch Open Interest
â‹®----
raw_oi = await oi_adapter.fetch_open_interest(
â‹®----
preprocessed = list(preprocessor.preprocess_open_interest(instrument, raw_oi))
â‹®----
async def demo_gateio_spot()
â‹®----
async def main()
</file>

<file path="examples/ccxt_huobi_demo.py">
logger = logging.getLogger(__name__)
â‹®----
async def demo_huobi_ohlcv()
â‹®----
instrument = Instrument(
â‹®----
# Fetch OHLCV
start = datetime.now(UTC) - timedelta(hours=2)
end = datetime.now(UTC)
â‹®----
raw_ohlcv = await ohlcv_adapter.fetch_ohlcv(
â‹®----
preprocessor = HuobiPreprocessor()
preprocessed = list(preprocessor.preprocess_ohlcv(instrument, raw_ohlcv))
â‹®----
async def demo_huobi_open_interest()
â‹®----
# Fetch Open Interest
â‹®----
raw_oi = await oi_adapter.fetch_open_interest(
â‹®----
preprocessed = list(preprocessor.preprocess_open_interest(instrument, raw_oi))
â‹®----
async def main()
</file>

<file path="infrastructure/checkpoint/__init__.py">
__all__ = [
</file>

<file path="infrastructure/checkpoint/coordinator.py">
@dataclass
class GapStatus
â‹®----
lag_ms: int
warn: bool
alert: bool
message: str
â‹®----
class CheckpointCoordinator
â‹®----
key = CheckpointPathBuilder.coinalyze_backfill("oi", symbol, timeframe)
â‹®----
key = CheckpointPathBuilder.ccxt_incremental(
â‹®----
coinalyze = self.load_coinalyze_checkpoint(canonical_symbol, timeframe)
â‹®----
start_ts = max(0, coinalyze.last_successful_timestamp - self.overlap_ms)
now_iso = datetime.now(UTC).isoformat()
â‹®----
bootstrap = CheckpointDocument(
â‹®----
doc = existing or CheckpointDocument(
â‹®----
new_etag = self.store.write_atomic(key, doc, if_match=etag)
â‹®----
lag = (
warn = lag > warn_threshold_ms
alert = lag > alert_threshold_ms
message = "ok"
â‹®----
message = "ccxt more than 4h behind coinalyze"
â‹®----
message = "ccxt more than 2h behind coinalyze"
</file>

<file path="infrastructure/checkpoint/path_builder.py">
class CheckpointPathBuilder
â‹®----
CHECKPOINTS_ROOT = "checkpoints"
DATA_ROOT = "data/open_interest"
â‹®----
@classmethod
    def coinalyze_backfill(cls, data_type: str, symbol: str, timeframe: str) -> str
</file>

<file path="infrastructure/checkpoint/store.py">
@dataclass
class CheckpointDocument
â‹®----
source: str
exchange: str
symbol: str
timeframe: str
market_type: str | None
last_successful_timestamp: int
last_updated: str
total_records: int
metadata: dict[str, Any] = field(default_factory=dict)
â‹®----
def to_dict(self) -> dict[str, Any]
â‹®----
doc = asdict(self)
â‹®----
@classmethod
    def from_dict(cls, raw: dict[str, Any]) -> CheckpointDocument
â‹®----
class CheckpointStore
â‹®----
env_endpoint = os.getenv("MINIO_ENDPOINT", "http://minio:9000")
â‹®----
use_ssl = secure if secure is not None else self.endpoint.startswith("https")
â‹®----
def exists(self, key: str) -> bool
â‹®----
code = e.response.get("Error", {}).get("Code")
â‹®----
def read(self, key: str) -> tuple[CheckpointDocument | None, str | None]
â‹®----
path = self.local_root / key
â‹®----
raw = json.loads(path.read_text())
â‹®----
resp = self.s3.get_object(Bucket=self.bucket, Key=key)
body = resp["Body"].read().decode("utf-8")
etag = resp.get("ETag")
â‹®----
payload = json.dumps(document.to_dict(), ensure_ascii=True).encode("utf-8")
â‹®----
put_kwargs: dict[str, Any] = {
â‹®----
resp = self.s3.put_object(**put_kwargs)
â‹®----
def list(self, prefix: str) -> list[str]
â‹®----
base = self.local_root / prefix
â‹®----
resp = self.s3.list_objects_v2(Bucket=self.bucket, Prefix=prefix)
contents = resp.get("Contents", [])
</file>

<file path="infrastructure/config/__init__.py">
class EndpointType(str, Enum)
â‹®----
OHLC = "ohlc"
OPEN_INTEREST = "open_interest"
â‹®----
@dataclass(frozen=True)
class SymbolConfig
â‹®----
canonical: str
â‹®----
class EnhancedSymbolRegistry
â‹®----
def __init__(self, config_path: str | Path = "config/assets/crypto_universe.yaml")
â‹®----
alt = Path("/opt/airflow") / config_path
â‹®----
@staticmethod
    def _load_yaml(path: Path) -> dict[str, Any]
â‹®----
def get_active_symbols(self, exchange: str) -> list[SymbolConfig]
â‹®----
results: list[SymbolConfig] = []
â‹®----
seen: set[str] = set()
deduped: list[SymbolConfig] = []
â‹®----
"""Resolve the concrete exchange symbol for a canonical asset."""
â‹®----
sym_map = ex.get("symbols", {})
â‹®----
settlements: list[str] = []
â‹®----
cur = ex.get("settlement_currency")
â‹®----
class CCXTExchangeMapper
â‹®----
def map(self, exchange: str) -> str
â‹®----
__all__ = [
</file>

<file path="infrastructure/config/settings.py">
__all__ = ["settings", "config_loader", "config_registry"]
</file>

<file path="infrastructure/database/__init__.py">

</file>

<file path="infrastructure/database/ports.py">
class IDatabaseAdapter(Protocol)
â‹®----
async def connect(self) -> None
â‹®----
async def disconnect(self) -> None
â‹®----
async def fetch_one(self, query: str, *args: Any) -> dict[str, Any] | None
â‹®----
async def fetch_all(self, query: str, *args: Any) -> list[dict[str, Any]]
â‹®----
class DatabaseAdapter
â‹®----
def __init__(self, database: "Database")
â‹®----
result = await conn.fetchrow(query, *args)
â‹®----
results = await conn.fetch(query, *args)
â‹®----
result = await self.execute_query(query, *args, fetch_all=True)
â‹®----
@property
    def pool(self)
</file>

<file path="infrastructure/impls/__init__.py">
__all__ = [
</file>

<file path="infrastructure/impls/system.py">
class SystemClock(IClock)
â‹®----
def now(self) -> datetime
â‹®----
def utcnow(self) -> datetime
â‹®----
class OsFileSystem(IFileSystem)
â‹®----
def read_text(self, path: Path) -> str
â‹®----
def write_text(self, path: Path, content: str) -> None
â‹®----
def exists(self, path: Path) -> bool
â‹®----
def mkdir(self, path: Path, parents: bool = True) -> None
â‹®----
class ProjectConfigPathResolver(IConfigPathResolver)
â‹®----
def __init__(self, project_root: Path | None = None)
â‹®----
current_file = Path(__file__)
project_root = current_file.parent.parent.parent.parent.parent.parent
â‹®----
def resolve_project_root(self) -> Path
â‹®----
def resolve_config_file(self, relative_path: str) -> Path
â‹®----
def resolve_env_config_file(self, env: str) -> Path
</file>

<file path="infrastructure/minio/init_bronze.sh">
set -euo pipefail

echo "Initializing Bronze Layer (MinIO)..."

ENDPOINT="${MINIO_ENDPOINT:-http://minio:9000}"
BUCKET="${MINIO_BUCKET_NAME:-bronze}"
MINIO_ROOT_USER="${MINIO_ROOT_USER:-admin}"
MINIO_ROOT_PASSWORD="${MINIO_ROOT_PASSWORD:-password123}"
MINIO_REGION="${MINIO_REGION:-us-east-1}"

echo "Using endpoint: $ENDPOINT"
echo "Using bucket: $BUCKET"


echo "Waiting for MinIO to be ready..."
for i in {1..30}; do
    if curl -s -f "${ENDPOINT}/minio/health/live" > /dev/null 2>&1; then
        echo "MinIO is ready"
        break
    fi
    echo "Attempt $i/10..."
    sleep 2
done


sleep 2


echo "Configuring MinIO client..."
if mc alias set local "${ENDPOINT}" "${MINIO_ROOT_USER}" "${MINIO_ROOT_PASSWORD}" > /dev/null 2>&1; then
    echo "MC client configured"
else
    echo "Could not configure mc client, trying with --insecure flag..."
    mc alias set local "${ENDPOINT}" "${MINIO_ROOT_USER}" "${MINIO_ROOT_PASSWORD}" --insecure
fi


echo "Creating bucket: ${BUCKET}..."
if mc ls "local/${BUCKET}" > /dev/null 2>&1; then
    echo "Bucket '${BUCKET}' already exists"
else
    if mc mb "local/${BUCKET}" --region="${MINIO_REGION}" > /dev/null 2>&1; then
        echo "Created bucket: ${BUCKET}"
    else
        echo "Could not create bucket, trying with --insecure flag..."
        mc mb "local/${BUCKET}" --region="${MINIO_REGION}" --insecure
        echo "Created bucket: ${BUCKET} (with --insecure)"
    fi
fi


echo "Creating folder structure..."
FOLDERS=(
    "source=coinalyze/data_type=ohlc"
    "source=coinalyze/data_type=open_interest"
    "source=ccxt/data_type=ohlc"
    "source=ccxt/data_type=open_interest"
)

for folder in "${FOLDERS[@]}"; do
    echo "  Creating: ${folder}/"
    if echo -n "" | mc pipe "local/${BUCKET}/${folder}/.keep" > /dev/null 2>&1; then
        echo "    Created"
    else
        echo "    Trying with --insecure flag..."
        echo -n "" | mc pipe "local/${BUCKET}/${folder}/.keep" --insecure > /dev/null 2>&1 || true
        echo "    Created (with --insecure)"
    fi
done


echo "Setting bucket policy..."
if mc policy set download "local/${BUCKET}" > /dev/null 2>&1; then
    echo "Bucket policy set"
else
    echo "Could not set policy, trying with --insecure flag..."
    mc policy set download "local/${BUCKET}" --insecure > /dev/null 2>&1 || true
    echo "Bucket policy set (with --insecure)"
fi

echo "Bronze layer initialization complete!"
echo "Bucket: ${BUCKET}"
echo "Console: http://localhost:9001"
echo "Endpoint: ${ENDPOINT}"
</file>

<file path="infrastructure/observability/__init__.py">
__all__ = [
</file>

<file path="infrastructure/observability/logging.py">
Layer = Literal[
â‹®----
def add_app_context(logger: Any, method_name: str, event_dict: EventDict) -> EventDict
â‹®----
level = event_dict.get("level")
â‹®----
severity_map = {
â‹®----
log_level = getattr(logging, level.upper(), logging.INFO)
â‹®----
processors = [
â‹®----
logger = structlog.get_logger(name)
â‹®----
context = {}
â‹®----
logger = logger.bind(**context)
â‹®----
ctx = {}
â‹®----
def get_database_logger(**context: Any) -> structlog.stdlib.BoundLogger
</file>

<file path="infrastructure/ports/__init__.py">
__all__ = [
</file>

<file path="infrastructure/ports/system.py">
class IClock(ABC)
â‹®----
@abstractmethod
    def now(self) -> datetime
â‹®----
@abstractmethod
    def utcnow(self) -> datetime
â‹®----
class IFileSystem(ABC)
â‹®----
@abstractmethod
    def read_text(self, path: Path) -> str
â‹®----
@abstractmethod
    def write_text(self, path: Path, content: str) -> None
â‹®----
@abstractmethod
    def exists(self, path: Path) -> bool
â‹®----
@abstractmethod
    def mkdir(self, path: Path, parents: bool = True) -> None
â‹®----
class IConfigPathResolver(ABC)
â‹®----
@abstractmethod
    def resolve_project_root(self) -> Path
â‹®----
@abstractmethod
    def resolve_config_file(self, relative_path: str) -> Path
â‹®----
@abstractmethod
    def resolve_env_config_file(self, env: str) -> Path
</file>

<file path="infrastructure/__init__.py">

</file>

<file path="ingestion/adapters/ccxt_plugin/__init__.py">
logger = logging.getLogger(__name__)
â‹®----
class CCXTAdapterBase(BaseAdapter)
â‹®----
client_type: ClientType = ClientType.WRAPPER
connection_type: ConnectionType = ConnectionType.REST
â‹®----
def __init__(self, client: Any, *args: Any, **kwargs: Any)
â‹®----
async def connect(self) -> None
â‹®----
load_markets = getattr(self.client, "load_markets", None)
â‹®----
async def close(self) -> None
â‹®----
close_fn = getattr(self.client, "close", None)
â‹®----
maybe_coro = close_fn()
</file>

<file path="ingestion/adapters/ccxt_plugin/builders.py">
logger = logging.getLogger(__name__)
â‹®----
config = config or {}
â‹®----
client = create_ccxt_client(
â‹®----
ohlcv_adapter = CCXTOHLCVAdapter(
â‹®----
oi_adapter = CCXTOpenInterestAdapter(
â‹®----
"""
    Convenience builder for Huobi/HTX adapters.

    Args:
        market_type: Market type (LINEAR_PERPETUAL, INVERSE_PERPETUAL, SPOT)
        api_key: API key
        api_secret: API secret
        testnet: Testnet mode
        config: Additional config

    Returns:
        Tuple of (ohlcv_adapter, open_interest_adapter)

    Example:
        >>> ohlcv, oi = create_huobi_adapters(MarketType.LINEAR_PERPETUAL)
    """
â‹®----
# Merge Huobi-specific defaults
huobi_defaults = {
â‹®----
merged_config = {**huobi_defaults, **config}
â‹®----
venue = DataVenue.BINANCE
â‹®----
venue = DataVenue.BINANCE_USDM
â‹®----
venue = DataVenue.BINANCE_COINM
â‹®----
binance_defaults = {
â‹®----
merged_config = {**binance_defaults, **config}
â‹®----
bybit_defaults = {
â‹®----
merged_config = {**bybit_defaults, **config}
â‹®----
gateio_defaults = {
â‹®----
merged_config = {**gateio_defaults, **config}
</file>

<file path="ingestion/adapters/ccxt_plugin/client_factory.py">
ccxt = None
â‹®----
options: dict[str, Any] = {
â‹®----
xopts: dict[str, Any] = {}
â‹®----
venue_to_ccxt = {
â‹®----
ccxt_class_name = venue_to_ccxt.get(venue)
â‹®----
exchange_class = getattr(ccxt, ccxt_class_name)
options = _build_options(
</file>

<file path="ingestion/adapters/ccxt_plugin/ohlcv_adapter.py">
class CCXTOHLCVAdapter(CCXTAdapterBase, OHLCVPort)
â‹®----
provider: DataProvider
supported_asset_classes = {AssetClass.CRYPTO}
capabilities = {OHLCVPort}
â‹®----
fetch = getattr(self.client, "fetch_ohlcv", None) or getattr(
â‹®----
since = int(start.timestamp() * 1000) if start else None
â‹®----
result = await asyncio.to_thread(
</file>

<file path="ingestion/adapters/ccxt_plugin/open_interest_adapter.py">
logger = logging.getLogger(__name__)
â‹®----
class CCXTOpenInterestAdapter(CCXTAdapterBase, OpenInterestPort)
â‹®----
capabilities = {OpenInterestPort}
â‹®----
fetch = getattr(self.client, "fetch_open_interest_history", None) or getattr(
â‹®----
since = int(start.timestamp() * 1000) if start else None
â‹®----
params: dict[str, Any] = {}
â‹®----
symbol = instrument.raw_symbol or instrument.symbol
result = await asyncio.to_thread(
</file>

<file path="ingestion/adapters/coinalyze_plugin/__init__.py">

</file>

<file path="ingestion/adapters/coinalyze_plugin/backfill_adapter.py">
logger = logging.getLogger(__name__)
â‹®----
class CoinalyzeBackfillAdapter
â‹®----
normalized_tf = self.rate_limiter.normalize_timeframe(timeframe)
â‹®----
# Fetch data using existing adapter
â‹®----
result = await self.ohlcv_adapter.fetch_ohlcv(
â‹®----
# Convert result to list if needed
â‹®----
data = result
â‹®----
data = list(result)
â‹®----
# Extract history from response
records = []
â‹®----
validation = self.rate_limiter.validate_chunk_response(
â‹®----
chunk_size_days = (end_date - start_date).days
â‹®----
"""
        Fetch Open Interest history with rate limit validation.

        Args:
            symbol: Trading symbol
            timeframe: Data timeframe
            start_date: Start of date range
            end_date: End of date range
            instrument: Instrument object with venue info

        Returns:
            List of OI records
        """
# Normalize timeframe for rate limiting
â‹®----
result = await self.oi_adapter.fetch_open_interest(
â‹®----
def supports_ohlcv(self) -> bool
â‹®----
"""Check if adapter supports OHLCV data."""
â‹®----
def supports_open_interest(self) -> bool
â‹®----
"""Check if adapter supports Open Interest data."""
â‹®----
def get_performance_summary(self) -> dict
â‹®----
"""
        Get rate limiter performance summary.

        Returns:
            Performance statistics dictionary
        """
</file>

<file path="ingestion/adapters/coinalyze_plugin/base.py">
class CoinalyzeAdapterBase(BaseAdapter)
â‹®----
venue: DataVenue
wrapper = WrapperImplementation.COINALYZE
â‹®----
client_type = ClientType.WRAPPER
connection_type = ConnectionType.REST
â‹®----
supported_asset_classes = {AssetClass.CRYPTO}
â‹®----
supports_multiple_venues = True
â‹®----
def __init__(self, client: CoinalyzeClient, *args, **kwargs)
â‹®----
async def connect(self) -> None
â‹®----
async def close(self) -> None
</file>

<file path="ingestion/adapters/coinalyze_plugin/client.py">
logger = logging.getLogger(__name__)
â‹®----
class CoinalyzeClient
â‹®----
async def fetch_data_async(self, endpoint: str, params: dict) -> dict
â‹®----
url = f"{self.base_url}/{endpoint}"
max_attempts = self.config.retry_config.max_attempts
â‹®----
# Get headers with API key from provider
headers = await self.api_key_provider.get_headers()
â‹®----
# Execute HTTP request via injected client
response = await self.http_client.get(
â‹®----
# Success response
â‹®----
# Validate response structure via injected validator
validation_result = self.response_validator.validate(
â‹®----
# Error response - check if retryable
â‹®----
# Non-retryable error - fail immediately
# Import here to avoid circular imports
â‹®----
chain = create_error_mapper_chain()
error = chain.map_error(
â‹®----
# Retryable error - check if we have attempts left
â‹®----
sleep_time = self.retry_handler.get_retry_delay(
â‹®----
# Network or other errors
â‹®----
# Should not reach here
</file>

<file path="ingestion/adapters/coinalyze_plugin/dependency_container.py">
class CoinalyzeDependencyContainer
â‹®----
def create_http_client(self) -> IHttpClient
â‹®----
def create_api_key_provider(self) -> IApiKeyProvider
â‹®----
def create_response_validator(self) -> IResponseValidator
â‹®----
def create_retry_handler(self) -> IRetryHandler
â‹®----
def create_coinalyze_client(self) -> CoinalyzeClient
â‹®----
container = CoinalyzeDependencyContainer(
</file>

<file path="ingestion/adapters/coinalyze_plugin/error_handlers.py">
class CoinAlyzeAPIError(Exception)
â‹®----
class BadRequestError(CoinAlyzeAPIError)
â‹®----
class AuthenticationError(CoinAlyzeAPIError)
â‹®----
class NotFoundError(CoinAlyzeAPIError)
â‹®----
class RateLimitError(CoinAlyzeAPIError)
â‹®----
class ServerError(CoinAlyzeAPIError)
â‹®----
class BadRequestHandler(IErrorHandler)
â‹®----
def can_handle(self, status_code: int, body: Any) -> bool
â‹®----
def handle(self, status_code: int, body: Any, endpoint: str) -> Exception
â‹®----
message = "Bad request"
â‹®----
message = body.get("message", message)
â‹®----
class AuthenticationHandler(IErrorHandler)
â‹®----
message = "Invalid API key or authentication failed"
â‹®----
class NotFoundHandler(IErrorHandler)
â‹®----
message = f"Endpoint not found: {endpoint}"
â‹®----
class RateLimitHandler(IErrorHandler)
â‹®----
message = "Rate limit exceeded"
retry_after = None
â‹®----
retry_after_str = body.get("retry_after", body.get("retryAfter"))
â‹®----
retry_after = float(retry_after_str)
â‹®----
class ServerErrorHandler(IErrorHandler)
â‹®----
message = "Server error"
â‹®----
def create_error_mapper_chain() -> ErrorMapperChain
â‹®----
chain = ErrorMapperChain()
</file>

<file path="ingestion/adapters/coinalyze_plugin/error_mapper.py">
class CoinAlyzeErrorMapper
â‹®----
@staticmethod
    def extract_error_message(response_body: Any) -> str
â‹®----
error_msg = CoinAlyzeErrorMapper.extract_error_message(response_body)
â‹®----
retry_after_int = None
â‹®----
retry_after_int = int(retry_after)
</file>

<file path="ingestion/adapters/coinalyze_plugin/exceptions.py">
class CoinAlyzeAPIError(Exception)
â‹®----
class BadRequestError(CoinAlyzeAPIError)
â‹®----
class AuthenticationError(CoinAlyzeAPIError)
â‹®----
class NotFoundError(CoinAlyzeAPIError)
â‹®----
class RateLimitError(CoinAlyzeAPIError)
â‹®----
def __init__(self, message: str, retry_after: int | None = None, **kwargs)
â‹®----
class ServerError(CoinAlyzeAPIError)
â‹®----
class ValidationError(CoinAlyzeAPIError)
</file>

<file path="ingestion/adapters/coinalyze_plugin/formatters.py">
class BinanceSymbolFormatter(ISymbolFormatter)
â‹®----
class BybitSymbolFormatter(ISymbolFormatter)
â‹®----
"""Format symbols for Bybit futures."""
â‹®----
"""Format symbol for Bybit.

        Spot: BTCUSDT
        Linear perpetual: BTCUSDT
        Inverse perpetual: BTCUSD
        """
â‹®----
class GateioSymbolFormatter(ISymbolFormatter)
â‹®----
"""Format symbols for Gate.io futures."""
â‹®----
"""Format symbol for Gate.io.

        Spot: BTC_USDT
        Perpetual: BTC_USDT (with _PERP suffix when needed)
        """
â‹®----
settlement = settlement or "USDT"
â‹®----
class HuobiSymbolFormatter(ISymbolFormatter)
â‹®----
"""Format symbols for Huobi futures."""
â‹®----
"""Format symbol for Huobi.

        Spot: btcusdt (lowercase)
        Perpetual: BTC_USDT
        """
â‹®----
class OkxSymbolFormatter(ISymbolFormatter)
â‹®----
"""Format symbols for OKX futures."""
â‹®----
"""Format symbol for OKX.

        Spot: BTC-USDT
        Perpetual: BTC-USDT-SWAP or BTC-USD-SWAP
        """
â‹®----
class KrakenSymbolFormatter(ISymbolFormatter)
â‹®----
"""Format symbols for Kraken futures."""
â‹®----
"""Format symbol for Kraken.

        Spot: XXBTZUSD, XETHZUSD (X-prefix for crypto)
        Perpetual: PF_XBTUSD (futures prefix)
        """
â‹®----
def create_symbol_formatter_registry() -> "SymbolFormatterRegistry"
â‹®----
registry = SymbolFormatterRegistry()
</file>

<file path="ingestion/adapters/coinalyze_plugin/impls.py">
class CoinAlyzeResponseValidator(IResponseValidator)
â‹®----
def __init__(self, config: ValidationConfig)
â‹®----
def validate(self, endpoint: str, data: Any) -> ValidationResult
â‹®----
def _validate_ohlcv_response(self, data: dict) -> ValidationResult
â‹®----
candles = data.get("data", [])
â‹®----
first = candles[0]
required_fields = {"t", "o", "h", "l", "c", "v"}
missing = required_fields - set(first.keys())
â‹®----
def _validate_oi_response(self, data: dict) -> ValidationResult
â‹®----
oi_data = data.get("data", [])
â‹®----
first = oi_data[0]
required_fields = {"t", "o"}
â‹®----
def _validate_generic_response(self, data: dict) -> ValidationResult
â‹®----
class CoinAlyzeRetryHandler(IRetryHandler)
â‹®----
def __init__(self, config: RetryConfig)
â‹®----
def should_retry(self, status_code: int) -> bool
â‹®----
delay = self.config.base_delay * (
</file>

<file path="ingestion/adapters/coinalyze_plugin/key_rotator.py">
HEAD = "api_key"
MAX_PER_KEY = 39
WINDOW = 60
â‹®----
class KeyRotator
â‹®----
def __init__(self, keys: list[str])
â‹®----
async def get_headers(self) -> dict[str, str]
â‹®----
key = await self._next_available_key()
â‹®----
async def _next_available_key(self) -> str | None
â‹®----
now = time.time()
â‹®----
key = self._keys[0]
â‹®----
async def _call_count(self, key: str, now: float) -> int
â‹®----
dq = self._local.setdefault(key, deque(maxlen=MAX_PER_KEY * 2))
â‹®----
async def _record_call(self, key: str) -> None
â‹®----
async def _refresh_keys(self) -> None
</file>

<file path="ingestion/adapters/coinalyze_plugin/mappers.py">
COINALYZE_INTERVAL_MAP = {
â‹®----
def get_coinalyze_interval(timeframe: str) -> str
â‹®----
def get_coinalyze_symbol(instrument: Instrument) -> str
â‹®----
"""
    Format symbol using registry of exchange-specific formatters.
    Takes Instrument â†’ CoinAlyze symbol format
    """
# Map venue to exchange name
exchange = instrument.venue.value.replace("_usdm", "").replace("_coinm", "")
â‹®----
# Determine market type and settlement
market_type = (
settlement = (
â‹®----
registry = create_symbol_formatter_registry()
formatter = registry.get_formatter(exchange.lower())
</file>

<file path="ingestion/adapters/coinalyze_plugin/ohlcv_adapter.py">
logger = logging.getLogger(__name__)
â‹®----
class CoinalyzeOHLCVAdapter(CoinalyzeAdapterBase, OHLCVPort)
â‹®----
capabilities = {OHLCVPort}
â‹®----
symbol = get_coinalyze_symbol(instrument)
interval = get_coinalyze_interval(timeframe)
â‹®----
params = {
â‹®----
# ðŸ”¥ BRONZE: retorna JSON cru sem tocar
â‹®----
raw = await self.client.fetch_data_async("ohlcv-history", params)
</file>

<file path="ingestion/adapters/coinalyze_plugin/open_interest_adapter.py">
logger = logging.getLogger(__name__)
â‹®----
class CoinalyzeOpenInterestAdapter(CoinalyzeAdapterBase, OpenInterestPort)
â‹®----
capabilities = {OpenInterestPort}
â‹®----
symbol = get_coinalyze_symbol(instrument)
interval = get_coinalyze_interval(timeframe)
â‹®----
params = {
</file>

<file path="ingestion/adapters/coinalyze_plugin/response_validator.py">
class ResponseValidator
â‹®----
@staticmethod
    def validate_ohlc_response(data: Any) -> tuple[bool, str]
â‹®----
# Empty list is valid (no data for the range)
â‹®----
# Validate history entries (if not empty)
â‹®----
first_entry = item["history"][0]
required_fields = ["t", "o", "h", "l", "c"]
â‹®----
@staticmethod
    def validate_oi_response(data: Any) -> tuple[bool, str]
â‹®----
@staticmethod
    def validate_response(endpoint: str, data: Any) -> tuple[bool, str]
</file>

<file path="ingestion/adapters/coinalyze_plugin/retry_handler.py">
class RetryHandler
â‹®----
RETRYABLE_STATUS_CODES = (429, 500, 502, 503, 504)
â‹®----
NON_RETRYABLE_STATUS_CODES = (400, 401, 404)
â‹®----
@classmethod
    def should_retry(cls, status_code: int) -> bool
â‹®----
base_delay = 2**attempt
â‹®----
max_delay = 60
â‹®----
max_delay = 30
</file>

<file path="ingestion/adapters/coinalyze_plugin/strategies.py">
class ISymbolFormatter(Protocol)
â‹®----
class SymbolFormatterRegistry
â‹®----
def __init__(self)
â‹®----
def register(self, exchange: str, formatter: ISymbolFormatter) -> None
â‹®----
def set_default(self, formatter: ISymbolFormatter) -> None
â‹®----
def get_formatter(self, exchange: str) -> ISymbolFormatter
â‹®----
class IErrorHandler(Protocol)
â‹®----
"""Strategy for handling specific error condition."""
â‹®----
def can_handle(self, status_code: int, body: Any) -> bool
â‹®----
"""Check if this handler can handle the error.

        Args:
            status_code: HTTP status code
            body: Response body (may be JSON or text)

        Returns:
            True if this handler should process the error
        """
â‹®----
"""Convert error response to exception.

        Args:
            status_code: HTTP status code
            body: Response body
            endpoint: API endpoint

        Returns:
            Domain-specific exception
        """
â‹®----
class ErrorMapperChain
â‹®----
"""Chain of Responsibility for error mapping.

    Replaces the 7 if/elif branches in error_mapper.py.
    Each error type has its own handler class.
    """
â‹®----
"""Initialize empty chain."""
â‹®----
def register(self, handler: IErrorHandler) -> None
â‹®----
"""Register error handler.

        Handlers are tried in registration order; first match wins.

        Args:
            handler: Error handler
        """
â‹®----
"""Map error response to exception using chain.

        Args:
            status_code: HTTP status code
            body: Response body
            endpoint: API endpoint

        Returns:
            Domain-specific exception
        """
â‹®----
# Fallback to generic error
</file>

<file path="ingestion/adapters/coinalyze_plugin/symbol_registry.py">
logger = logging.getLogger(__name__)
â‹®----
class CoinAlyzeSymbolRegistry
â‹®----
COINALYZE_EXCHANGE_CODES = {
â‹®----
def __init__(self)
â‹®----
base = base_asset.upper()
â‹®----
"""
        Bybit symbol formats (NO _PERP suffix):
        - Linear USDT: BTCUSDT.6 âœ… (confirmed working)
        - Linear USDC: BTCUSDC.6
        - Inverse (NATIVE): BTCUSD.6 âœ… (confirmed working)
        """
â‹®----
"""
        Gate.io symbol formats (underscore separator):
        - Linear USDT: BTC_USDT.Y âœ… (confirmed working)
        - Linear USDC: BTC_USDC.Y (if available)
        - Inverse (NATIVE): BTC_USD.Y
        """
â‹®----
"""
        Huobi symbol formats:
        - Linear USDT: BTCUSDT_PERP.4 âœ… (confirmed working)
        - Linear USDC: BTCUSDC_PERP.4 (if available)
        - Inverse (NATIVE): BTCUSD.4
        """
â‹®----
"""
        OKX symbol formats (needs testing):
        - Linear USDT: BTCUSDT_PERP.3 (assumed)
        - Inverse: BTCUSD.3 (assumed)
        """
â‹®----
"""
        Deribit symbol formats (needs testing):
        - Inverse: BTC-USD.2 or BTC_USD.2 (assumed)
        """
â‹®----
"""
        Generate CoinAlyze symbol using exchange-specific formatters.

        Args:
            base_asset: Base asset (e.g., "BTC")
            exchange: Exchange name (e.g., "binance")
            market_type: "linear_perpetual" or "inverse_perpetual"
            settlement_currency: "USDT", "USDC", "BUSD", or "NATIVE" for inverse

        Returns:
            CoinAlyze symbol string (e.g., "BTCUSDT_PERP.A")



































Get all possible CoinAlyze symbols for a base asset"""
symbols = []
â‹®----
include_exchanges = ["binance", "bybit", "gateio", "huobi"]
â‹®----
include_market_types = ["linear_perpetual", "inverse_perpetual"]
â‹®----
symbol_usdt = self.get_coinalyze_symbol(
â‹®----
symbol_usdc = self.get_coinalyze_symbol(
â‹®----
symbol_busd = self.get_coinalyze_symbol(
â‹®----
symbol = self.get_coinalyze_symbol(
â‹®----
coin_alyze_registry = CoinAlyzeSymbolRegistry()
</file>

<file path="ingestion/adapters/coinmetrics_plugin/__init__.py">

</file>

<file path="ingestion/adapters/coinmetrics_plugin/base.py">
class CoinMetricsAdapterBase(BaseAdapter)
â‹®----
venue = DataVenue.COINMETRICS
wrapper = WrapperImplementation.COINMETRICS
client_type = ClientType.NATIVE
connection_type = ConnectionType.REST
supported_asset_classes = {AssetClass.CRYPTO}
</file>

<file path="ingestion/adapters/coinmetrics_plugin/client.py">
logger = logging.getLogger(__name__)
â‹®----
class CoinMetricsClient
â‹®----
def __init__(self, base_url: str = "https://community-api.coinmetrics.io/v4")
â‹®----
async def __aenter__(self)
â‹®----
async def __aexit__(self, exc_type, exc_val, exc_tb)
â‹®----
url = f"{self.base_url}/{endpoint}"
â‹®----
# Rate limiting
â‹®----
# Handle rate limit
retry_after = int(response.headers.get("Retry-After", 6))
â‹®----
text = await response.text()
â‹®----
async def get_assets(self) -> list[str]
â‹®----
"""Get list of available assets."""
data = await self._make_request("catalog/assets")
â‹®----
params = {
â‹®----
data = await self._make_request("timeseries/asset-metrics", params)
â‹®----
df = pd.DataFrame(data.get("data", []))
â‹®----
df = df.set_index("time")
â‹®----
async def get_available_metrics(self, asset: str) -> list[str]
â‹®----
data = await self._make_request(f"catalog/asset-metrics/{asset}")
â‹®----
class CoinMetricsCSVClient
â‹®----
async def get_csv_data(self, symbol: str) -> pd.DataFrame
â‹®----
url = f"{self.base_url}/{symbol}.csv"
â‹®----
df = pd.read_csv(url)
</file>

<file path="ingestion/adapters/coinmetrics_plugin/onchain_adapter.py">

</file>

<file path="ingestion/adapters/__init__.py">
__all__ = [
</file>

<file path="ingestion/adapters/base.py">
logger = logging.getLogger(__name__)
â‹®----
class BaseAdapter(ABC)
â‹®----
venue: DataVenue
wrapper: WrapperImplementation
â‹®----
client_type: ClientType
connection_type: ConnectionType
â‹®----
supported_asset_classes: set[AssetClass] = set()
capabilities: set[type] = set()
â‹®----
@abstractmethod
    async def connect(self) -> None
â‹®----
@abstractmethod
    async def close(self) -> None
â‹®----
def supports_asset_class(self, asset_class: AssetClass) -> bool
â‹®----
def supports_capability(self, port_type: type) -> bool
â‹®----
def validate_instrument(self, instrument: Instrument) -> bool
â‹®----
# Check venue match (with flexibility for AGGREGATED)
# Adapters that access multiple venues (like CoinAlyze) should allow any venue
â‹®----
# Check wrapper match (strict - wrapper must match)
â‹®----
# Check active status
â‹®----
async def __aenter__(self)
â‹®----
async def __aexit__(self, exc_type, exc_val, exc_tb)
â‹®----
def __repr__(self) -> str
</file>

<file path="ingestion/backfill/__init__.py">
__all__ = [
</file>

<file path="ingestion/backfill/checkpoint_manager.py">
logger = logging.getLogger(__name__)
â‹®----
@dataclass
class BackfillCheckpoint
â‹®----
symbol: str
timeframe: str
data_type: str
source: str
last_completed_date: datetime
total_chunks_completed: int
total_records_written: int
started_at: datetime
updated_at: datetime
status: str
error_message: str | None = None
â‹®----
class ICheckpointStore(Protocol)
â‹®----
async def save_checkpoint(self, checkpoint: BackfillCheckpoint) -> None
â‹®----
class DatabaseCheckpointStore
â‹®----
def __init__(self, db_adapter: "IDatabaseAdapter")
â‹®----
query = f"""
â‹®----
result = await self.db.fetch_one(query, symbol, timeframe, data_type, source)
â‹®----
results = await self.db.fetch_all(query, status)
â‹®----
results = await self.db.fetch_all(query)
â‹®----
"""Mark backfill as completed."""
â‹®----
class CheckpointManager
â‹®----
"""
    High-level checkpoint management with automatic progress tracking.
    """
â‹®----
def __init__(self, checkpoint_store: ICheckpointStore)
â‹®----
"""
        Initialize manager.

        Args:
            checkpoint_store: Checkpoint storage backend
        """
â‹®----
"""
        Start new backfill or resume existing one.

        Returns:
            Existing or new checkpoint
        """
# Check for existing checkpoint
existing = await self.store.load_checkpoint(
â‹®----
# Create new checkpoint
now = datetime.utcnow()
checkpoint = BackfillCheckpoint(
â‹®----
last_completed_date=datetime.min,  # Will be updated on first chunk
â‹®----
"""
        Update checkpoint with progress.

        Args:
            checkpoint: Current checkpoint
            last_completed_date: Latest date completed
            records_written: Number of records written in this update
        """
â‹®----
async def complete_backfill(self, checkpoint: BackfillCheckpoint) -> None
â‹®----
async def get_progress_summary(self) -> dict[str, any]
â‹®----
"""
        Get overall progress summary.

        Returns:
            Dictionary with progress statistics
        """
all_checkpoints = await self.store.list_checkpoints()
â‹®----
in_progress = [c for c in all_checkpoints if c.status == "in_progress"]
completed = [c for c in all_checkpoints if c.status == "completed"]
failed = [c for c in all_checkpoints if c.status == "failed"]
</file>

<file path="ingestion/backfill/chunk_generator.py">
logger = logging.getLogger(__name__)
â‹®----
class TimeRangeChunkGenerator
â‹®----
def __init__(self, rate_limiter: IRateLimiter)
â‹®----
chunks = []
chunk_size = self.rate_limiter.calculate_chunk_size(timeframe)
â‹®----
current_start = start_date
â‹®----
chunk_end = min(current_start + chunk_size, end_date)
â‹®----
current_start = chunk_end
</file>

<file path="ingestion/backfill/chunk_processor.py">
logger = logging.getLogger(__name__)
â‹®----
class BackfillChunkProcessor
â‹®----
async def process_chunk(self, chunk: BackfillChunk) -> ChunkResult
â‹®----
start_time = datetime.utcnow()
â‹®----
# Create instrument object if not provided
â‹®----
records = await self.data_adapter.fetch_ohlcv_history(
â‹®----
records = await self.data_adapter.fetch_oi_history(
â‹®----
# Group records by date for daily partitioning
records_by_date = defaultdict(list)
â‹®----
# Extract timestamp from record
ts = self._extract_timestamp(record)
â‹®----
date_key = ts.date()
â‹®----
# Write each daily batch to bronze layer
total_written = 0
â‹®----
result = await self.bronze_writer.write_daily_batch(
â‹®----
written = result.get("records_written", 0)
â‹®----
duration = (datetime.utcnow() - start_time).total_seconds()
â‹®----
def _extract_timestamp(self, record: Any) -> datetime | None
â‹®----
"""
        Extract timestamp from record (dict or object).

        Args:
            record: Record object or dictionary

        Returns:
            Parsed datetime or None if not found
        """
# Try dictionary access first
â‹®----
ts = record.get("timestamp") or record.get("time")
â‹®----
ts = getattr(record, "timestamp", None) or getattr(record, "time", None)
â‹®----
ts = datetime.fromtimestamp(ts)
â‹®----
ts = datetime.fromisoformat(ts)
</file>

<file path="ingestion/backfill/coordinator.py">
logger = logging.getLogger(__name__)
â‹®----
@dataclass
class BackfillRequest
â‹®----
symbols: list[str]
timeframes: list[str]
data_types: list[str]
start_date: datetime
end_date: datetime
source: str = "coinalyze"
exchanges: list[str] | None = None
â‹®----
@dataclass
class BackfillChunk
â‹®----
symbol: str
timeframe: str
data_type: str
exchange: str
â‹®----
@dataclass
class BackfillResult
â‹®----
total_chunks: int
successful_chunks: int
total_records: int
failed_chunks: list[dict]
duration_seconds: float
status: str
â‹®----
class BackfillCoordinator
â‹®----
path_resolver = ProjectConfigPathResolver()
config_path = path_resolver.resolve_config_file("backfill.yaml")
â‹®----
exec_config = self.config.get("execution", {})
â‹®----
results = {}
â‹®----
exchanges = request.exchanges
â‹®----
exchanges = ["binance"]
â‹®----
key = f"{symbol}/{timeframe}/{data_type}/{exchange}"
â‹®----
result = await self._execute_single_backfill(
â‹®----
start_time = datetime.utcnow()
â‹®----
# Start or resume checkpoint
checkpoint = await self.checkpoint_manager.start_backfill(
â‹®----
# Adjust start date if resuming
â‹®----
start_date = checkpoint.last_completed_date + timedelta(days=1)
â‹®----
# Apply global floor date if exists
â‹®----
floor_date = datetime.fromisoformat(self.global_floor_dates[symbol]["date"])
â‹®----
start_date = floor_date
â‹®----
# Generate chunks using injected generator
chunks = self.chunk_generator.generate_chunks(
â‹®----
total_records = 0
successful_chunks = 0
failed_chunks = []
â‹®----
result = await self.chunk_processor.process_chunk(chunk)
â‹®----
status = "completed"
â‹®----
status = "partial"
â‹®----
status = "failed"
â‹®----
duration = (datetime.utcnow() - start_time).total_seconds()
â‹®----
result = BackfillResult(
â‹®----
def _log_summary(self, results: dict[str, BackfillResult]) -> None
â‹®----
total_records = sum(r.total_records for r in results.values())
total_duration = sum(r.duration_seconds for r in results.values())
â‹®----
successful = sum(1 for r in results.values() if r.status == "completed")
failed = sum(1 for r in results.values() if r.status in ("failed", "partial"))
â‹®----
errors = []
â‹®----
summary = BackfillSummary(
</file>

<file path="ingestion/backfill/dependency_container.py">
logger = logging.getLogger(__name__)
â‹®----
class BackfillDependencyContainer
â‹®----
def create_chunk_generator(self) -> TimeRangeChunkGenerator
â‹®----
def create_chunk_processor(self) -> BackfillChunkProcessor
â‹®----
def create_reporter(self) -> BackfillReporter
</file>

<file path="ingestion/backfill/rate_limiter.py">
logger = logging.getLogger(__name__)
â‹®----
class IRateLimiter(Protocol)
â‹®----
def normalize_timeframe(self, timeframe: str) -> str
â‹®----
def calculate_chunk_size(self, timeframe: str) -> timedelta
â‹®----
def should_reduce_chunk(self, points_received: int, timeframe: str) -> bool
â‹®----
def should_increase_chunk(self, points_received: int, timeframe: str) -> bool
â‹®----
class CoinAlyzeRateLimiter
â‹®----
def __init__(self, config_path: str | None = None, verbose: bool = False)
â‹®----
resolver = ProjectConfigPathResolver()
config_path = resolver.resolve_config_file("backfill.yaml")
â‹®----
config = yaml.safe_load(f)
â‹®----
rate_config = config["rate_limiting"]["coinalyze"]
â‹®----
adaptive_config = rate_config.get("adaptive_chunking", {})
â‹®----
"""
        Normalize timeframe to standard CCXT format.

        Examples:
            '1hour' -> '1h'
            '5min' -> '5m'
            '1day' -> '1d'
            '1h' -> '1h' (already normalized)
        """
normalized = self.timeframe_aliases.get(timeframe, timeframe)
â‹®----
# Default to 1h for unknown timeframes
normalized = "1h"
â‹®----
"""
        Calculate optimal chunk size for timeframe.
        Uses safe defaults with optional adaptive adjustment.
        """
normalized_tf = self.normalize_timeframe(timeframe)
base_days = self.safe_chunk_days.get(normalized_tf, 36.0)
â‹®----
# Apply adaptive adjustment if enabled and we have performance data
â‹®----
stats = self.performance_stats[normalized_tf]
â‹®----
avg_points = sum(s["points"] for s in stats) / len(stats)
points_ratio = avg_points / self.max_points
â‹®----
adjusted_days = base_days * 0.9
â‹®----
base_days = adjusted_days
â‹®----
# Safe to increase, grow by 10%
adjusted_days = min(
â‹®----
"""
        Check if we're approaching API limits.
        Returns True if >= 95% of max points received.
        """
threshold = self.max_points * self.reduce_threshold
approaching_limit = points_received >= threshold
â‹®----
"""
        Check if safe to increase chunk size.
        Returns True if < 70% of max points received.
        """
threshold = self.max_points * self.increase_threshold
safe_to_increase = points_received < threshold
â‹®----
"""
        Record performance metrics for adaptive learning.
        """
â‹®----
"""
        Validate API response for truncation or issues.

        Args:
            points_received: Number of points in response
            timeframe: Timeframe of request
            expected_range: Optional (min, max) expected point range

        Returns:
            Validation result dictionary with warnings/errors
        """
result = {"valid": True, "warnings": [], "truncated": False}
â‹®----
# Check if approaching limit
â‹®----
# Check expected range if provided
â‹®----
def get_performance_summary(self) -> dict[str, dict]
â‹®----
"""
        Get performance statistics summary for all timeframes.
        """
summary = {}
â‹®----
successful = [s for s in stats if s["success"]]
</file>

<file path="ingestion/backfill/reporter.py">
logger = logging.getLogger(__name__)
â‹®----
class BackfillReporter
â‹®----
def __init__(self, verbose: bool = False)
â‹®----
progress_pct = (
â‹®----
def log_summary(self, summary: BackfillSummary) -> None
â‹®----
rate = summary.total_records_written / max(
â‹®----
def _success_rate_pct(self, summary: BackfillSummary) -> float
â‹®----
total = summary.successful_chunks + summary.failed_chunks
</file>

<file path="ingestion/config/value_objects.py">
@dataclass(frozen=True)
class HttpClientConfig
â‹®----
timeout: float = 30.0
max_retries: int = 3
backoff_factor: float = 1.5
connect_timeout: float = 10.0
â‹®----
@dataclass(frozen=True)
class RetryConfig
â‹®----
max_attempts: int = 3
base_delay: float = 1.0
max_delay: float = 60.0
backoff_multiplier: float = 2.0
retryable_status_codes: tuple[int, ...] = (429, 500, 502, 503, 504)
â‹®----
@dataclass(frozen=True)
class ValidationConfig
â‹®----
strict_mode: bool = True
validate_nulls: bool = False
â‹®----
@dataclass(frozen=True)
class CoinalyzeConfig
â‹®----
base_url: str
api_keys: list[str]
request_interval: float = 0.1
rate_limit: int = 100
rate_limit_period: float = 60.0
http_config: HttpClientConfig = None
retry_config: RetryConfig = None
validation_config: ValidationConfig = None
â‹®----
def __post_init__(self)
â‹®----
@dataclass(frozen=True)
class BackfillConfig
â‹®----
chunk_size_days: int = 30
max_concurrent_chunks: int = 3
batch_write_size: int = 1000
enable_checkpointing: bool = True
checkpoint_interval_chunks: int = 5
â‹®----
@dataclass(frozen=True)
class SymbolFormattingConfig
â‹®----
exchange: str
use_normalized_symbols: bool = True
contract_separator: str = ":"
</file>

<file path="ingestion/connectors/__init__.py">

</file>

<file path="ingestion/connectors/aiohttp_client.py">
class AiohttpClient(IHttpClient)
â‹®----
def __init__(self, config: HttpClientConfig)
â‹®----
async def _get_session(self) -> aiohttp.ClientSession
â‹®----
timeout = aiohttp.ClientTimeout(total=self.config.timeout)
â‹®----
session = await self._get_session()
timeout_obj = aiohttp.ClientTimeout(total=timeout or self.config.timeout)
â‹®----
body = await resp.json() if resp.status == 200 else await resp.text()
â‹®----
async def close(self) -> None
</file>

<file path="ingestion/connectors/rest.py">
logger = logging.getLogger(__name__)
â‹®----
class RestConnector
â‹®----
def __init__(self, timeout: int = 30)
â‹®----
async def __aenter__(self)
â‹®----
async def __aexit__(self, exc_type, exc_val, exc_tb)
</file>

<file path="ingestion/factories/__init__.py">
__all__ = ["AdapterFactory", "PreprocessorFactory"]
</file>

<file path="ingestion/factories/adapter_factory.py">
AdapterBuilder = Callable[..., BaseAdapter]
â‹®----
class AdapterFactory
â‹®----
def __init__(self) -> None
â‹®----
def register(self, provider: DataProvider, builder: AdapterBuilder) -> None
â‹®----
def create(self, provider: DataProvider, *args: Any, **kwargs: Any) -> BaseAdapter
â‹®----
def available_providers(self) -> list[DataProvider]
</file>

<file path="ingestion/factories/preprocessor_factory.py">
PreprocessorBuilder = Callable[..., DataPreprocessor]
â‹®----
class PreprocessorFactory
â‹®----
def __init__(self) -> None
â‹®----
def register(self, provider: DataProvider, builder: PreprocessorBuilder) -> None
â‹®----
def available_providers(self) -> list[DataProvider]
</file>

<file path="ingestion/models/__init__.py">

</file>

<file path="ingestion/models/enums.py">
class DataProvider(str, Enum)
â‹®----
BINANCE = "binance"
BYBIT = "bybit"
GATEIO = "gateio"
HUOBI = "huobi"
BITGET = "bitget"
OKX = "okx"
KRAKEN = "kraken"
COINBASE = "coinbase"
â‹®----
ICE = "ice"
BLOOMBERG = "bloomberg"
REFINITIV = "refinitiv"
REUTERS = "reuters"
FACTSET = "factset"
â‹®----
OANDA = "oanda"
FXCM = "fxcm"
â‹®----
CME = "cme"
â‹®----
INTERNAL = "internal"
FILE = "file"
â‹®----
class ClientType(str, Enum)
â‹®----
WRAPPER = "wrapper"
NATIVE = "native"
VENDOR_SDK = "vendor_sdk"
FILE_PARSER = "file_parser"
â‹®----
class ConnectionType(str, Enum)
â‹®----
REST = "rest"
WEBSOCKET = "websocket"
FIX = "fix"
GRPC = "grpc"
SFTP = "sftp"
â‹®----
DATABASE = "database"
â‹®----
class PipelineMode(str, Enum)
â‹®----
BACKFILL = "backfill"
INCREMENTAL = "incremental"
STREAMING = "streaming"
â‹®----
class IngestionStatus(str, Enum)
â‹®----
PENDING = "pending"
RUNNING = "running"
SUCCESS = "success"
FAILED = "failed"
PARTIAL = "partial"
SKIPPED = "skipped"
</file>

<file path="ingestion/orchestration/backfill/__init__.py">
__all__ = [
</file>

<file path="ingestion/orchestration/backfill/checkpoint_manager.py">
logger = logging.getLogger(__name__)
â‹®----
@dataclass
class BackfillCheckpoint
â‹®----
symbol: str
timeframe: str
data_type: str
source: str
last_completed_date: datetime
total_chunks_completed: int
total_records_written: int
started_at: datetime
updated_at: datetime
status: str
error_message: str | None = None
â‹®----
class ICheckpointStore(Protocol)
â‹®----
async def save_checkpoint(self, checkpoint: BackfillCheckpoint) -> None
â‹®----
class DatabaseCheckpointStore
â‹®----
def __init__(self, db_adapter: "IDatabaseAdapter")
â‹®----
query = f"""
â‹®----
result = await self.db.fetch_one(query, symbol, timeframe, data_type, source)
â‹®----
results = await self.db.fetch_all(query, status)
â‹®----
results = await self.db.fetch_all(query)
â‹®----
"""Mark backfill as completed."""
â‹®----
class CheckpointManager
â‹®----
"""
    High-level checkpoint management with automatic progress tracking.
    """
â‹®----
def __init__(self, checkpoint_store: ICheckpointStore)
â‹®----
"""
        Initialize manager.

        Args:
            checkpoint_store: Checkpoint storage backend
        """
â‹®----
"""
        Start new backfill or resume existing one.

        Returns:
            Existing or new checkpoint
        """
# Check for existing checkpoint
existing = await self.store.load_checkpoint(
â‹®----
# Create new checkpoint
now = datetime.utcnow()
checkpoint = BackfillCheckpoint(
â‹®----
last_completed_date=datetime.min,  # Will be updated on first chunk
â‹®----
"""
        Update checkpoint with progress.

        Args:
            checkpoint: Current checkpoint
            last_completed_date: Latest date completed
            records_written: Number of records written in this update
        """
â‹®----
async def complete_backfill(self, checkpoint: BackfillCheckpoint) -> None
â‹®----
async def get_progress_summary(self) -> dict[str, any]
â‹®----
"""
        Get overall progress summary.

        Returns:
            Dictionary with progress statistics
        """
all_checkpoints = await self.store.list_checkpoints()
â‹®----
in_progress = [c for c in all_checkpoints if c.status == "in_progress"]
completed = [c for c in all_checkpoints if c.status == "completed"]
failed = [c for c in all_checkpoints if c.status == "failed"]
</file>

<file path="ingestion/orchestration/backfill/chunk_generator.py">
logger = logging.getLogger(__name__)
â‹®----
class TimeRangeChunkGenerator
â‹®----
def __init__(self, rate_limiter: IRateLimiter)
â‹®----
chunks = []
chunk_size = self.rate_limiter.calculate_chunk_size(timeframe)
â‹®----
current_start = start_date
â‹®----
chunk_end = min(current_start + chunk_size, end_date)
â‹®----
current_start = chunk_end
</file>

<file path="ingestion/orchestration/backfill/chunk_processor.py">
logger = logging.getLogger(__name__)
â‹®----
class BackfillChunkProcessor
â‹®----
async def process_chunk(self, chunk: BackfillChunk) -> ChunkResult
â‹®----
start_time = datetime.utcnow()
â‹®----
# Create instrument object if not provided
â‹®----
records = await self.data_adapter.fetch_ohlcv_history(
â‹®----
records = await self.data_adapter.fetch_oi_history(
â‹®----
# Group records by date for daily partitioning
records_by_date = defaultdict(list)
â‹®----
# Extract timestamp from record
ts = self._extract_timestamp(record)
â‹®----
date_key = ts.date()
â‹®----
# Write each daily batch to bronze layer
total_written = 0
â‹®----
result = await self.bronze_writer.write_daily_batch(
â‹®----
written = result.get("records_written", 0)
â‹®----
duration = (datetime.utcnow() - start_time).total_seconds()
â‹®----
def _extract_timestamp(self, record: Any) -> datetime | None
â‹®----
"""
        Extract timestamp from record (dict or object).

        Args:
            record: Record object or dictionary

        Returns:
            Parsed datetime or None if not found
        """
# Try dictionary access first
â‹®----
ts = record.get("timestamp") or record.get("time")
â‹®----
ts = getattr(record, "timestamp", None) or getattr(record, "time", None)
â‹®----
ts = datetime.fromtimestamp(ts)
â‹®----
ts = datetime.fromisoformat(ts)
</file>

<file path="ingestion/orchestration/backfill/coordinator.py">
logger = logging.getLogger(__name__)
â‹®----
@dataclass
class BackfillRequest
â‹®----
symbols: list[str]
timeframes: list[str]
data_types: list[str]
start_date: datetime
end_date: datetime
source: str = "coinalyze"
exchanges: list[str] | None = None
â‹®----
@dataclass
class BackfillChunk
â‹®----
symbol: str
timeframe: str
data_type: str
exchange: str
â‹®----
@dataclass
class BackfillResult
â‹®----
total_chunks: int
successful_chunks: int
total_records: int
failed_chunks: list[dict]
duration_seconds: float
status: str
â‹®----
class BackfillCoordinator
â‹®----
path_resolver = ProjectConfigPathResolver()
config_path = path_resolver.resolve_config_file("backfill.yaml")
â‹®----
exec_config = self.config.get("execution", {})
â‹®----
results = {}
â‹®----
exchanges = request.exchanges
â‹®----
exchanges = ["binance"]
â‹®----
key = f"{symbol}/{timeframe}/{data_type}/{exchange}"
â‹®----
result = await self._execute_single_backfill(
â‹®----
start_time = datetime.utcnow()
â‹®----
# Start or resume checkpoint
checkpoint = await self.checkpoint_manager.start_backfill(
â‹®----
# Adjust start date if resuming
â‹®----
start_date = checkpoint.last_completed_date + timedelta(days=1)
â‹®----
# Apply global floor date if exists
â‹®----
floor_date = datetime.fromisoformat(self.global_floor_dates[symbol]["date"])
â‹®----
start_date = floor_date
â‹®----
# Generate chunks using injected generator
chunks = self.chunk_generator.generate_chunks(
â‹®----
total_records = 0
successful_chunks = 0
failed_chunks = []
â‹®----
result = await self.chunk_processor.process_chunk(chunk)
â‹®----
status = "completed"
â‹®----
status = "partial"
â‹®----
status = "failed"
â‹®----
duration = (datetime.utcnow() - start_time).total_seconds()
â‹®----
result = BackfillResult(
â‹®----
def _log_summary(self, results: dict[str, BackfillResult]) -> None
â‹®----
total_records = sum(r.total_records for r in results.values())
total_duration = sum(r.duration_seconds for r in results.values())
â‹®----
successful = sum(1 for r in results.values() if r.status == "completed")
failed = sum(1 for r in results.values() if r.status in ("failed", "partial"))
â‹®----
errors = []
â‹®----
summary = BackfillSummary(
</file>

<file path="ingestion/orchestration/backfill/dependency_container.py">
logger = logging.getLogger(__name__)
â‹®----
class BackfillDependencyContainer
â‹®----
def create_chunk_generator(self) -> TimeRangeChunkGenerator
â‹®----
def create_chunk_processor(self) -> BackfillChunkProcessor
â‹®----
def create_reporter(self) -> BackfillReporter
</file>

<file path="ingestion/orchestration/backfill/rate_limiter.py">
logger = logging.getLogger(__name__)
â‹®----
class IRateLimiter(Protocol)
â‹®----
def normalize_timeframe(self, timeframe: str) -> str
â‹®----
def calculate_chunk_size(self, timeframe: str) -> timedelta
â‹®----
def should_reduce_chunk(self, points_received: int, timeframe: str) -> bool
â‹®----
def should_increase_chunk(self, points_received: int, timeframe: str) -> bool
â‹®----
class CoinAlyzeRateLimiter
â‹®----
def __init__(self, config_path: str | None = None, verbose: bool = False)
â‹®----
resolver = ProjectConfigPathResolver()
config_path = resolver.resolve_config_file("backfill.yaml")
â‹®----
config = yaml.safe_load(f)
â‹®----
rate_config = config["rate_limiting"]["coinalyze"]
â‹®----
adaptive_config = rate_config.get("adaptive_chunking", {})
â‹®----
"""
        Normalize timeframe to standard CCXT format.

        Examples:
            '1hour' -> '1h'
            '5min' -> '5m'
            '1day' -> '1d'
            '1h' -> '1h' (already normalized)
        """
normalized = self.timeframe_aliases.get(timeframe, timeframe)
â‹®----
# Default to 1h for unknown timeframes
normalized = "1h"
â‹®----
"""
        Calculate optimal chunk size for timeframe.
        Uses safe defaults with optional adaptive adjustment.
        """
normalized_tf = self.normalize_timeframe(timeframe)
base_days = self.safe_chunk_days.get(normalized_tf, 36.0)
â‹®----
# Apply adaptive adjustment if enabled and we have performance data
â‹®----
stats = self.performance_stats[normalized_tf]
â‹®----
avg_points = sum(s["points"] for s in stats) / len(stats)
points_ratio = avg_points / self.max_points
â‹®----
adjusted_days = base_days * 0.9
â‹®----
base_days = adjusted_days
â‹®----
# Safe to increase, grow by 10%
adjusted_days = min(
â‹®----
"""
        Check if we're approaching API limits.
        Returns True if >= 95% of max points received.
        """
threshold = self.max_points * self.reduce_threshold
approaching_limit = points_received >= threshold
â‹®----
"""
        Check if safe to increase chunk size.
        Returns True if < 70% of max points received.
        """
threshold = self.max_points * self.increase_threshold
safe_to_increase = points_received < threshold
â‹®----
"""
        Record performance metrics for adaptive learning.
        """
â‹®----
"""
        Validate API response for truncation or issues.

        Args:
            points_received: Number of points in response
            timeframe: Timeframe of request
            expected_range: Optional (min, max) expected point range

        Returns:
            Validation result dictionary with warnings/errors
        """
result = {"valid": True, "warnings": [], "truncated": False}
â‹®----
# Check if approaching limit
â‹®----
# Check expected range if provided
â‹®----
def get_performance_summary(self) -> dict[str, dict]
â‹®----
"""
        Get performance statistics summary for all timeframes.
        """
summary = {}
â‹®----
successful = [s for s in stats if s["success"]]
</file>

<file path="ingestion/orchestration/backfill/reporter.py">
logger = logging.getLogger(__name__)
â‹®----
class BackfillReporter
â‹®----
def __init__(self, verbose: bool = False)
â‹®----
progress_pct = (
â‹®----
def log_summary(self, summary: BackfillSummary) -> None
â‹®----
rate = summary.total_records_written / max(
â‹®----
def _success_rate_pct(self, summary: BackfillSummary) -> float
â‹®----
total = summary.successful_chunks + summary.failed_chunks
</file>

<file path="ingestion/orchestration/data_fetcher.py">
class Normalizer(Protocol)
â‹®----
class DataFetchOrchestrator
â‹®----
raw = await self.adapter.fetch_ohlcv(instrument, timeframe, start, end, limit)
preprocessed = self.preprocessor.preprocess_ohlcv(instrument, raw)
â‹®----
raw = await self.adapter.fetch_open_interest(
preprocessed = self.preprocessor.preprocess_open_interest(instrument, raw)
</file>

<file path="ingestion/pipelines/__init__.py">

</file>

<file path="ingestion/ports/__init__.py">
__all__ = [
</file>

<file path="ingestion/ports/backfill.py">
@dataclass
class BackfillChunk
â‹®----
symbol: str
timeframe: str
data_type: str
start_date: datetime
end_date: datetime
instrument: Instrument | None = None
â‹®----
@dataclass
class ChunkResult
â‹®----
chunk: BackfillChunk
records_written: int
success: bool
error: str | None = None
duration_seconds: float = 0.0
â‹®----
class IChunkGenerator(Protocol)
â‹®----
class IChunkProcessor(Protocol)
â‹®----
async def process_chunk(self, chunk: BackfillChunk) -> ChunkResult
â‹®----
@dataclass
class BackfillSummary
â‹®----
total_chunks: int
successful_chunks: int
failed_chunks: int
total_records_written: int
total_duration_seconds: float
errors: list[tuple[str, str]]
â‹®----
class IBackfillReporter(Protocol)
â‹®----
def log_summary(self, summary: BackfillSummary) -> None
â‹®----
class ICheckpointManager(Protocol)
â‹®----
class IBackfillAdapter(Protocol)
â‹®----
class IBronzeWriter(Protocol)
</file>

<file path="ingestion/ports/data_ports.py">
class OHLCVPort(Protocol)
â‹®----
class OpenInterestPort(Protocol)
</file>

<file path="ingestion/ports/http.py">
@dataclass
class HttpResponse
â‹®----
status_code: int
body: Any
headers: dict[str, str]
url: str
â‹®----
class IHttpClient(Protocol)
â‹®----
class IApiKeyProvider(Protocol)
â‹®----
async def get_headers(self) -> dict[str, str]
â‹®----
async def rotate_key_on_failure(self, failed_key: str | None = None) -> None
</file>

<file path="ingestion/ports/validators.py">
@dataclass
class ValidationResult
â‹®----
is_valid: bool
error_message: str | None = None
error_code: str | None = None
â‹®----
class IResponseValidator(Protocol)
â‹®----
def validate(self, endpoint: str, data: Any) -> ValidationResult
â‹®----
class IErrorMapper(Protocol)
â‹®----
class IRetryHandler(Protocol)
â‹®----
def should_retry(self, status_code: int) -> bool
</file>

<file path="ingestion/preprocessing/base.py">
class DataPreprocessor(ABC)
</file>

<file path="ingestion/preprocessing/providers.py">
def _to_ms(value: Any) -> Any
â‹®----
class BinancePreprocessor(DataPreprocessor)
â‹®----
ts = row[0] if isinstance(row, (list, tuple)) else row.get("timestamp")
â‹®----
amount = row.get("openInterestAmount") or row.get("sumOpenInterest")
value = row.get("openInterestValue")
â‹®----
class BybitPreprocessor(DataPreprocessor)
â‹®----
amount = row.get("openInterest")
â‹®----
class GateIOPreprocessor(DataPreprocessor)
â‹®----
class HuobiPreprocessor(DataPreprocessor)
â‹®----
ts = (
â‹®----
amount = (
â‹®----
value = amount
â‹®----
class BitgetPreprocessor(DataPreprocessor)
â‹®----
logger = logging.getLogger(__name__)
â‹®----
class CoinalyzePreprocessor(DataPreprocessor)
â‹®----
history = symbol_data.get("history", [])
â‹®----
def _to_ms(self, timestamp: int) -> int
</file>

<file path="ingestion/symbol_resolution/resolver.py">
class SymbolRegistry(Protocol)
â‹®----
def resolve(self, symbol: str) -> dict: ...
â‹®----
class SymbolResolver
â‹®----
def __init__(self, registry: SymbolRegistry)
â‹®----
def resolve(self, symbol: str) -> Instrument
â‹®----
data = self.registry.resolve(symbol)
</file>

<file path="ingestion/__init__.py">

</file>

<file path="ingestion/dependency_container.py">
logger = logging.getLogger(__name__)
â‹®----
class IngestionDependencyContainer
â‹®----
def create_http_client(self) -> IHttpClient
â‹®----
def create_api_key_provider(self) -> IApiKeyProvider
â‹®----
coinalyze_container = CoinalyzeDependencyContainer(
â‹®----
def create_response_validator(self) -> IResponseValidator
â‹®----
def create_error_mapper(self) -> IErrorMapper
â‹®----
def create_retry_handler(self) -> IRetryHandler
â‹®----
def create_coinalyze_client(self) -> CoinalyzeClient
â‹®----
def create_data_adapter(self) -> CoinalyzeBackfillAdapter
â‹®----
def create_chunk_generator(self) -> IChunkGenerator
â‹®----
def create_chunk_processor(self) -> IChunkProcessor
â‹®----
def create_reporter(self) -> IBackfillReporter
â‹®----
def create_backfill_container(self) -> BackfillDependencyContainer
â‹®----
def create_backfill_coordinator(self) -> Any
â‹®----
def _load_http_config(self) -> HttpClientConfig
â‹®----
def _load_coinalyze_config(self) -> CoinalyzeConfig
â‹®----
def _load_validation_config(self) -> ValidationConfig
â‹®----
def _load_retry_config(self) -> RetryConfig
â‹®----
def _load_backfill_config(self) -> BackfillConfig
</file>

<file path="ingestion/service.py">
class IngestionService
â‹®----
adapter = self.adapter_factory.create(provider, *args, **kwargs)
preprocessor = self.preprocessor_factory.create(provider)
â‹®----
orchestrator = self._build_orchestrator(provider, **adapter_kwargs)
</file>

<file path="legacy/asset_coverage.py">
__all__ = ["AssetCoverageManager"]
</file>

<file path="legacy/ccxt_exchanges.py">
__all__ = [
</file>

<file path="legacy/coinalyze.py">
__all__ = ["CoinalyzeClient", "CoinAlyzeFetcher"]
</file>

<file path="legacy/database.py">
__all__ = ["Database"]
</file>

<file path="legacy/enums.py">
__all__ = ["MarketDataType", "MarketType"]
</file>

<file path="legacy/orchestrator.py">
__all__ = ["CoreOrchestrator", "OrchestrationConfig"]
</file>

<file path="orchestration/operators/__init__.py">
__all__ = [
</file>

<file path="orchestration/operators/base.py">
logger = logging.getLogger(__name__)
â‹®----
@dataclass
class OperatorResult
â‹®----
success: bool
records_processed: int = 0
duration_seconds: float = 0.0
error: str | None = None
metadata: dict[str, Any] = field(default_factory=dict)
â‹®----
def to_dict(self) -> dict[str, Any]
â‹®----
class BaseDataOperator
â‹®----
"""
        Execute operation with retry logic.

        Args:
            operation: Async callable to execute
            *args: Positional arguments for operation
            **kwargs: Keyword arguments for operation

        Returns:
            Operation result

        Raises:
            Exception: If all retries exhausted
        """
last_error = None
â‹®----
result = await operation(*args, **kwargs)
â‹®----
last_error = e
â‹®----
"""
        Create operator result.

        Args:
            success: Whether operation succeeded
            records_processed: Number of records processed
            duration: Duration in seconds
            error: Error message if failed
            **metadata: Additional metadata

        Returns:
            OperatorResult instance
        """
â‹®----
def log_info(self, message: str) -> None
â‹®----
"""Log informational message."""
â‹®----
def log_warning(self, message: str) -> None
â‹®----
"""Log warning message."""
â‹®----
def log_error(self, message: str) -> None
â‹®----
"""Log error message."""
</file>

<file path="orchestration/operators/checkpoint_operators.py">
logger = logging.getLogger(__name__)
â‹®----
class CheckpointReaderOperator(BaseDataOperator)
â‹®----
async def execute(self, context: dict[str, Any] | None = None) -> dict[str, Any]
â‹®----
start_time = datetime.utcnow()
â‹®----
checkpoint_data = await self.checkpoint_store.load_checkpoint(
â‹®----
duration = (datetime.utcnow() - start_time).total_seconds()
â‹®----
last_ts = checkpoint_data.get("last_timestamp")
last_date = (
â‹®----
error_msg = f"Checkpoint read failed: {str(e)}"
â‹®----
class CheckpointWriterOperator(BaseDataOperator)
â‹®----
"""
    Operator for writing checkpoint state.

    Saves checkpoint after successful data processing.
    """
â‹®----
"""
        Initialize checkpoint writer operator.

        Args:
            task_id: Task identifier
            checkpoint_store: Checkpoint storage
            symbol: Symbol identifier
            timeframe: Timeframe
            data_type: Type of data
            source: Data source
            **kwargs: Additional args for BaseDataOperator
        """
â‹®----
"""
        Execute checkpoint write.

        Args:
            context: Airflow context dictionary
            last_timestamp: Last processed timestamp (Unix ms)
            metadata: Additional checkpoint metadata

        Returns:
            Result dictionary
        """
â‹®----
# Get last_timestamp from XCom if not provided
â‹®----
ti = context.get("ti")
â‹®----
upstream_result = ti.xcom_pull(task_ids=None)
â‹®----
last_timestamp = upstream_result.get("last_timestamp")
metadata = upstream_result.get("metadata", metadata)
â‹®----
error_msg = "No last_timestamp provided for checkpoint"
â‹®----
last_date = datetime.utcfromtimestamp(last_timestamp / 1000)
â‹®----
success = await self.checkpoint_store.save_checkpoint(
â‹®----
error_msg = f"Checkpoint write failed: {str(e)}"
â‹®----
class CheckpointDeleteOperator(BaseDataOperator)
â‹®----
"""
    Operator for deleting checkpoint state.

    Useful for forcing cold start or cleanup.
    """
â‹®----
"""
        Initialize checkpoint delete operator.

        Args:
            task_id: Task identifier
            checkpoint_store: Checkpoint storage
            symbol: Symbol identifier
            timeframe: Timeframe
            data_type: Type of data
            source: Data source
            **kwargs: Additional args for BaseDataOperator
        """
â‹®----
"""
        Execute checkpoint delete.

        Args:
            context: Airflow context dictionary

        Returns:
            Result dictionary
        """
â‹®----
success = await self.checkpoint_store.delete_checkpoint(
â‹®----
error_msg = f"Checkpoint delete failed: {str(e)}"
</file>

<file path="orchestration/operators/fetch_operators.py">
logger = logging.getLogger(__name__)
â‹®----
class FetchOHLCVOperator(BaseDataOperator)
â‹®----
async def execute(self, context: dict[str, Any] | None = None) -> dict[str, Any]
â‹®----
start_time = datetime.utcnow()
â‹®----
# Rate limiting
â‹®----
# Fetch with retry
data = await self.execute_with_retry(
â‹®----
duration = (datetime.utcnow() - start_time).total_seconds()
â‹®----
result = self.create_result(
â‹®----
error_msg = f"OHLCV fetch failed: {str(e)}"
â‹®----
class FetchOpenInterestOperator(BaseDataOperator)
â‹®----
"""
    Operator for fetching open interest data from an adapter.
    """
â‹®----
adapter: Any,  # Data adapter with fetch_oi method
â‹®----
"""
        Initialize open interest fetch operator.

        Args:
            task_id: Task identifier
            adapter: Data adapter
            symbol: Symbol to fetch
            timeframe: Timeframe
            exchange: Exchange identifier
            since: Start timestamp (Unix ms)
            limit: Max records to fetch
            rate_limiter: Optional rate limiter
            **kwargs: Additional args for BaseDataOperator
        """
â‹®----
"""
        Execute open interest fetch.

        Args:
            context: Airflow context dictionary

        Returns:
            Result dictionary for XCom
        """
â‹®----
error_msg = f"OI fetch failed: {str(e)}"
â‹®----
class FetchHistoricalOperator(BaseDataOperator)
â‹®----
"""
    Operator for fetching historical data over a time range.

    Suitable for backfill operations that need a date range.
    """
â‹®----
adapter: Any,  # Adapter with fetch_*_history methods
â‹®----
data_type: str,  # "ohlcv" or "open_interest"
â‹®----
# Determine fetch method based on data type
â‹®----
fetch_method = self.adapter.fetch_ohlcv_history
fetch_kwargs = {
â‹®----
fetch_method = self.adapter.fetch_oi_history
â‹®----
error_msg = f"Historical fetch failed: {str(e)}"
</file>

<file path="orchestration/operators/write_operators.py">
logger = logging.getLogger(__name__)
â‹®----
class WriteBronzeOperator(BaseDataOperator)
â‹®----
start_time = datetime.utcnow()
â‹®----
ti = context.get("ti")
â‹®----
upstream_result = ti.xcom_pull(task_ids=None)
â‹®----
data = upstream_result.get("data")
â‹®----
error_msg = "No data provided or available in XCom"
â‹®----
# Write with retry
result = await self.execute_with_retry(
â‹®----
duration = (datetime.utcnow() - start_time).total_seconds()
â‹®----
records_written = result.get("records_written", 0)
file_path = result.get("file_path")
â‹®----
error_msg = result.get("error", "Unknown write error")
â‹®----
error_msg = f"Bronze write exception: {str(e)}"
â‹®----
class WriteSilverOperator(BaseDataOperator)
â‹®----
"""
    Operator for writing data to silver layer (TimescaleDB).

    Writes normalized, validated data to database repositories.
    """
â‹®----
repository: Any,  # Repository instance (OHLCVRepository, etc.)
â‹®----
"""
        Initialize silver write operator.

        Args:
            task_id: Task identifier
            repository: Repository instance
            **kwargs: Additional args for BaseDataOperator
        """
â‹®----
"""
        Execute silver write.

        Args:
            context: Airflow context dictionary
            records: Records to write (Pydantic models)

        Returns:
            Result dictionary for XCom
        """
â‹®----
# Pull records from XCom if not provided
â‹®----
records = upstream_result.get("records")
â‹®----
error_msg = "No records provided or available in XCom"
â‹®----
records_written = result if isinstance(result, int) else len(records)
â‹®----
error_msg = f"Silver write exception: {str(e)}"
</file>

<file path="orchestration/workflows/__init__.py">
__all__ = [
</file>

<file path="orchestration/workflows/backfill_workflow.py">
logger = logging.getLogger(__name__)
â‹®----
@dataclass
class BackfillRequest
â‹®----
symbols: list[str]
timeframes: list[str]
data_types: list[str]
start_date: datetime
end_date: datetime
source: str = "coinalyze"
exchanges: list[str] | None = None
â‹®----
@dataclass
class BackfillChunk
â‹®----
symbol: str
timeframe: str
data_type: str
exchange: str
â‹®----
metadata: dict[str, Any] = field(default_factory=dict)
â‹®----
@dataclass
class BackfillResult
â‹®----
total_chunks: int
successful_chunks: int
total_records: int
failed_chunks: list[dict[str, Any]]
duration_seconds: float
status: str
â‹®----
def to_dict(self) -> dict[str, Any]
â‹®----
class BackfillWorkflow(BaseWorkflow)
â‹®----
async def validate(self, context: IWorkflowContext) -> tuple[bool, list[str]]
â‹®----
errors = []
â‹®----
request = context.get_parameter("request")
â‹®----
required_fields = [
â‹®----
# Validate date range
â‹®----
start = request.get("start_date")
end = request.get("end_date")
â‹®----
async def _execute_impl(self, context: IWorkflowContext) -> dict[str, Any]
â‹®----
request_data = context.get_parameter("request")
â‹®----
request = BackfillRequest(**request_data)
â‹®----
request = request_data
â‹®----
results = {}
â‹®----
exchanges = request.exchanges
â‹®----
exchanges = ["binance"]
â‹®----
key = f"{symbol}/{timeframe}/{data_type}/{exchange}"
â‹®----
result = await self._execute_single_backfill(
â‹®----
error_msg = f"Backfill failed for {key}: {str(e)}"
â‹®----
all_success = all(r["status"] == "completed" for r in results.values())
any_partial = any(r["status"] == "partial" for r in results.values())
â‹®----
start_time = datetime.utcnow()
â‹®----
# Load checkpoint to resume if needed
checkpoint_data = await self.checkpoint_store.load_checkpoint(
â‹®----
last_ts = checkpoint_data["last_timestamp"]
resume_date = datetime.utcfromtimestamp(last_ts / 1000)
â‹®----
start_date = resume_date
â‹®----
# Apply global floor date if exists
â‹®----
floor_str = self.global_floor_dates[symbol]
â‹®----
floor_str = floor_str.get("date", floor_str.get("start_date"))
floor_date = datetime.fromisoformat(floor_str)
â‹®----
start_date = floor_date
â‹®----
# Generate chunks
chunk_ranges = self.chunk_strategy.generate_chunks(
â‹®----
total_records = 0
successful_chunks = 0
failed_chunks = []
â‹®----
chunk = BackfillChunk(
â‹®----
records_written = 0
success = False
last_error = None
â‹®----
records_written = await self._process_chunk(chunk, context)
success = True
â‹®----
last_error = e
â‹®----
status = "partial" if successful_chunks > 0 else "failed"
â‹®----
status = "completed"
â‹®----
duration = (datetime.utcnow() - start_time).total_seconds()
â‹®----
raw_data = await self.data_adapter.fetch_ohlcv_history(
â‹®----
raw_data = await self.data_adapter.fetch_oi_history(
â‹®----
# Write to bronze layer
result = await self.bronze_writer.write_daily_batch(
</file>

<file path="orchestration/workflows/base.py">
logger = logging.getLogger(__name__)
â‹®----
@dataclass
class WorkflowResult
â‹®----
status: WorkflowStatus
duration_seconds: float
records_processed: int = 0
records_written: int = 0
errors: list[str] = field(default_factory=list)
metadata: dict[str, Any] = field(default_factory=dict)
â‹®----
def to_dict(self) -> dict[str, Any]
â‹®----
class AirflowWorkflowContext
â‹®----
def __init__(self, ti: Any, **kwargs)
â‹®----
@property
    def workflow_id(self) -> str
â‹®----
"""Unique identifier for this workflow execution."""
â‹®----
@property
    def execution_date(self) -> datetime
â‹®----
@property
    def params(self) -> dict[str, Any]
â‹®----
def get_parameter(self, key: str, default: Any = None) -> Any
â‹®----
def set_state(self, key: str, value: Any) -> None
â‹®----
def get_state(self, key: str, task_id: str | None = None) -> Any
â‹®----
"""Get workflow state (XCom pull in Airflow)."""
â‹®----
def log_info(self, message: str) -> None
â‹®----
"""Log informational message."""
â‹®----
def log_warning(self, message: str) -> None
â‹®----
"""Log warning message."""
â‹®----
def log_error(self, message: str) -> None
â‹®----
"""Log error message."""
â‹®----
class BaseWorkflow(ABC)
â‹®----
"""
    Base class for workflow implementations.

    Provides template for workflow execution with standard lifecycle:
    1. Validate preconditions
    2. Execute workflow logic
    3. Handle errors and cleanup
    4. Report results

    Subclasses implement _execute_impl() with specific business logic.
    """
â‹®----
def __init__(self)
â‹®----
"""Initialize base workflow."""
â‹®----
async def execute(self, context: IWorkflowContext) -> dict[str, Any]
â‹®----
"""
        Execute the workflow with standard lifecycle.

        Args:
            context: Workflow execution context

        Returns:
            Execution result dictionary

        Raises:
            WorkflowError: If workflow execution fails critically
        """
â‹®----
# 1. Validate preconditions
â‹®----
error_msg = f"Workflow validation failed: {errors}"
â‹®----
# 2. Execute workflow logic
â‹®----
result = await self._execute_impl(context)
â‹®----
duration = (self._end_time - self._start_time).total_seconds()
â‹®----
error_msg = f"Workflow execution failed: {str(e)}"
â‹®----
async def validate(self, context: IWorkflowContext) -> tuple[bool, list[str]]
â‹®----
"""
        Validate workflow preconditions.

        Default implementation always returns valid.
        Override in subclasses for custom validation.

        Args:
            context: Workflow execution context

        Returns:
            Tuple of (is_valid, error_messages)
        """
â‹®----
@abstractmethod
    async def _execute_impl(self, context: IWorkflowContext) -> dict[str, Any]
â‹®----
"""
        Execute workflow-specific logic.

        Subclasses must implement this method with their business logic.

        Args:
            context: Workflow execution context

        Returns:
            Execution result dictionary

        Raises:
            WorkflowError: If execution fails
        """
â‹®----
def get_status(self) -> WorkflowStatus
â‹®----
"""Get current workflow status."""
â‹®----
"""Create a failure result dictionary."""
</file>

<file path="orchestration/workflows/incremental_workflow.py">
logger = logging.getLogger(__name__)
â‹®----
@dataclass
class IncrementalRequest
â‹®----
symbols: list[str]
timeframes: list[str]
data_types: list[str]
source: str = "ccxt"
exchanges: list[str] | None = None
lookback_hours: int = 24
â‹®----
class IncrementalWorkflow(BaseWorkflow)
â‹®----
async def validate(self, context: IWorkflowContext) -> tuple[bool, list[str]]
â‹®----
errors = []
â‹®----
request = context.get_parameter("request")
â‹®----
required_fields = ["symbols", "timeframes", "data_types"]
â‹®----
async def _execute_impl(self, context: IWorkflowContext) -> dict[str, Any]
â‹®----
"""
        Execute incremental update workflow.

        Args:
            context: Workflow execution context

        Returns:
            Execution result dictionary
        """
# Get request from context
request_data = context.get_parameter("request")
â‹®----
request = IncrementalRequest(**request_data)
â‹®----
request = request_data
â‹®----
results = {}
total_records = 0
â‹®----
exchanges = request.exchanges
â‹®----
exchanges = ["binance"]
â‹®----
key = f"{symbol}/{timeframe}/{data_type}/{exchange}"
â‹®----
records = await self._execute_single_update(
â‹®----
error_msg = f"Incremental update failed for {key}: {str(e)}"
â‹®----
successful = sum(1 for r in results.values() if r.get("success"))
â‹®----
# Load checkpoint
checkpoint_data = await self.checkpoint_store.load_checkpoint(
â‹®----
# Determine start timestamp
â‹®----
since = checkpoint_data["last_timestamp"]
â‹®----
# No checkpoint, fetch recent data
since = int(
â‹®----
raw_data = None
last_error = None
â‹®----
raw_data = await self.data_adapter.fetch_ohlcv(
â‹®----
raw_data = await self.data_adapter.fetch_oi(
â‹®----
last_error = e
â‹®----
# Write to bronze
result = await self.bronze_writer.write_daily_batch(
â‹®----
records_written = result.get("records_written", len(raw_data))
â‹®----
last_ts = raw_data[-1].get("timestamp", since)
â‹®----
last_ts = raw_data[-1][0]
â‹®----
last_ts = since
</file>

<file path="orchestration/__init__.py">
__all__ = [
â‹®----
__version__ = "1.0.0"
</file>

<file path="orchestration/ports.py">
T = TypeVar("T")
â‹®----
class WorkflowStatus(str, Enum)
â‹®----
PENDING = "pending"
RUNNING = "running"
SUCCESS = "success"
FAILED = "failed"
CANCELLED = "cancelled"
RETRYING = "retrying"
â‹®----
class CheckpointType(str, Enum)
â‹®----
MINIO = "minio"
DATABASE = "database"
HYBRID = "hybrid"
â‹®----
@runtime_checkable
class IWorkflowContext(Protocol)
â‹®----
@property
    def workflow_id(self) -> str
â‹®----
@property
    def execution_date(self) -> datetime
â‹®----
@property
    def params(self) -> dict[str, Any]
â‹®----
def get_parameter(self, key: str, default: Any = None) -> Any
â‹®----
def set_state(self, key: str, value: Any) -> None
â‹®----
def get_state(self, key: str, task_id: str | None = None) -> Any
â‹®----
def log_info(self, message: str) -> None
â‹®----
def log_warning(self, message: str) -> None
â‹®----
def log_error(self, message: str) -> None
â‹®----
@runtime_checkable
class IWorkflow(Protocol)
â‹®----
async def execute(self, context: IWorkflowContext) -> dict[str, Any]
â‹®----
async def validate(self, context: IWorkflowContext) -> tuple[bool, list[str]]
â‹®----
def get_status(self) -> WorkflowStatus
â‹®----
@runtime_checkable
class IOperator(Protocol)
â‹®----
def execute(self, context: dict[str, Any]) -> Any
â‹®----
@runtime_checkable
class ICheckpointStore(Protocol)
â‹®----
@runtime_checkable
class IChunkStrategy(Protocol)
â‹®----
@runtime_checkable
class IRateLimiter(Protocol)
â‹®----
async def acquire(self) -> None
â‹®----
async def record_success(self) -> None
â‹®----
async def record_error(self, error_code: int | None = None) -> None
â‹®----
def get_current_rate(self) -> float
â‹®----
@runtime_checkable
class IProgressReporter(Protocol)
â‹®----
def report_start(self, total_items: int) -> None
â‹®----
def report_progress(self, completed_items: int, message: str | None = None) -> None
â‹®----
def report_completion(self, success: bool, summary: dict[str, Any]) -> None
â‹®----
@runtime_checkable
class IDagBuilder(Protocol)
â‹®----
def build(self) -> Any
â‹®----
def validate(self) -> tuple[bool, list[str]]
â‹®----
class WorkflowError(Exception)
â‹®----
class CheckpointError(Exception)
â‹®----
class OperatorError(Exception)
â‹®----
class ValidationError(Exception)
â‹®----
__all__ = [
</file>

<file path="orchestration/README.md">
# Phase 4: Orchestration Layer - README

## Overview

The orchestration layer provides high-level coordination of data pipelines, abstracting workflow execution, operator implementations, and DAG construction from business logic in the ingestion and transformation layers.

## Architecture Philosophy

### Layer Responsibilities

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Airflow Scheduler                        â”‚
â”‚                  (Execution Engine)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ executes
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Orchestration Layer (HERE)                   â”‚
â”‚                                                             â”‚
â”‚  Coordinates: What to run, when, and how                   â”‚
â”‚  â€¢ Workflows: Business process coordination                â”‚
â”‚  â€¢ Operators: Airflow task wrappers                        â”‚
â”‚  â€¢ DAG Builders: Programmatic DAG construction             â”‚
â”‚  â€¢ Tasks: Reusable task functions                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ uses
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Ingestion Layer                           â”‚
â”‚                                                             â”‚
â”‚  Executes: Data fetching and initial processing            â”‚
â”‚  â€¢ Adapters: API clients                                   â”‚
â”‚  â€¢ Processors: Data transformation                          â”‚
â”‚  â€¢ Normalizers: Format conversion                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ uses
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Infrastructure Layer                         â”‚
â”‚                                                             â”‚
â”‚  Provides: Shared utilities and resources                  â”‚
â”‚  â€¢ Database: Connection pooling                            â”‚
â”‚  â€¢ Checkpoint: State management                             â”‚
â”‚  â€¢ Config: Settings management                              â”‚
â”‚  â€¢ Logging: Structured logging                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Principles

1. **Orchestration Coordinates, Doesn't Execute**
   - Orchestration layer defines *what* and *when*
   - Ingestion/transformation layers define *how*
   - Infrastructure provides *resources*

2. **Protocol-Based Interfaces**
   - All dependencies expressed as protocols
   - Enables testing without Airflow scheduler
   - Decouples from concrete implementations

3. **Airflow Independence**
   - Core workflow logic doesn't depend on Airflow
   - Operators and DAG builders handle Airflow-specific features
   - Can run workflows outside Airflow for testing

## Directory Structure

```
orchestration/
â”œâ”€â”€ __init__.py                  # Layer documentation and exports
â”œâ”€â”€ ports.py                     # Protocol definitions (392 lines)
â”œâ”€â”€ README.md                    # This file
â”‚
â”œâ”€â”€ workflows/                   # Workflow coordination
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                 # BaseWorkflow, AirflowWorkflowContext
â”‚   â”œâ”€â”€ backfill_workflow.py    # Historical data backfill
â”‚   â”œâ”€â”€ incremental_workflow.py # Real-time incremental updates
â”‚   â””â”€â”€ validation_workflow.py  # Cross-source validation
â”‚
â”œâ”€â”€ operators/                   # Airflow operators
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                 # BaseDataOperator
â”‚   â”œâ”€â”€ fetch_operators.py      # FetchOHLCVOperator, FetchOIOperator
â”‚   â”œâ”€â”€ write_operators.py      # WriteBronzeOperator, WriteSilverOperator
â”‚   â”œâ”€â”€ validation_operators.py # ValidationOperator, GapDetector
â”‚   â””â”€â”€ checkpoint_operators.py # CheckpointReader, CheckpointWriter
â”‚
â”œâ”€â”€ dag_builders/                # DAG construction
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                 # BaseDagBuilder
â”‚   â”œâ”€â”€ backfill_dag_builder.py # Backfill DAG generation
â”‚   â”œâ”€â”€ incremental_dag_builder.py # Incremental DAG generation
â”‚   â””â”€â”€ factory.py              # Config-driven DAG factory
â”‚
â”œâ”€â”€ tasks/                       # Task function library
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ fetch_tasks.py          # Generic fetch operations
â”‚   â”œâ”€â”€ write_tasks.py          # Generic write operations
â”‚   â”œâ”€â”€ validation_tasks.py     # Validation operations
â”‚   â””â”€â”€ checkpoint_tasks.py     # Checkpoint management
â”‚
â””â”€â”€ checkpoint/                  # Unified checkpoint abstraction
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ base.py                 # Base checkpoint implementations
    â”œâ”€â”€ minio_store.py          # MinIO-backed checkpoint store
    â”œâ”€â”€ database_store.py       # PostgreSQL-backed checkpoint store
    â””â”€â”€ hybrid_store.py         # Hybrid MinIO + Database store
```

## Module Purposes

### ports.py - Protocol Definitions

Defines contracts for all orchestration abstractions:

- **IWorkflow**: Workflow execution interface
- **IWorkflowContext**: Execution context abstraction (wraps Airflow context)
- **IOperator**: Operator execution interface
- **ICheckpointStore**: Unified checkpoint storage
- **IChunkStrategy**: Time-range chunking for backfills
- **IRateLimiter**: API rate limiting
- **IProgressReporter**: Progress tracking and reporting
- **IDagBuilder**: Programmatic DAG construction

### workflows/ - Business Logic Coordination

Coordinates multi-step business processes:

- **BaseWorkflow**: Template for workflow execution (validate â†’ execute â†’ report)
- **BackfillWorkflow**: Historical data backfill with chunking and checkpoints
- **IncrementalWorkflow**: Real-time incremental updates with state management
- **ValidationWorkflow**: Cross-source data validation and reconciliation

### operators/ - Airflow Task Wrappers

Reusable Airflow operators for common operations:

- **BaseDataOperator**: Base class with error handling and logging
- **FetchOHLCVOperator**: Fetch OHLCV data from adapter
- **FetchOIOperator**: Fetch open interest data
- **WriteBronzeOperator**: Write to MinIO bronze layer
- **ValidationOperator**: Validate data quality
- **CheckpointReader/Writer**: Checkpoint state management

### dag_builders/ - DAG Construction

Programmatic DAG generation:

- **BaseDagBuilder**: Template for DAG construction
- **BackfillDagBuilder**: Generate backfill DAGs from config
- **IncrementalDagBuilder**: Generate incremental update DAGs
- **DagFactory**: Config-driven DAG factory (replaces universal_dag_factory)

### tasks/ - Task Function Library

Reusable task functions (TaskFlow API compatible):

- **fetch_tasks**: Generic fetch operations with retry logic
- **write_tasks**: Generic write operations with validation
- **validation_tasks**: Data quality validation
- **checkpoint_tasks**: Checkpoint read/write/delete operations

### checkpoint/ - Unified Checkpoint Abstraction

Checkpoint storage implementations:

- **MinIOCheckpointStore**: File-based checkpoints in MinIO/S3
- **DatabaseCheckpointStore**: PostgreSQL-backed checkpoints
- **HybridCheckpointStore**: Dual storage (file + database)

## Design Patterns

### 1. Template Method (Workflows)

Base workflow provides algorithm structure:

```python
class BaseWorkflow:
    async def execute(self, context: IWorkflowContext):
        # 1. Validate
        is_valid, errors = await self.validate(context)
        if not is_valid:
            return {"status": "failed", "errors": errors}
        
        # 2. Execute (subclass implements)
        result = await self._execute_impl(context)
        
        # 3. Report
        await self._report_results(result, context)
        
        return result
    
    async def _execute_impl(self, context):
        raise NotImplementedError
```

### 2. Dependency Injection (Protocols)

All dependencies expressed as protocols:

```python
class BackfillWorkflow:
    def __init__(
        self,
        adapter: IDataAdapter,  # Protocol
        writer: IDataWriter,    # Protocol
        checkpoint_store: ICheckpointStore,  # Protocol
        rate_limiter: IRateLimiter | None = None,
    ):
        self.adapter = adapter
        self.writer = writer
        self.checkpoint_store = checkpoint_store
        self.rate_limiter = rate_limiter
```

### 3. Strategy Pattern (Chunk Strategy)

Pluggable chunking strategies:

```python
class AdaptiveChunkStrategy:
    def generate_chunks(self, start, end, timeframe):
        # Small chunks for 1m data, large for 1d
        ...

class FixedChunkStrategy:
    def generate_chunks(self, start, end, timeframe):
        # Fixed-size chunks regardless of timeframe
        ...
```

### 4. Builder Pattern (DAG Construction)

Fluent DAG construction:

```python
dag = (BackfillDagBuilder("backfill_btc")
    .with_symbols(["BTC", "ETH"])
    .with_timeframes(["1h", "4h"])
    .with_source("coinalyze")
    .with_schedule(None)  # Manual trigger
    .build())
```

## Usage Examples

### Example 1: Backfill Workflow

```python
from quant_framework.orchestration.workflows import BackfillWorkflow
from quant_framework.orchestration.checkpoint import DatabaseCheckpointStore

# Create workflow
workflow = BackfillWorkflow(
    adapter=coinalyze_adapter,
    writer=bronze_writer,
    checkpoint_store=DatabaseCheckpointStore(db),
    chunk_strategy=AdaptiveChunkStrategy(),
    rate_limiter=CoinalyzeRateLimiter(),
)

# Execute
context = AirflowWorkflowContext(ti=task_instance)
result = await workflow.execute(context)

print(f"Status: {result['status']}")
print(f"Records: {result['records_written']}")
print(f"Duration: {result['duration_seconds']}s")
```

### Example 2: Custom Operator

```python
from quant_framework.orchestration.operators import FetchOHLCVOperator

class FetchBTCOHLCV(FetchOHLCVOperator):
    def __init__(self, **kwargs):
        super().__init__(
            task_id="fetch_btc_ohlcv",
            adapter=ccxt_adapter,
            symbol="BTC/USDT",
            timeframe="1h",
            **kwargs
        )

# Use in DAG
with DAG("btc_ingestion") as dag:
    fetch = FetchBTCOHLCV()
    write = WriteBronzeOperator(task_id="write_bronze")
    fetch >> write
```

### Example 3: DAG Builder

```python
from quant_framework.orchestration.dag_builders import BackfillDagBuilder

builder = BackfillDagBuilder(
    dag_id="coinalyze_backfill",
    description="Historical OHLCV backfill from Coinalyze",
    schedule=None,
    symbols=["BTC", "ETH", "SOL"],
    timeframes=["1h", "4h", "1d"],
    source="coinalyze",
    data_types=["ohlcv", "open_interest"],
)

# Validate configuration
is_valid, errors = builder.validate()
if not is_valid:
    print(f"Invalid config: {errors}")
else:
    dag = builder.build()
```

## Migration from Phase 3

### Old Structure (Ingestion Layer)

```python
# Before: Orchestration logic in ingestion
from quant_framework.ingestion.orchestration.backfill.coordinator import BackfillCoordinator

coordinator = BackfillCoordinator(
    adapter=adapter,
    checkpoint_manager=checkpoint_manager,
    # ...
)
```

### New Structure (Orchestration Layer)

```python
# After: Orchestration logic in orchestration layer
from quant_framework.orchestration.workflows import BackfillWorkflow
from quant_framework.orchestration.checkpoint import DatabaseCheckpointStore

workflow = BackfillWorkflow(
    adapter=adapter,
    checkpoint_store=DatabaseCheckpointStore(db),
    # ...
)
```

### Backward Compatibility

Old imports still work with deprecation warnings:

```python
# Deprecated but functional
from quant_framework.ingestion.orchestration.backfill.coordinator import BackfillCoordinator
# DeprecationWarning: Moved to quant_framework.orchestration.workflows.BackfillWorkflow
```

## Testing Strategy

### Unit Tests

Test workflows without Airflow:

```python
# Mock context
class MockWorkflowContext:
    def __init__(self):
        self.state = {}
    
    def set_state(self, key, value):
        self.state[key] = value
    
    def get_state(self, key, task_id=None):
        return self.state.get(key)

# Test workflow
context = MockWorkflowContext()
workflow = BackfillWorkflow(...)
result = await workflow.execute(context)
assert result["status"] == "success"
```

### Integration Tests

Test operators with Airflow:

```python
from airflow.utils.state import State
from airflow.models import TaskInstance

def test_fetch_operator():
    op = FetchOHLCVOperator(...)
    ti = TaskInstance(task=op.task, execution_date=datetime.now())
    op.execute({"ti": ti})
    assert ti.state == State.SUCCESS
```

### DAG Validation Tests

Validate generated DAGs:

```python
from airflow.models import DagBag

def test_dag_builder():
    builder = BackfillDagBuilder(...)
    dag = builder.build()
    
    # Validate with Airflow
    dag_bag = DagBag()
    dag_bag.bag_dag(dag, root_dag=dag)
    assert len(dag_bag.import_errors) == 0
```

## Performance Considerations

### Checkpoint Strategy Selection

- **MinIO (file-based)**: Fast writes, eventual consistency, good for streaming
- **Database (PostgreSQL)**: ACID guarantees, immediate consistency, good for backfills
- **Hybrid**: Write to both, read from database, best for critical workflows

### Rate Limiting

Adaptive rate limiting adjusts chunk size based on API responses:

```python
rate_limiter = CoinalyzeRateLimiter(
    base_rate=10,  # 10 req/sec baseline
    max_rate=50,   # 50 req/sec max
    min_rate=1,    # 1 req/sec under rate limit
    backoff_factor=2,  # Exponential backoff
)
```

### Chunking Strategy

Chunk sizes impact memory and failure recovery:

- **1m timeframe**: Small chunks (1 day) for fast recovery
- **1h timeframe**: Medium chunks (7 days)
- **1d timeframe**: Large chunks (30 days)

## Next Steps

1. **Implement workflows/** - Create base workflow and concrete implementations
2. **Implement operators/** - Create Airflow operators
3. **Implement dag_builders/** - Create DAG builders
4. **Implement tasks/** - Extract task functions from DAG common files
5. **Implement checkpoint/** - Create unified checkpoint stores
6. **Migrate DAGs** - Refactor existing DAGs to use new orchestration layer
7. **Write tests** - Comprehensive unit and integration tests
8. **Documentation** - Update DAG documentation and migration guide

---

**Phase 4 Status**: Foundation created, implementation in progress
</file>

<file path="shared/enums/__init__.py">
__all__ = ["AssetClass", "Exchange", "MarketDataType", "MarketType"]
</file>

<file path="shared/models/__init__.py">
__all__ = [
</file>

<file path="shared/models/enums.py">
class AssetClass(str, enum.Enum)
â‹®----
CRYPTO = "crypto"
EQUITY = "equity"
FUTURE = "future"
OPTION = "option"
FOREX = "forex"
COMMODITY = "commodity"
RATES = "rates"
INDEX = "index"
OTHER = "other"
â‹®----
class ContractType(str, enum.Enum)
â‹®----
LINEAR = "linear"
INVERSE = "inverse"
â‹®----
class CandleType(str, enum.Enum)
â‹®----
SPOT = "spot"
FUTURES = "futures"
MARK = "mark"
â‹®----
FUNDING_RATE = "funding_rate"
â‹®----
class PriceType(str, enum.Enum)
â‹®----
LAST = "last"
â‹®----
class InstrumentType(str, enum.Enum)
â‹®----
PERPETUAL = "perpetual"
â‹®----
CALL = "call"
PUT = "put"
STOCK = "stock"
PAIR = "pair"
â‹®----
class MarketType(str, enum.Enum)
â‹®----
LINEAR_PERPETUAL = "linear_perpetual"
INVERSE_PERPETUAL = "inverse_perpetual"
LINEAR_FUTURE = "linear_future"
INVERSE_FUTURE = "inverse_future"
â‹®----
class OrderType(str, enum.Enum)
â‹®----
LIMIT = "limit"
MARKET = "market"
STOP_LOSS = "stop_loss"
STOP_LOSS_LIMIT = "stop_loss_limit"
TAKE_PROFIT = "take_profit"
TAKE_PROFIT_LIMIT = "take_profit_limit"
â‹®----
class MarketDataType(str, enum.Enum)
â‹®----
OHLC = "ohlc"
OHLCV = "ohlcv"
OPEN_INTEREST = "open_interest"
â‹®----
LIQUIDATION = "liquidation"
ORDERBOOK = "orderbook"
TRADES = "trades"
TICKER = "ticker"
â‹®----
class DataVenue(str, enum.Enum)
â‹®----
BINANCE = "binance"
BINANCE_USDM = "binance_usdm"
BINANCE_COINM = "binance_coinm"
â‹®----
BYBIT = "bybit"
GATEIO = "gateio"
HUOBI = "huobi"
BITGET = "bitget"
OKX = "okx"
KRAKEN = "kraken"
COINBASE = "coinbase"
DERIBIT = "deribit"
â‹®----
NYSE = "nyse"
NASDAQ = "nasdaq"
LSE = "lse"
â‹®----
CME = "cme"
CBOE = "cboe"
ICE_FUTURES = "ice_futures"
EUREX = "eurex"
â‹®----
BLOOMBERG = "bloomberg"
REFINITIV = "refinitiv"
ICE_DATA = "ice_data"
FACTSET = "factset"
â‹®----
OANDA = "oanda"
FXCM = "fxcm"
â‹®----
AGGREGATED = "aggregated"
INTERNAL = "internal"
FILE = "file"
UNKNOWN = "unknown"
â‹®----
COINMETRICS = "coinmetrics"
â‹®----
class WrapperImplementation(str, enum.Enum)
â‹®----
CCXT = "ccxt"
COINALYZE = "coinalyze"
â‹®----
YAHOO_FINANCE = "yahoo_finance"
ALPHA_VANTAGE = "alpha_vantage"
POLYGON = "polygon"
TWELVE_DATA = "twelve_data"
FINNHUB = "finnhub"
â‹®----
ETHERSCAN = "etherscan"
ALCHEMY = "alchemy"
INFURA = "infura"
THEGRAPH = "thegraph"
â‹®----
IB_INSYNC = "ib_insync"
ALPACA = "alpaca"
ROBINHOOD = "robinhood"
â‹®----
BLOOMBERG_SDK = "bloomberg_sdk"
REFINITIV_SDK = "refinitiv_sdk"
â‹®----
NONE = "none"
â‹®----
class ClientType(str, enum.Enum)
â‹®----
WRAPPER = "wrapper"
NATIVE = "native"
VENDOR_SDK = "vendor_sdk"
FILE_PARSER = "file_parser"
â‹®----
class ConnectionType(str, enum.Enum)
â‹®----
REST = "rest"
WEBSOCKET = "websocket"
FIX = "fix"
GRPC = "grpc"
SFTP = "sftp"
â‹®----
DATABASE = "database"
â‹®----
class PipelineMode(str, enum.Enum)
â‹®----
BACKFILL = "backfill"
INCREMENTAL = "incremental"
STREAMING = "streaming"
â‹®----
class IngestionStatus(str, enum.Enum)
â‹®----
PENDING = "pending"
RUNNING = "running"
SUCCESS = "success"
FAILED = "failed"
PARTIAL = "partial"
SKIPPED = "skipped"
â‹®----
class Exchange(str, enum.Enum)
â‹®----
BINANCEUSDM = "binance_usdm"
BINANCECOINM = "binance_coinm"
â‹®----
@classmethod
    def _missing_(cls, value)
â‹®----
value_lower = value.lower()
â‹®----
DataProvider = DataVenue
</file>

<file path="shared/models/instruments.py">
class Instrument(BaseModel)
â‹®----
instrument_id: str = Field(..., description="Unique identifier")
asset_class: AssetClass
market_type: MarketType
venue: DataVenue
â‹®----
base_asset: str
quote_asset: str
â‹®----
wrapper: WrapperImplementation = Field(
â‹®----
contract_type: str | None = Field(default=None)
contract_size: Decimal | None = Field(default=None)
tick_size: Decimal | None = Field(default=None)
lot_size: Decimal | None = Field(default=None)
â‹®----
listing_date: datetime | None = Field(default=None)
expiry_date: datetime | None = Field(default=None)
is_active: bool = Field(default=True)
â‹®----
is_inverse: bool = Field(default=False)
â‹®----
raw_symbol: str = Field(default="")
metadata: dict[str, Any] | None = Field(default_factory=dict)
â‹®----
# ========== CALCULATED FIELD (Not in __init__) ==========
settlement_currency: str | None = Field(default=None)
â‹®----
# ==================== VALIDATORS ====================
â‹®----
@validator("raw_symbol", pre=True, always=True)
    def set_raw_symbol(cls, v, values)
â‹®----
@validator("metadata", pre=True, always=True)
    def set_metadata(cls, v)
â‹®----
@root_validator(skip_on_failure=True)
    def calculate_settlement_currency(cls, values)
â‹®----
settlement = values.get("settlement_currency")
â‹®----
is_inverse = values.get("is_inverse", False)
base_asset = values.get("base_asset")
quote_asset = values.get("quote_asset")
â‹®----
@property
    def symbol(self) -> str
â‹®----
@property
    def full_symbol(self) -> str
â‹®----
@property
    def data_lineage(self) -> str
â‹®----
"""Human-readable data lineage string"""
â‹®----
@property
    def is_derivative(self) -> bool
â‹®----
"""Check if instrument is a derivative"""
â‹®----
@property
    def is_perpetual(self) -> bool
â‹®----
"""Check if instrument is a perpetual swap"""
â‹®----
# ==================== SERIALIZATION ====================
â‹®----
def to_dict(self) -> dict[str, Any]
â‹®----
"""Convert to dictionary (Pydantic does this natively)"""
â‹®----
def to_json(self) -> str
â‹®----
"""Convert to JSON string"""
â‹®----
class Config
â‹®----
"""Pydantic configuration"""
â‹®----
validate_assignment = True
arbitrary_types_allowed = True
</file>

<file path="shared/__init__.py">

</file>

<file path="storage/adapters/__init__.py">
__all__ = [
</file>

<file path="storage/adapters/postgres.py">
__all__ = ["PostgresAdapter"]
â‹®----
PostgresAdapter = DatabaseAdapter
</file>

<file path="storage/migrations/README.md">
"""Storage layer migrations and schema versioning.

This directory is reserved for database schema migrations using Alembic.

Current Status: No migrations yet
- Existing database schema is versioned manually in db/schemas/
- All tables are pre-created and stable
- New storage layer repositories work with existing schema

Migration Strategy:
1. Use Alembic for future schema changes
2. Keep migrations organized by phase/feature
3. Always run migrations in staging before production
4. Document breaking changes in MIGRATION_NOTES.md

Future: When Alembic is integrated, migrations will live here with:
- versions/001_initial_schema.py
- versions/002_add_settlement_currency.py
- env.py (Alembic configuration)
- script.py.mako (migration template)
"""
</file>

<file path="storage/repositories/__init__.py">
__all__ = [
</file>

<file path="storage/repositories/checkpoint.py">
logger = logging.getLogger(__name__)
â‹®----
@dataclass
class BackfillCheckpointRecord
â‹®----
symbol: str
timeframe: str
data_type: str
source: str
last_completed_date: datetime
total_chunks_completed: int
total_records_written: int
started_at: datetime
updated_at: datetime
status: str
error_message: str | None = None
id: int | None = None
â‹®----
class CheckpointRepository
â‹®----
def __init__(self, db: DatabaseAdapter)
â‹®----
query = """
â‹®----
result = await self.db.fetchval(
â‹®----
async def update(self, checkpoint: BackfillCheckpointRecord) -> None
â‹®----
"""Update existing checkpoint record.

        Args:
            checkpoint: Checkpoint data to update (must have id)

        Raises:
            ValueError: If checkpoint has no id
            NotFoundError: If checkpoint doesn't exist
        """
â‹®----
"""Get checkpoint for specific symbol/timeframe/source combination.

        Args:
            symbol: Trading symbol
            timeframe: Data timeframe
            data_type: Type of data
            source: Data source name

        Returns:
            Checkpoint record if exists, None otherwise
        """
â‹®----
row = await self.db.fetchrow(query, symbol, timeframe, data_type, source)
â‹®----
"""Get all checkpoints, optionally filtered by status.

        Args:
            status: Filter by status (in_progress, completed, failed) or None for all

        Returns:
            List of checkpoint records
        """
â‹®----
rows = await self.db.fetch(query, status)
â‹®----
rows = await self.db.fetch(query)
â‹®----
checkpoints = []
â‹®----
async def delete(self, checkpoint_id: int) -> bool
â‹®----
"""Delete checkpoint by id.

        Args:
            checkpoint_id: Checkpoint id to delete

        Returns:
            True if deleted, False if not found
        """
query = "DELETE FROM system.backfill_checkpoints WHERE id = $1"
â‹®----
result = await self.db.execute(query, checkpoint_id)
</file>

<file path="storage/repositories/metadata.py">
logger = logging.getLogger(__name__)
â‹®----
class InstrumentRepository
â‹®----
def __init__(self, db: DatabaseAdapter)
â‹®----
async def create(self, instrument: Instrument) -> Instrument
â‹®----
query = """
â‹®----
now = datetime.utcnow()
result = await self.db.fetchval(
â‹®----
async def find_by_symbol(self, symbol: str) -> Instrument | None
â‹®----
"""Get instrument by canonical symbol.

        Args:
            symbol: Trading symbol (e.g., 'BTC', 'ETH')

        Returns:
            Instrument record if exists, None otherwise
        """
â‹®----
row = await self.db.fetchrow(query, symbol)
â‹®----
async def find_all_major(self) -> list[Instrument]
â‹®----
"""Get all major trading instruments.

        Major instruments are primary assets like BTC, ETH, etc.

        Returns:
            List of major Instrument records
        """
â‹®----
rows = await self.db.fetch(query)
â‹®----
instruments = [
â‹®----
async def update(self, instrument: Instrument) -> None
â‹®----
"""Update existing instrument record.

        Args:
            instrument: Instrument data to update (must have instrument_id)

        Raises:
            ValueError: If instrument has no id
        """
â‹®----
class ExchangeRepository
â‹®----
"""Repository for exchange/venue reference data.

    Provides CRUD operations for exchanges where trading occurs.
    """
â‹®----
"""Initialize exchange repository.

        Args:
            db: DatabaseAdapter instance for SQL execution
        """
â‹®----
async def create(self, exchange: Exchange) -> Exchange
â‹®----
async def find_by_name(self, name: str) -> Exchange | None
â‹®----
"""Get exchange by name.

        Args:
            name: Exchange name (e.g., 'binance', 'coinalyze')

        Returns:
            Exchange record if exists, None otherwise
        """
â‹®----
row = await self.db.fetchrow(query, name)
â‹®----
async def find_all_active(self) -> list[Exchange]
â‹®----
"""Get all active exchanges.

        Returns:
            List of active Exchange records
        """
â‹®----
exchanges = [
â‹®----
class MarketTypeRepository
â‹®----
"""Repository for market type reference data.

    Provides CRUD operations for market type categorization.
    """
â‹®----
"""Initialize market type repository.

        Args:
            db: DatabaseAdapter instance for SQL execution
        """
â‹®----
async def create(self, market: Market) -> Market
â‹®----
async def find_by_type(self, market_type: str) -> Market | None
â‹®----
"""Get market type by identifier.

        Args:
            market_type: Market type identifier (e.g., 'spot', 'linear_perpetual')

        Returns:
            Market record if exists, None otherwise
        """
â‹®----
row = await self.db.fetchrow(query, market_type)
â‹®----
async def find_all_with_oi(self) -> list[Market]
â‹®----
"""Get all market types that support open interest.

        Returns:
            List of Market records where has_oi = true
        """
â‹®----
markets = [
</file>

<file path="storage/repositories/ohlcv.py">
logger = logging.getLogger(__name__)
â‹®----
class OHLCVRepository
â‹®----
def __init__(self, db: DatabaseAdapter)
â‹®----
async def save(self, record: OHLCVRecord) -> OHLCVRecord
â‹®----
query = """
â‹®----
placeholders = []
values_flat = []
â‹®----
base_idx = idx * 11
â‹®----
conflict_clause = (
â‹®----
query = f"""
â‹®----
result = await self.db.execute(query, *values_flat)
â‹®----
"""Get most recent OHLCV records for a symbol/timeframe.

        Args:
            symbol: Trading symbol
            exchange: Exchange name
            timeframe: Timeframe (5m, 1h, etc)
            limit: Number of records to return (max 10000)

        Returns:
            List of OHLCVRecord sorted by time DESC (newest first)
        """
â‹®----
rows = await self.db.fetch(query, symbol, exchange, timeframe, limit)
â‹®----
records = [
â‹®----
"""Get OHLCV records within time range.

        Args:
            symbol: Trading symbol
            exchange: Exchange name
            timeframe: Timeframe
            start_time: Start timestamp (inclusive)
            end_time: End timestamp (inclusive)

        Returns:
            List of OHLCVRecord in time order (ASC)
        """
â‹®----
rows = await self.db.fetch(
â‹®----
async def count(self, symbol: str, exchange: str, timeframe: str) -> int
â‹®----
"""Get total record count for symbol/timeframe.

        Args:
            symbol: Trading symbol
            exchange: Exchange name
            timeframe: Timeframe

        Returns:
            Total record count
        """
â‹®----
result = await self.db.fetchval(query, symbol, exchange, timeframe)
</file>

<file path="storage/repositories/open_interest.py">
logger = logging.getLogger(__name__)
â‹®----
class OpenInterestRepository
â‹®----
def __init__(self, db: DatabaseAdapter)
â‹®----
async def save(self, record: OpenInterestRecord) -> OpenInterestRecord
â‹®----
query = """
â‹®----
placeholders = []
values_flat = []
â‹®----
base_idx = idx * 9
â‹®----
conflict_clause = (
â‹®----
query = f"""
â‹®----
result = await self.db.execute(query, *values_flat)
â‹®----
"""Get most recent Open Interest records for a symbol/timeframe.

        Args:
            symbol: Trading symbol
            exchange: Exchange name
            timeframe: Timeframe (5m, 1h, etc)
            limit: Number of records to return (max 10000)

        Returns:
            List of OpenInterestRecord sorted by time DESC (newest first)
        """
â‹®----
rows = await self.db.fetch(query, symbol, exchange, timeframe, limit)
â‹®----
records = [
â‹®----
"""Get Open Interest records within time range.

        Args:
            symbol: Trading symbol
            exchange: Exchange name
            timeframe: Timeframe
            start_time: Start timestamp (inclusive)
            end_time: End timestamp (inclusive)

        Returns:
            List of OpenInterestRecord in time order (ASC)
        """
â‹®----
rows = await self.db.fetch(
â‹®----
async def count(self, symbol: str, exchange: str, timeframe: str) -> int
â‹®----
"""Get total record count for symbol/timeframe.

        Args:
            symbol: Trading symbol
            exchange: Exchange name
            timeframe: Timeframe

        Returns:
            Total record count
        """
â‹®----
result = await self.db.fetchval(query, symbol, exchange, timeframe)
</file>

<file path="storage/schemas/__init__.py">
__all__ = [
</file>

<file path="storage/schemas/relational.py">
class Exchange(BaseModel)
â‹®----
exchange_id: int | None = Field(
name: str = Field(
display_name: str = Field(
â‹®----
is_cex: bool = Field(
is_active: bool = Field(True, description="Whether exchange is actively monitored")
â‹®----
country: str | None = Field(None, description="Country of origin (optional)")
website: str | None = Field(None, description="Official website URL (optional)")
api_documentation: str | None = Field(None, description="API docs URL (optional)")
â‹®----
created_at: datetime | None = Field(None, description="Record creation timestamp")
updated_at: datetime | None = Field(None, description="Record update timestamp")
â‹®----
class Config
â‹®----
json_encoders = {
â‹®----
class Instrument(BaseModel)
â‹®----
instrument_id: int | None = Field(
symbol: str = Field(
asset_type: str = Field(
â‹®----
name: str | None = Field(
decimals: int | None = Field(
â‹®----
coingecko_id: str | None = Field(None, description="CoinGecko API identifier")
coinalyze_symbol: str | None = Field(
ccxt_symbol_binance: str | None = Field(
â‹®----
is_active: bool = Field(True, description="Whether instrument is actively tracked")
is_major: bool = Field(
â‹®----
class Market(BaseModel)
â‹®----
market_type_id: int | None = Field(
market_type: str = Field(
â‹®----
description: str | None = Field(
â‹®----
has_oi: bool = Field(
is_perpetual: bool = Field(
is_leveraged: bool = Field(
</file>

<file path="storage/schemas/time_series.py">
class OHLCVRecord(BaseModel)
â‹®----
timestamp: datetime = Field(..., description="Candle close time (UTC)")
symbol: str = Field(
exchange: str = Field(
timeframe: str = Field(
â‹®----
open_price: Decimal = Field(
high_price: Decimal = Field(
low_price: Decimal = Field(
close_price: Decimal = Field(
volume: Decimal = Field(
â‹®----
quote_volume: Decimal | None = Field(
trades_count: int | None = Field(
â‹®----
class Config
â‹®----
json_encoders = {
â‹®----
class TradeRecord(BaseModel)
â‹®----
timestamp: datetime = Field(..., description="Trade execution time (UTC)")
â‹®----
exchange: str = Field(..., min_length=1, description="Exchange name")
â‹®----
price: Decimal = Field(..., decimal_places=8, description="Trade price (DECIMAL)")
quantity: Decimal = Field(
quote_amount: Decimal | None = Field(
â‹®----
side: str = Field(..., description="Trade side: 'buy' or 'sell'")
buyer_maker: bool | None = Field(
â‹®----
class OpenInterestRecord(BaseModel)
â‹®----
timestamp: datetime = Field(..., description="OI snapshot time (UTC)")
â‹®----
timeframe: str = Field(..., min_length=1, description="Timeframe (e.g., 5m, 1h)")
â‹®----
open_interest_usd: Decimal = Field(
settlement_currency: str | None = Field(
â‹®----
long_oi_usd: Decimal | None = Field(
short_oi_usd: Decimal | None = Field(
long_short_ratio: Decimal | None = Field(
</file>

<file path="storage/__init__.py">
__all__ = [
</file>

<file path="transformation/adapters/__init__.py">
__all__ = [
</file>

<file path="transformation/adapters/ccxt.py">
class CCXTOHLCVNormalizer(BaseOHLCVNormalizer)
â‹®----
def _extract_timestamp(self, raw_ohlcv: dict | list) -> int
â‹®----
"""Extract OHLC prices from CCXT OHLCV.

        Args:
            raw_ohlcv: CCXT OHLCV array [timestamp, o, h, l, c, v]

        Returns:
            (open, high, low, close) as Decimal objects

        Raises:
            KeyError: If price fields not found
            ValueError: If prices invalid
        """
â‹®----
# Dict format: {'open': ..., 'high': ..., 'low': ..., 'close': ...}
open_price = Decimal(str(raw_ohlcv["open"]))
high = Decimal(str(raw_ohlcv["high"]))
low = Decimal(str(raw_ohlcv["low"]))
close = Decimal(str(raw_ohlcv["close"]))
â‹®----
open_price = Decimal(str(raw_ohlcv[1]))
high = Decimal(str(raw_ohlcv[2]))
low = Decimal(str(raw_ohlcv[3]))
close = Decimal(str(raw_ohlcv[4]))
â‹®----
def _extract_volume(self, raw_ohlcv: dict | list) -> Decimal
â‹®----
"""Extract volume from CCXT OHLCV.

        Args:
            raw_ohlcv: CCXT OHLCV array [timestamp, o, h, l, c, v]

        Returns:
            Volume as Decimal

        Raises:
            KeyError: If volume field not found
            ValueError: If volume invalid
        """
â‹®----
volume = Decimal(str(raw_ohlcv["volume"]))
â‹®----
volume = Decimal(str(raw_ohlcv[5]))
â‹®----
class CCXTTradeNormalizer(BaseTradeNormalizer)
â‹®----
"""Normalizer for CCXT trade data.

    CCXT returns trades as dict:
    {
        'id': '12345',
        'timestamp': 1704067200000,
        'datetime': '2024-01-01T00:00:00.000Z',
        'symbol': 'BTC/USDT',
        'order': None,
        'type': 'limit',
        'side': 'buy',
        'takerOrMaker': 'taker',
        'price': 42000.5,
        'amount': 0.123,
        'cost': 5166.0615,
        'fee': {'cost': 5.166, 'currency': 'USDT'},
        'info': {...}  # Raw exchange response
    }
    """
â‹®----
def _extract_timestamp(self, raw_trade: dict) -> int
â‹®----
"""Extract timestamp from CCXT trade.

        Args:
            raw_trade: CCXT trade dict

        Returns:
            Unix timestamp in milliseconds

        Raises:
            KeyError: If timestamp not found
        """
â‹®----
def _extract_price(self, raw_trade: dict) -> Decimal
â‹®----
def _extract_quantity(self, raw_trade: dict) -> Decimal
â‹®----
def _extract_side(self, raw_trade: dict) -> str
â‹®----
side = raw_trade["side"].lower()
</file>

<file path="transformation/adapters/coinalyze.py">
class CoinalyzeOHLCVNormalizer(BaseOHLCVNormalizer)
â‹®----
def _extract_timestamp(self, raw_ohlcv: dict) -> int
â‹®----
open_price = Decimal(str(raw_ohlcv["open"]))
high = Decimal(str(raw_ohlcv["high"]))
low = Decimal(str(raw_ohlcv["low"]))
close = Decimal(str(raw_ohlcv["close"]))
â‹®----
def _extract_volume(self, raw_ohlcv: dict) -> Decimal
â‹®----
"""Extract volume from Coinalyze OHLCV.

        Args:
            raw_ohlcv: Coinalyze OHLCV dict

        Returns:
            Volume as Decimal (in USD)

        Raises:
            KeyError: If volume field not found
        """
â‹®----
# Coinalyze volume is in USD
volume = Decimal(str(raw_ohlcv["volume"]))
â‹®----
class CoinalyzeOINormalizer(BaseOpenInterestNormalizer)
</file>

<file path="transformation/adapters/coinmetrics.py">
class CoinMetricsOHLCVNormalizer(BaseOHLCVNormalizer)
â‹®----
def _extract_timestamp(self, raw_ohlcv: dict) -> int
â‹®----
unix_seconds = int(raw_ohlcv["timestamp"])
â‹®----
time_str = raw_ohlcv["time"]
â‹®----
dt = datetime.fromisoformat(time_str.replace("Z", "+00:00"))
â‹®----
"""Extract OHLC prices from CoinMetrics OHLCV.

        Handles both prefixed (price_open) and unprefixed (open) formats.

        Args:
            raw_ohlcv: CoinMetrics OHLCV dict

        Returns:
            (open, high, low, close) as Decimal objects

        Raises:
            KeyError: If price fields not found
        """
â‹®----
# Try prefixed format (price_open, price_high, etc.)
â‹®----
open_price = Decimal(str(raw_ohlcv["price_open"]))
high = Decimal(str(raw_ohlcv["price_high"]))
low = Decimal(str(raw_ohlcv["price_low"]))
close = Decimal(str(raw_ohlcv["price_close"]))
â‹®----
open_price = Decimal(str(raw_ohlcv["open"]))
high = Decimal(str(raw_ohlcv["high"]))
low = Decimal(str(raw_ohlcv["low"]))
close = Decimal(str(raw_ohlcv["close"]))
â‹®----
def _extract_volume(self, raw_ohlcv: dict) -> Decimal
â‹®----
"""Extract volume from CoinMetrics OHLCV.

        Args:
            raw_ohlcv: CoinMetrics OHLCV dict

        Returns:
            Volume as Decimal (in base currency)

        Raises:
            KeyError: If volume field not found
        """
â‹®----
# CoinMetrics volume is in base currency (e.g., BTC for BTC/USDT)
volume = Decimal(str(raw_ohlcv["volume"]))
</file>

<file path="transformation/normalizers/__init__.py">

</file>

<file path="transformation/pipelines/__init__.py">
__all__ = [
</file>

<file path="transformation/pipelines/base.py">
logger = logging.getLogger(__name__)
â‹®----
class IRepository(Protocol)
â‹®----
async def save_batch(self, records: list[Any]) -> int
â‹®----
class PipelineError(Exception)
â‹®----
class BaseTransformationPipeline
â‹®----
@property
    def data_type(self) -> str
â‹®----
start_time = time.time()
errors = []
â‹®----
normalized_records = await self._normalize(raw_data)
records_normalized = len(normalized_records)
â‹®----
records_valid = len(valid_records)
â‹®----
records_stored = await self._store(valid_records)
â‹®----
duration = time.time() - start_time
â‹®----
async def _normalize(self, raw_data: Any) -> list[Any]
â‹®----
"""Normalize raw API data to schema objects.

        Args:
            raw_data: Raw API response

        Returns:
            List of normalized schema objects

        Raises:
            PipelineError: If normalization fails
        """
â‹®----
record = await self.normalizer.normalize_single(raw_data)
â‹®----
async def _validate(self, records: list[Any]) -> tuple[list[Any], list[str]]
â‹®----
"""Validate normalized records.

        Args:
            records: List of normalized records

        Returns:
            (valid_records, error_messages)

        Raises:
            PipelineError: If validation process fails (not if records are invalid)
        """
â‹®----
valid_records = []
all_errors = []
â‹®----
error_msg = f"Record {i}: {'; '.join(errors)}"
â‹®----
async def _store(self, records: list[Any]) -> int
â‹®----
"""Store valid records to repository.

        Args:
            records: List of valid records

        Returns:
            Number of records stored

        Raises:
            PipelineError: If storage fails
        """
â‹®----
records_stored = await self.repository.save_batch(records)
</file>

<file path="transformation/pipelines/ohlcv.py">
class OHLCVRepository
â‹®----
class OHLCVTransformationPipeline(BaseTransformationPipeline)
â‹®----
@property
    def data_type(self) -> str
</file>

<file path="transformation/pipelines/open_interest.py">
class OpenInterestRepository
â‹®----
class OpenInterestPipeline(BaseTransformationPipeline)
â‹®----
@property
    def data_type(self) -> str
</file>

<file path="transformation/pipelines/trades.py">
class TradeRepository
â‹®----
class TradeTransformationPipeline(BaseTransformationPipeline)
â‹®----
@property
    def data_type(self) -> str
</file>

<file path="transformation/validators/__init__.py">

</file>

<file path="transformation/__init__.py">
__all__ = [
</file>

<file path="transformation/normalizers.py">
class NormalizationError(Exception)
â‹®----
class BaseOHLCVNormalizer
â‹®----
def __init__(self, symbol: str, exchange: str)
â‹®----
async def normalize_single(self, raw_ohlcv: dict) -> OHLCVRecord
â‹®----
timestamp = self._extract_timestamp(raw_ohlcv)
â‹®----
volume = self._extract_volume(raw_ohlcv)
â‹®----
async def normalize_batch(self, raw_ohlcvs: list[dict]) -> list[OHLCVRecord]
â‹®----
"""Normalize batch of OHLCV records.

        Args:
            raw_ohlcvs: List of raw OHLCV records from API

        Returns:
            List of OHLCVRecord objects

        Raises:
            NormalizationError: If batch normalization fails
        """
records = []
errors = []
â‹®----
record = await self.normalize_single(raw)
â‹®----
def _extract_timestamp(self, raw_ohlcv: dict) -> int
â‹®----
"""Extract timestamp (Unix milliseconds) from API format.

        Override in subclass to handle source-specific formats.

        Args:
            raw_ohlcv: Raw OHLCV record

        Returns:
            Unix timestamp in milliseconds

        Raises:
            KeyError: If timestamp field not found
        """
â‹®----
"""Extract OHLC prices from API format.

        Override in subclass to handle source-specific formats.

        Args:
            raw_ohlcv: Raw OHLCV record

        Returns:
            (open, high, low, close) as Decimal objects

        Raises:
            KeyError: If price fields not found
        """
â‹®----
def _extract_volume(self, raw_ohlcv: dict) -> Decimal
â‹®----
"""Extract volume from API format.

        Override in subclass to handle source-specific formats.

        Args:
            raw_ohlcv: Raw OHLCV record

        Returns:
            Volume as Decimal

        Raises:
            KeyError: If volume field not found
        """
â‹®----
class BaseOpenInterestNormalizer
â‹®----
"""Base normalizer for Open Interest data.

    Source adapters override:
    - _extract_timestamp(): Get timestamp from API format
    - _extract_open_interest_usd(): Get OI USD value from API format
    - _extract_optional_fields(): Get optional fields (long/short OI, etc.)
    """
</file>

<file path="transformation/ports.py">
class IDataNormalizer(Protocol)
â‹®----
async def normalize(self, raw_data: Any) -> Any
â‹®----
class IOHLCVNormalizer(IDataNormalizer, Protocol)
â‹®----
async def normalize_single(self, raw_ohlcv: dict) -> OHLCVRecord
â‹®----
async def normalize_batch(self, raw_ohlcvs: list[dict]) -> list[OHLCVRecord]
â‹®----
class IOpenInterestNormalizer(IDataNormalizer, Protocol)
â‹®----
async def normalize_single(self, raw_oi: dict) -> OpenInterestRecord
â‹®----
async def normalize_batch(self, raw_ois: list[dict]) -> list[OpenInterestRecord]
â‹®----
class ITradeNormalizer(IDataNormalizer, Protocol)
â‹®----
async def normalize_single(self, raw_trade: dict) -> TradeRecord
â‹®----
async def normalize_batch(self, raw_trades: list[dict]) -> list[TradeRecord]
â‹®----
class IDataValidator(Protocol)
â‹®----
async def validate(self, record: Any) -> tuple[bool, list[str]]
â‹®----
class IOHLCVValidator(IDataValidator, Protocol)
â‹®----
async def validate_single(self, record: OHLCVRecord) -> tuple[bool, list[str]]
â‹®----
class IOpenInterestValidator(IDataValidator, Protocol)
â‹®----
class ITransformationPipeline(Protocol)
</file>

<file path="transformation/validators.py">
class ValidationError(Exception)
â‹®----
class BaseOHLCVValidator
â‹®----
def __init__(self, allow_zero_volume: bool = False)
â‹®----
async def validate_single(self, record: OHLCVRecord) -> tuple[bool, list[str]]
â‹®----
errors = []
â‹®----
# Validate that all prices are positive (or zero for some cases)
â‹®----
# Validate volume
â‹®----
"""Validate batch of OHLCV records.

        Args:
            records: List of OHLCVRecord objects

        Returns:
            (validities, error_lists) - Parallel lists of validation results
        """
validities = []
error_lists = []
â‹®----
class BaseOpenInterestValidator
â‹®----
"""Base validator for Open Interest records.

    Checks:
    - open_interest_usd must be positive
    - If long_oi_usd and short_oi_usd present, both must be positive
    - If long_short_ratio present, must be positive
    - settlement_currency if present must be 3-4 char code (USDT, USDC, BNB, etc.)
    - Timestamp must be valid Unix milliseconds
    - No required fields are null
    """
â‹®----
VALID_SETTLEMENT_CURRENCIES = {
â‹®----
def __init__(self, strict_settlement_currency: bool = False)
â‹®----
# Validate optional long/short OI fields
â‹®----
# Validate long/short ratio if present
â‹®----
# Validate settlement_currency if present
â‹®----
currency = record.settlement_currency.upper()
â‹®----
"""Validate batch of OI records.

        Args:
            records: List of OpenInterestRecord objects

        Returns:
            (validities, error_lists)
        """
â‹®----
class BaseTradeValidator
â‹®----
"""Base validator for Trade records.

    Checks:
    - price must be positive
    - quantity must be positive
    - side must be 'buy' or 'sell'
    - timestamp must be valid Unix milliseconds
    - No required fields are null
    """
â‹®----
def __init__(self, allow_zero_quantity: bool = False)
â‹®----
"""Initialize validator.

        Args:
            allow_zero_quantity: If True, quantity of 0 is allowed
        """
â‹®----
async def validate_single(self, record: TradeRecord) -> tuple[bool, list[str]]
â‹®----
"""Validate single trade record.

        Args:
            record: TradeRecord to validate

        Returns:
            (is_valid, errors)
        """
â‹®----
# Validate required fields
â‹®----
# Validate quantity
â‹®----
# Validate timestamp
â‹®----
"""Validate batch of trade records.

        Args:
            records: List of TradeRecord objects

        Returns:
            (validities, error_lists)
        """
</file>

<file path="__init__.py">

</file>

<file path="config/__init__.py">
__all__ = [
</file>

</files>
