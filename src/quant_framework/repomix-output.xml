This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
infrastructure/
  config/
    __init__.py
  database/
    __init__.py
    ports.py
  minio/
    init_bronze.sh
  __init__.py
ingestion/
  adapters/
    ccxt_plugin/
      __init__.py
      ohlcv_adapter.py
      open_interest_adapter.py
    coinalyze_plugin/
      __init__.py
      backfill_adapter.py
      base.py
      client.py
      error_mapper.py
      exceptions.py
      key_rotator.py
      mappers.py
      ohlcv_adapter.py
      open_interest_adapter.py
      response_validator.py
      retry_handler.py
      symbol_registry.py
    coinmetrics_plugin/
      __init__.py
      base.py
      client.py
      onchain_adapter.py
      repomix-output.xml
    __init__.py
    base.py
  backfill/
    __init__.py
    checkpoint_manager.py
    coordinator.py
    rate_limiter.py
  connectors/
    __init__.py
    rest.py
  factories/
    adapter_factory.py
    preprocessor_factory.py
  models/
    __init__.py
    enums.py
  orchestration/
    data_fetcher.py
  pipelines/
    __init__.py
  ports/
    __init__.py
    data_ports.py
  preprocessing/
    base.py
    providers.py
  symbol_resolution/
    resolver.py
  __init__.py
  service.py
shared/
  enums/
    __init__.py
  models/
    __init__.py
    enums.py
    instruments.py
  __init__.py
__init__.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="infrastructure/config/__init__.py">
"""
Configuration infrastructure.
Re-exports from existing config utilities.
"""

from mnemo_quant.data_sources.ccxt.config_validator import config_registry
from mnemo_quant.data_sources.ccxt.exchange_mapper import CCXTExchangeMapper
from mnemo_quant.data_sources.ccxt.registry import EnhancedSymbolRegistry

__all__ = ["config_registry", "CCXTExchangeMapper", "EnhancedSymbolRegistry"]
</file>

<file path="infrastructure/database/__init__.py">
"""
Database infrastructure (placeholder for future implementation).
"""
</file>

<file path="infrastructure/database/ports.py">
"""
Database adapter interfaces and implementations.
Provides abstraction over database operations for dependency injection.
"""

from abc import ABC, abstractmethod
from datetime import datetime
from typing import Any, Dict, List, Optional, Protocol

from pydantic import BaseModel


class IDatabaseAdapter(Protocol):
    """
    Protocol defining database operations interface.
    Enables dependency injection and testing with different implementations.
    """

    async def connect(self) -> None:
        """Establish database connection."""
        ...

    async def disconnect(self) -> None:
        """Close database connection."""
        ...

    async def execute_query(
        self,
        query: str,
        *args: Any,
        fetch_one: bool = False,
        fetch_all: bool = False
    ) -> Optional[Any]:
        """
        Execute arbitrary SQL query.
        
        Args:
            query: SQL query string
            *args: Query parameters
            fetch_one: Return single row
            fetch_all: Return all rows
            
        Returns:
            Query result if fetch_one/fetch_all, else None
        """
        ...

    async def fetch_one(self, query: str, *args: Any) -> Optional[Dict[str, Any]]:
        """Fetch single row as dictionary."""
        ...

    async def fetch_all(self, query: str, *args: Any) -> List[Dict[str, Any]]:
        """Fetch all rows as list of dictionaries."""
        ...

    async def upsert_batch(
        self,
        records: List[BaseModel],
        table: str,
        conflict_keys: List[str],
        data_type: str,
        mode: str = "consumer"
    ) -> int:
        """
        Batch upsert Pydantic models.
        
        Args:
            records: List of Pydantic model instances
            table: Target table name
            conflict_keys: Columns for ON CONFLICT
            data_type: Type of data for logging
            mode: Operation mode ('consumer', 'backfill')
            
        Returns:
            Number of rows affected
        """
        ...

    async def update_metadata(
        self,
        data_type: str,
        symbol: str,
        interval: str,
        earliest_ts: Optional[datetime] = None,
        latest_ts: Optional[datetime] = None,
        mode: str = 'extraction'
    ) -> bool:
        """
        Update metadata tracking table.
        
        Args:
            data_type: Type of data ('ohlc', 'open_interest')
            symbol: Trading symbol
            interval: Timeframe
            earliest_ts: Earliest timestamp in batch
            latest_ts: Latest timestamp in batch
            mode: 'extraction', 'backfill', or 'ingestion'
            
        Returns:
            True if successful
        """
        ...


class DatabaseAdapter:
    """
    Concrete implementation wrapping mnemo_quant.db.Database.
    Adapts existing Database class to IDatabaseAdapter protocol.
    """

    def __init__(self, database: 'Database'):
        """
        Initialize adapter.
        
        Args:
            database: Instance of mnemo_quant.db.Database
        """
        self._db = database

    async def connect(self) -> None:
        """Establish database connection."""
        await self._db.connect()

    async def disconnect(self) -> None:
        """Close database connection."""
        await self._db.disconnect()

    async def execute_query(
        self,
        query: str,
        *args: Any,
        fetch_one: bool = False,
        fetch_all: bool = False
    ) -> Optional[Any]:
        """Execute arbitrary SQL query."""
        if not self._db.pool:
            raise RuntimeError("Database not connected")

        async with self._db.pool.acquire() as conn:
            if fetch_one:
                result = await conn.fetchrow(query, *args)
                return dict(result) if result else None
            elif fetch_all:
                results = await conn.fetch(query, *args)
                return [dict(row) for row in results]
            else:
                await conn.execute(query, *args)
                return None

    async def fetch_one(self, query: str, *args: Any) -> Optional[Dict[str, Any]]:
        """Fetch single row as dictionary."""
        return await self.execute_query(query, *args, fetch_one=True)

    async def fetch_all(self, query: str, *args: Any) -> List[Dict[str, Any]]:
        """Fetch all rows as list of dictionaries."""
        result = await self.execute_query(query, *args, fetch_all=True)
        return result if result else []

    async def upsert_batch(
        self,
        records: List[BaseModel],
        table: str,
        conflict_keys: List[str],
        data_type: str,
        mode: str = "consumer"
    ) -> int:
        """Batch upsert Pydantic models."""
        return await self._db.upsert_from_model_batch(
            records=records,
            table=table,
            conflict_keys=conflict_keys,
            data_type=data_type,
            mode=mode
        )

    async def update_metadata(
        self,
        data_type: str,
        symbol: str,
        interval: str,
        earliest_ts: Optional[datetime] = None,
        latest_ts: Optional[datetime] = None,
        mode: str = 'extraction'
    ) -> bool:
        """Update metadata tracking table."""
        return await self._db.update_metadata(
            data_type=data_type,
            symbol=symbol,
            interval=interval,
            earliest_ts=earliest_ts,
            latest_ts=latest_ts,
            mode=mode
        )

    @property
    def pool(self):
        """Access underlying connection pool."""
        return self._db.pool
</file>

<file path="infrastructure/minio/init_bronze.sh">
#!/usr/bin/env bash
# infra/minio/init-bronze-layer.sh
set -euo pipefail

echo "üèóÔ∏è  Initializing Bronze Layer (MinIO)..."

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REPO_ROOT="$(cd "$SCRIPT_DIR/../../../.." && pwd)"
VALIDATE_ONLY="${BRONZE_VALIDATE_ONLY:-0}"

# Discover config file (env override -> /app -> repo)
CONFIG_CANDIDATES=()
[[ -n "${BRONZE_SOURCES_CONFIG:-}" ]] && CONFIG_CANDIDATES+=("${BRONZE_SOURCES_CONFIG}")
CONFIG_CANDIDATES+=("/app/config/bronze/sources.yaml" "$REPO_ROOT/config/bronze/sources.yaml")

CONFIG_FILE=""
for candidate in "${CONFIG_CANDIDATES[@]}"; do
    if [[ -f "$candidate" ]]; then
        CONFIG_FILE="$candidate"
        break
    fi
done

# Fallback defaults (kept for compatibility)
FALLBACK_SOURCES=("coinalyze" "binance_exchange" "yahoo_finance")
declare -A FALLBACK_DATA_TYPES
FALLBACK_DATA_TYPES[coinalyze]="ohlc open_interest funding_rate"
FALLBACK_DATA_TYPES[binance_exchange]="trades orderbook ohlc funding_rate"
FALLBACK_DATA_TYPES[yahoo_finance]="ohlcv dividends splits"

declare -A SOURCE_DATA_TYPES
declare -A SEEN_SOURCES
SOURCES=()

find_python() {
    if command -v python3 >/dev/null 2>&1; then
        PYTHON_BIN=python3
    elif command -v python >/dev/null 2>&1; then
        PYTHON_BIN=python
    else
        PYTHON_BIN=""
    fi
}

find_yq() {
    if command -v yq >/dev/null 2>&1; then
        YQ_BIN=yq
    else
        YQ_BIN=""
    fi
}

add_source_datatype() {
    local source="$1" data_type="$2"
    if [[ -z "${SEEN_SOURCES[$source]:-}" ]]; then
        SOURCES+=("$source")
        SEEN_SOURCES[$source]=1
    fi

    if [[ -n "$data_type" ]]; then
        local existing="${SOURCE_DATA_TYPES[$source]:-}"
        for dt in $existing; do
            [[ "$dt" == "$data_type" ]] && return
        done
        SOURCE_DATA_TYPES[$source]="${existing:+$existing }$data_type"
    fi
}

load_config_sources_py() {
    local cfg="$1"
    "$PYTHON_BIN" - "$cfg" <<'PY'
import pathlib
import sys

try:
    import yaml
except ImportError as exc:
    sys.stderr.write("ERROR: PyYAML is required to parse sources config.\n")
    sys.exit(2)

cfg_path = pathlib.Path(sys.argv[1])

try:
    data = yaml.safe_load(cfg_path.read_text()) or {}
except Exception as exc:  # noqa: BLE001
    sys.stderr.write(f"ERROR: Failed to parse YAML: {exc}\n")
    sys.exit(3)

sources = data.get("sources")
errors = []

if not isinstance(sources, list) or len(sources) == 0:
    errors.append("Config 'sources' must be a non-empty list.")
    sources = []

seen = set()
for idx, src in enumerate(sources):
    if not isinstance(src, dict):
        errors.append(f"Item {idx} must be a mapping.")
        continue
    source_id = src.get("id")
    if not source_id or not isinstance(source_id, str):
        errors.append(f"Item {idx} missing string 'id'.")
        continue
    if source_id in seen:
        errors.append(f"Duplicate source id: {source_id}")
        continue
    seen.add(source_id)

    data_types = src.get("data_types")
    if data_types is None:
        data_types = []
    if not isinstance(data_types, (list, tuple)):
        errors.append(f"source {source_id}: 'data_types' must be a list.")
        data_types = []

    for dt in data_types:
        if not dt:
            errors.append(f"source {source_id}: empty data_type entry.")

    # Emit pairs (allow empty list to just create source dir)
    if data_types:
        for dt in data_types:
            if dt:
                print(f"{source_id}|{dt}")
    else:
        print(f"{source_id}|")

if errors:
    for err in errors:
        sys.stderr.write(f"ERROR: {err}\n")
    sys.exit(4)
PY
}

load_config_sources_yq() {
    local cfg="$1"

    # Ensure sources list exists and non-empty
    local count
    count=$("$YQ_BIN" -r '.sources | length // 0' "$cfg" 2>/dev/null || echo 0)
    if [[ "$count" -lt 1 ]]; then
        echo "ERROR: Config 'sources' must be a non-empty list." >&2
        return 4
    fi

    local missing_ids
    missing_ids=$("$YQ_BIN" -r '[.sources[] | select(.id == null or .id == "")] | length' "$cfg" 2>/dev/null || echo 0)
    if [[ "$missing_ids" -gt 0 ]]; then
        echo "ERROR: One or more sources are missing id." >&2
        return 4
    fi

    "$YQ_BIN" -r '.sources[] | .id as $id | ( ( .data_types // [] ) | (if length==0 then [""] else . end))[] | "\($id)|\(.)"' "$cfg"
}

if [[ -n "$CONFIG_FILE" ]]; then
    echo "üìÇ Loading source configuration from $CONFIG_FILE"
    find_python
    find_yq

    PARSED=0

    if [[ -n "${PYTHON_BIN:-}" ]]; then
        if mapfile -t SOURCE_DT_PAIRS < <(load_config_sources_py "$CONFIG_FILE"); then
            PARSED=1
        else
            echo "‚ÑπÔ∏è  Python parser failed; attempting yq fallback..."
        fi
    fi

    if [[ $PARSED -eq 0 && -n "${YQ_BIN:-}" ]]; then
        if mapfile -t SOURCE_DT_PAIRS < <(load_config_sources_yq "$CONFIG_FILE"); then
            PARSED=1
        else
            echo "‚ùå Failed to parse config with yq; aborting"
            exit 1
        fi
    fi

    if [[ $PARSED -eq 0 ]]; then
        echo "‚ùå No parser available (python+PyYAML or yq). Install one of them."
        exit 1
    fi

    if ((${#SOURCE_DT_PAIRS[@]} > 0)); then
        for pair in "${SOURCE_DT_PAIRS[@]}"; do
            source="${pair%%|*}"
            data_type="${pair#*|}"
            add_source_datatype "$source" "$data_type"
        done
    else
        echo "‚ùå Config is empty; aborting"
        exit 1
    fi
fi

if ((${#SOURCES[@]} == 0)); then
    echo "‚ÑπÔ∏è  Using fallback source list"
    for src in "${FALLBACK_SOURCES[@]}"; do
        for dt in ${FALLBACK_DATA_TYPES[$src]}; do
            add_source_datatype "$src" "$dt"
        done
        # Ensure source exists even if no data types
        add_source_datatype "$src" ""
    done
fi

BUCKET="${MINIO_BUCKET_NAME:-bronze}"
ENDPOINT="${MINIO_ENDPOINT:-http://minio:9000}"

if [[ "$VALIDATE_ONLY" == "1" ]]; then
    echo "‚úÖ Validation-only mode: configuration parsed successfully."
    echo "Sources: ${SOURCES[*]}"
    for source in "${SOURCES[@]}"; do
        echo "  - ${source}: ${SOURCE_DATA_TYPES[$source]:-}"
    done
    exit 0
fi

# Validate required env
MISSING_ENV=()
[[ -z "${MINIO_ROOT_USER:-}" ]] && MISSING_ENV+=(MINIO_ROOT_USER)
[[ -z "${MINIO_ROOT_PASSWORD:-}" ]] && MISSING_ENV+=(MINIO_ROOT_PASSWORD)
if ((${#MISSING_ENV[@]} > 0)); then
    echo "‚ùå Missing required env vars: ${MISSING_ENV[*]}"
    exit 1
fi

# Wait for MinIO to be ready
echo "‚è≥ Waiting for MinIO..."
MAX_RETRIES=30
RETRY_COUNT=0

until curl -s -f "${ENDPOINT}/minio/health/live" > /dev/null 2>&1; do
    RETRY_COUNT=$((RETRY_COUNT + 1))
    if [ $RETRY_COUNT -ge $MAX_RETRIES ]; then
        echo "‚ùå MinIO not ready after $MAX_RETRIES retries"
        exit 1
    fi
    echo "   Retry $RETRY_COUNT/$MAX_RETRIES..."
    sleep 2
done
echo "‚úÖ MinIO is ready"

# Configure mc client
echo "üîß Configuring MinIO client..."
mc alias set bronze "$ENDPOINT" "${MINIO_ROOT_USER}" "${MINIO_ROOT_PASSWORD}" > /dev/null

# Create main bronze bucket
echo "üì¶ Creating bronze bucket..."
if ! mc ls "bronze/${BUCKET}" > /dev/null 2>&1; then
    mc mb "bronze/${BUCKET}" --region="${MINIO_REGION:-us-east-1}"
    echo "‚úÖ Created bucket: ${BUCKET}"
else
    echo "‚ÑπÔ∏è Bucket '${BUCKET}' already exists"
fi

echo "üìÅ Creating source directory structure..."
for source in "${SOURCES[@]}"; do
    echo "  Creating: source=${source}/"
    echo -n "" | mc pipe "bronze/${BUCKET}/source=${source}/_directory_marker" || true
done

echo "üìä Creating data type directories..."
for source in "${SOURCES[@]}"; do
    for data_type in ${SOURCE_DATA_TYPES[$source]:-}; do
        echo "  Creating: source=${source}/data_type=${data_type}/"
        echo -n "" | mc pipe "bronze/${BUCKET}/source=${source}/data_type=${data_type}/_directory_marker" || true
    done
done

# Set lifecycle policy (keep raw data for 1 year)
echo "üìÖ Setting lifecycle policy (1 year retention)..."
mc ilm add "bronze/${BUCKET}" --expire-days "365" || echo "‚ö†Ô∏è  Could not set lifecycle policy"

# Create a README file in the bucket
echo "üìù Creating bucket documentation..."
{
    echo "# Bronze Layer - Raw Data Storage"
    echo ""
    echo "## Sources and data types"
    for source in "${SOURCES[@]}"; do
        echo "- source=${source}"
        for data_type in ${SOURCE_DATA_TYPES[$source]:-}; do
            echo "  - data_type=${data_type}"
        done
    done
    echo ""
    echo "## File naming"
    echo "{source}-{data_type}-{exchange?}-{YYYYMMDD}-{HHMM}.parquet"
    echo "Example: coinalyze-ohlc-binance-20240115-1400.parquet"
    echo ""
    echo "## Retention"
    echo "- Raw data kept for 365 days"
    echo "- Parquet format with Snappy compression"
    echo "- Partitioned by source and data_type"
    echo ""
    echo "## Processing"
    echo "Files in this bucket are immutable raw data."
    echo "Processed data moves to Silver layer (TimescaleDB)."
} > /tmp/bronze_readme.md

mc cp /tmp/bronze_readme.md "bronze/${BUCKET}/README.md" || echo "‚ö†Ô∏è  Could not upload README"

echo "üéâ Bronze layer initialization complete!"
echo ""
echo "Bucket: ${BUCKET}"
echo "Sources: ${SOURCES[*]}"
echo "Console: http://localhost:9001 (user: ${MINIO_ROOT_USER})"
echo "API: http://localhost:9000"
</file>

<file path="infrastructure/__init__.py">
"""
Infrastructure modules.
"""
</file>

<file path="ingestion/adapters/ccxt_plugin/__init__.py">
"""
CCXT plugin adapters for capability-based ingestion.

This module provides a thin wrapper that treats CCXT as a plugin rather
than a core dependency. Specific capability adapters (OHLCV, Open
Interest, etc.) inherit from this base and implement the relevant ports.
"""

from __future__ import annotations

import asyncio
import logging
from typing import Any

from quant_framework.ingestion.adapters.base import BaseAdapter
from quant_framework.ingestion.models.enums import ClientType, ConnectionType

logger = logging.getLogger(__name__)


class CCXTAdapterBase(BaseAdapter):
    """Base class for CCXT-backed adapters used as plugins."""

    client_type: ClientType = ClientType.WRAPPER
    connection_type: ConnectionType = ConnectionType.REST

    def __init__(self, client: Any, *args: Any, **kwargs: Any):
        super().__init__(*args, **kwargs)
        self.client = client

    async def connect(self) -> None:
        """Initialize CCXT client markets lazily."""
        if self._connected:
            return
        load_markets = getattr(self.client, "load_markets", None)
        if callable(load_markets):
            await asyncio.to_thread(load_markets)
        self._connected = True

    async def close(self) -> None:
        """Close CCXT client if supported."""
        if not self._connected:
            return
        close_fn = getattr(self.client, "close", None)
        if callable(close_fn):
            try:
                maybe_coro = close_fn()
                if asyncio.iscoroutine(maybe_coro):
                    await maybe_coro
            except Exception as exc:  # pragma: no cover - best-effort cleanup
                logger.warning("Error closing CCXT client: %s", exc)
        self._connected = False
</file>

<file path="ingestion/adapters/ccxt_plugin/ohlcv_adapter.py">
"""
CCXT OHLCV capability adapter.

Implements the OHLCVPort using a provided CCXT client.
"""

from __future__ import annotations

import asyncio
from collections.abc import Iterable
from datetime import datetime
from typing import Any

from quant_framework.ingestion.adapters.ccxt_plugin import CCXTAdapterBase
from quant_framework.ingestion.models.enums import DataProvider
from quant_framework.ingestion.ports.data_ports import OHLCVPort
from quant_framework.shared.models.enums import AssetClass
from quant_framework.shared.models.instruments import Instrument


class CCXTOHLCVAdapter(CCXTAdapterBase, OHLCVPort):
    """CCXT-backed OHLCV adapter implementing OHLCVPort."""

    provider: DataProvider
    supported_asset_classes = {AssetClass.CRYPTO}
    capabilities = {OHLCVPort}

    async def fetch_ohlcv(
        self,
        instrument: Instrument,
        timeframe: str,
        start: datetime | None = None,
        end: datetime | None = None,
        limit: int | None = None,
    ) -> Iterable[list[Any]]:
        if not self.validate_instrument(instrument):
            return []

        await self.connect()
        fetch = getattr(self.client, "fetch_ohlcv", None) or getattr(
            self.client, "fetchOHLCV", None
        )
        if not callable(fetch):
            raise NotImplementedError("CCXT client does not support fetch_ohlcv")

        since = int(start.timestamp() * 1000) if start else None
        # CCXT does not support explicit end; rely on limit+since; upstream orchestrator will paginate
        result = await asyncio.to_thread(
            fetch,
            instrument.raw_symbol or instrument.symbol,
            timeframe,
            since,
            limit,
        )
        return result
</file>

<file path="ingestion/adapters/ccxt_plugin/open_interest_adapter.py">
"""
CCXT Open Interest capability adapter.

Implements OpenInterestPort using a provided CCXT client (or compatible wrapper).
"""

from __future__ import annotations

import asyncio
from collections.abc import Iterable
from datetime import datetime
from typing import Any

from quant_framework.ingestion.adapters.ccxt_plugin import CCXTAdapterBase
from quant_framework.ingestion.models.enums import DataProvider
from quant_framework.ingestion.ports.data_ports import OpenInterestPort
from quant_framework.shared.models.enums import AssetClass
from quant_framework.shared.models.instruments import Instrument


class CCXTOpenInterestAdapter(CCXTAdapterBase, OpenInterestPort):
    """CCXT-backed open interest adapter implementing OpenInterestPort."""

    provider: DataProvider
    supported_asset_classes = {AssetClass.CRYPTO}
    capabilities = {OpenInterestPort}

    async def fetch_open_interest(
        self,
        instrument: Instrument,
        timeframe: str,
        start: datetime | None = None,
        end: datetime | None = None,
        limit: int | None = None,
    ) -> Iterable[dict[str, Any]]:
        if not self.validate_instrument(instrument):
            return []

        await self.connect()
        fetch = getattr(self.client, "fetch_open_interest", None) or getattr(
            self.client, "fetchOpenInterest", None
        )
        if not callable(fetch):
            raise NotImplementedError(
                "CCXT client does not support fetch_open_interest"
            )

        since = int(start.timestamp() * 1000) if start else None
        result = await asyncio.to_thread(
            fetch,
            instrument.raw_symbol or instrument.symbol,
            timeframe,
            since,
            limit,
        )
        return result
</file>

<file path="ingestion/adapters/coinalyze_plugin/__init__.py">

</file>

<file path="ingestion/adapters/coinalyze_plugin/backfill_adapter.py">
"""
CoinAlyze Backfill Adapter
Unified adapter for historical data backfill with rate limiting integration.
"""

import logging
from collections.abc import Iterable
from datetime import datetime
from typing import Any, List

from quant_framework.ingestion.ports.data_ports import OHLCVPort, OpenInterestPort
from quant_framework.ingestion.adapters.coinalyze_plugin.ohlcv_adapter import CoinalyzeOHLCVAdapter
from quant_framework.ingestion.adapters.coinalyze_plugin.open_interest_adapter import CoinalyzeOpenInterestAdapter
from quant_framework.ingestion.adapters.coinalyze_plugin.client import CoinalyzeClient
from quant_framework.ingestion.backfill.rate_limiter import IRateLimiter
from quant_framework.shared.models.instruments import Instrument

logger = logging.getLogger(__name__)


class CoinalyzeBackfillAdapter:
    """
    Unified backfill adapter for CoinAlyze data.
    Combines OHLCV and OI capabilities with rate limiting.
    """

    def __init__(
        self,
        client: CoinalyzeClient,
        rate_limiter: IRateLimiter,
        verbose: bool = False
    ):
        """
        Initialize backfill adapter.
        
        Args:
            client: CoinAlyze API client
            rate_limiter: Rate limiting strategy
            verbose: Enable verbose logging
        """
        self.client = client
        self.rate_limiter = rate_limiter
        self.verbose = verbose
        
        # Initialize underlying adapters
        self.ohlcv_adapter = CoinalyzeOHLCVAdapter(client)
        self.oi_adapter = CoinalyzeOpenInterestAdapter(client)
        
        logger.info("CoinAlyze backfill adapter initialized")

    async def fetch_ohlcv_history(
        self,
        symbol: str,
        timeframe: str,
        start_date: datetime,
        end_date: datetime,
        instrument: Instrument
    ) -> List[dict]:
        """
        Fetch OHLCV history with rate limit validation.
        
        Args:
            symbol: Trading symbol
            timeframe: Data timeframe
            start_date: Start of date range
            end_date: End of date range
            instrument: Instrument object with venue info
            
        Returns:
            List of OHLCV records
        """
        # Normalize timeframe for rate limiting
        normalized_tf = self.rate_limiter.normalize_timeframe(timeframe)
        
        if self.verbose:
            logger.debug(
                f"Fetching OHLCV: {symbol}/{normalized_tf} "
                f"from {start_date} to {end_date}"
            )
        
        # Fetch data using existing adapter
        try:
            result = await self.ohlcv_adapter.fetch_ohlcv(
                instrument=instrument,
                timeframe=normalized_tf,
                start=start_date,
                end=end_date
            )
            
            # Convert result to list if needed
            if isinstance(result, list):
                data = result
            else:
                data = list(result)
            
            # Extract history from response
            records = []
            for item in data:
                if isinstance(item, dict) and 'history' in item:
                    records.extend(item['history'])
                elif isinstance(item, dict):
                    records.append(item)
            
            # Validate response with rate limiter
            validation = self.rate_limiter.validate_chunk_response(
                points_received=len(records),
                timeframe=normalized_tf
            )
            
            if not validation['valid']:
                logger.error(
                    f"Rate limit validation failed for {symbol}/{normalized_tf}: "
                    f"{validation['warnings']}"
                )
            elif validation['warnings'] and self.verbose:
                for warning in validation['warnings']:
                    logger.warning(warning)
            
            # Record performance for adaptive chunking
            chunk_size_days = (end_date - start_date).days
            self.rate_limiter.record_performance(
                timeframe=normalized_tf,
                chunk_size_days=chunk_size_days,
                points_received=len(records),
                success=validation['valid']
            )
            
            logger.info(
                f"‚úÖ Fetched {len(records)} OHLCV records for "
                f"{symbol}/{normalized_tf}"
            )
            
            return records
            
        except Exception as e:
            logger.error(
                f"‚ùå Failed to fetch OHLCV for {symbol}/{normalized_tf}: {e}"
            )
            raise

    async def fetch_oi_history(
        self,
        symbol: str,
        timeframe: str,
        start_date: datetime,
        end_date: datetime,
        instrument: Instrument
    ) -> List[dict]:
        """
        Fetch Open Interest history with rate limit validation.
        
        Args:
            symbol: Trading symbol
            timeframe: Data timeframe
            start_date: Start of date range
            end_date: End of date range
            instrument: Instrument object with venue info
            
        Returns:
            List of OI records
        """
        # Normalize timeframe for rate limiting
        normalized_tf = self.rate_limiter.normalize_timeframe(timeframe)
        
        if self.verbose:
            logger.debug(
                f"Fetching OI: {symbol}/{normalized_tf} "
                f"from {start_date} to {end_date}"
            )
        
        # Fetch data using existing adapter
        try:
            result = await self.oi_adapter.fetch_open_interest(
                instrument=instrument,
                timeframe=normalized_tf,
                start=start_date,
                end=end_date
            )
            
            # Convert result to list if needed
            if isinstance(result, list):
                data = result
            else:
                data = list(result)
            
            # Extract history from response
            records = []
            for item in data:
                if isinstance(item, dict) and 'history' in item:
                    records.extend(item['history'])
                elif isinstance(item, dict):
                    records.append(item)
            
            # Validate response with rate limiter
            validation = self.rate_limiter.validate_chunk_response(
                points_received=len(records),
                timeframe=normalized_tf
            )
            
            if not validation['valid']:
                logger.error(
                    f"Rate limit validation failed for {symbol}/{normalized_tf}: "
                    f"{validation['warnings']}"
                )
            elif validation['warnings'] and self.verbose:
                for warning in validation['warnings']:
                    logger.warning(warning)
            
            # Record performance for adaptive chunking
            chunk_size_days = (end_date - start_date).days
            self.rate_limiter.record_performance(
                timeframe=normalized_tf,
                chunk_size_days=chunk_size_days,
                points_received=len(records),
                success=validation['valid']
            )
            
            logger.info(
                f"‚úÖ Fetched {len(records)} OI records for "
                f"{symbol}/{normalized_tf}"
            )
            
            return records
            
        except Exception as e:
            logger.error(
                f"‚ùå Failed to fetch OI for {symbol}/{normalized_tf}: {e}"
            )
            raise

    def supports_ohlcv(self) -> bool:
        """Check if adapter supports OHLCV data."""
        return True

    def supports_open_interest(self) -> bool:
        """Check if adapter supports Open Interest data."""
        return True

    def get_performance_summary(self) -> dict:
        """
        Get rate limiter performance summary.
        
        Returns:
            Performance statistics dictionary
        """
        if hasattr(self.rate_limiter, 'get_performance_summary'):
            return self.rate_limiter.get_performance_summary()
        return {}
</file>

<file path="ingestion/adapters/coinalyze_plugin/base.py">
from quant_framework.ingestion.adapters.base import BaseAdapter
from quant_framework.ingestion.models.enums import (
    ClientType,
    ConnectionType,
)
from quant_framework.shared.models.enums import (
    AssetClass,
    DataVenue,
    WrapperImplementation,
)
from .client import CoinalyzeClient


class CoinalyzeAdapterBase(BaseAdapter):
    """Base adapter for CoinAlyze API.

    CoinAlyze is a multi-venue wrapper that provides access to:
    - BINANCE (spot)
    - BINANCE_USDM (linear perpetuals/futures)
    - BINANCE_COINM (inverse perpetuals/futures)
    - BYBIT (unified)
    - GATEIO (unified)
    - HUOBI (unified)
    """

    # Data lineage
    venue: DataVenue  # Set per-instance based on instrument
    wrapper = WrapperImplementation.COINALYZE

    # Technical implementation
    client_type = ClientType.WRAPPER
    connection_type = ConnectionType.REST

    # CoinAlyze supports crypto only
    supported_asset_classes = {AssetClass.CRYPTO}

    # Multi-venue support flag
    supports_multiple_venues = True  # Used in validate_instrument()

    def __init__(self, client: CoinalyzeClient, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.client = client
        # venue is set per-instrument in fetch methods
</file>

<file path="ingestion/adapters/coinalyze_plugin/client.py">
import asyncio
import logging
import time
from collections import deque

import aiohttp

from config.settings import settings

from .key_rotator import KeyRotator  # new

logger = logging.getLogger(__name__)


class APIKeyManager:
    """Manage coinalyze API keys with rate limiting."""

    def __init__(self, api_keys: list[str], rate_limit: int, request_interval: float):
        self.api_keys = api_keys
        self.rate_limit = rate_limit  # Calls per minute per key
        self.request_interval = request_interval  # Seconds between calls
        self.key_usage = {key: deque(maxlen=rate_limit) for key in api_keys}
        self.current_key_index = 0

    def get_next_key(self) -> str | None:
        """Get next available key or None if rate limit reached."""
        for _ in range(len(self.api_keys)):
            key = self.api_keys[self.current_key_index]
            usage = self.key_usage[key]
            now = time.time()
            # Remove timestamps older than 60s
            while usage and usage[0] < now - 60:
                usage.popleft()
            if len(usage) < self.rate_limit:
                usage.append(now)
                return key
            self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
        logger.warning("All API keys at rate limit")
        return None

    def get_headers(self) -> dict[str, str]:
        """Return headers with next available key."""
        key = self.get_next_key()
        if not key:
            raise RuntimeError("No available API keys due to rate limit")
        return {"api_key": key}


class CoinalyzeClient:
    """Async client for coinalyze API."""

    def __init__(self):
        self.base_url = settings.coinalyze.base_url
        keys = settings.coinalyze.api_keys
        self.key_manager = KeyRotator(settings.coinalyze.api_keys)
        self.session: aiohttp.ClientSession | None = None

    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
            self.session = None  # Clear the reference

    async def fetch_data_async(self, endpoint: str, params: dict) -> list[dict]:
        """
        Fetch data from coinalyze API with intelligent retry logic.

        Args:
            endpoint: API endpoint to call
            params: Query parameters for the request

        Returns:
            Parsed JSON response data

        Raises:
            Various CoinAlyzeAPIError subclasses based on error type
        """
        if not self.session:
            raise RuntimeError("Client session not initialized")

        url = f"{self.base_url}/{endpoint}"
        max_retries = 3

        # Log request details for debugging
        logger.debug(f"üîç Request: {endpoint}")
        logger.debug(f"üîç URL: {url}")
        logger.debug(f"üîç Params: {params}")

        for attempt in range(max_retries):
            headers = await self.key_manager.get_headers()

            try:
                async with self.session.get(
                    url, params=params, headers=headers
                ) as response:
                    # Capture response body for diagnostics
                    response_text = await response.text()

                    if response.status == 200:
                        # Success - parse and validate response
                        try:
                            data = (
                                await response.json()
                                if not response_text
                                else eval(response_text)
                                if response_text.startswith("[")
                                else await response.json()
                            )
                        except Exception:
                            # If already have text, try to parse it
                            import json

                            data = json.loads(response_text)

                        # Validate response structure
                        from .response_validator import ResponseValidator

                        is_valid, validation_msg = ResponseValidator.validate_response(
                            endpoint, data
                        )

                        if not is_valid:
                            logger.warning(
                                f"‚ö†Ô∏è Response validation failed for {endpoint}: {validation_msg}"
                            )
                            from .exceptions import ValidationError

                            raise ValidationError(
                                f"Invalid response structure for {endpoint}: {validation_msg}",
                                status_code=200,
                                endpoint=endpoint,
                            )

                        logger.info(
                            f"‚úÖ Successfully fetched {endpoint} for {params.get('symbols')}"
                        )
                        logger.debug(f"üîç Response validation: {validation_msg}")

                        return data

                    else:
                        # Error response - log details and classify error
                        logger.error(f"‚ùå HTTP {response.status} for {endpoint}")
                        logger.debug(
                            f"üîç Response body: {response_text[:500]}"
                        )  # First 500 chars

                        # Import here to avoid circular imports
                        from .error_mapper import CoinAlyzeErrorMapper
                        from .retry_handler import RetryHandler

                        # Get retry-after header if present
                        retry_after = response.headers.get("Retry-After")

                        # Map to specific exception
                        error = CoinAlyzeErrorMapper.map_error(
                            status_code=response.status,
                            response_body=response_text,
                            endpoint=endpoint,
                            retry_after=retry_after,
                        )

                        # Check if error is retryable
                        if not RetryHandler.should_retry(response.status):
                            # Non-retryable error - fail immediately
                            logger.error(
                                f"‚ùå Non-retryable error for {endpoint}: {error}"
                            )
                            raise error

                        # Retryable error - check if we have attempts left
                        if attempt < max_retries - 1:
                            sleep_time = RetryHandler.get_retry_delay(
                                attempt, response.status, retry_after
                            )
                            logger.warning(
                                f"‚ö†Ô∏è Retryable error (attempt {attempt + 1}/{max_retries}), "
                                f"sleeping {sleep_time}s before retry: {error}"
                            )
                            await asyncio.sleep(sleep_time)
                            continue
                        else:
                            # Out of retries
                            logger.error(
                                f"‚ùå Max retries reached for {endpoint}: {error}"
                            )
                            raise error

            except aiohttp.ClientError as e:
                # Network error
                logger.error(f"‚ùå Network error for {endpoint}: {e}")

                if attempt < max_retries - 1:
                    sleep_time = 2**attempt  # Exponential backoff
                    logger.warning(
                        f"‚ö†Ô∏è Retrying after network error (attempt {attempt + 1}/{max_retries}), sleeping {sleep_time}s"
                    )
                    await asyncio.sleep(sleep_time)
                    continue
                else:
                    raise RuntimeError(
                        f"Network error after {max_retries} attempts: {e}"
                    )

        # Should not reach here, but just in case
        raise RuntimeError(f"Failed to fetch {endpoint} after {max_retries} attempts")
</file>

<file path="ingestion/adapters/coinalyze_plugin/error_mapper.py">
"""
CoinAlyze Error Mapper

Maps HTTP status codes and response bodies to specific exception types,
providing context-rich error messages for debugging.
"""

from typing import Any

from .exceptions import (
    AuthenticationError,
    BadRequestError,
    CoinAlyzeAPIError,
    NotFoundError,
    RateLimitError,
    ServerError,
)


class CoinAlyzeErrorMapper:
    """Maps HTTP status codes to appropriate exception types."""

    @staticmethod
    def extract_error_message(response_body: Any) -> str:
        """Extract error message from response body."""
        if isinstance(response_body, str):
            return response_body
        elif isinstance(response_body, dict):
            # Try common error message keys
            return response_body.get('error') or response_body.get('message') or str(response_body)
        else:
            return str(response_body)

    @staticmethod
    def map_error(
        status_code: int,
        response_body: Any,
        endpoint: str,
        retry_after: str | None = None
    ) -> CoinAlyzeAPIError:
        """
        Map HTTP status code to specific exception with context.
        
        Args:
            status_code: HTTP status code
            response_body: Response body (dict, str, or other)
            endpoint: API endpoint that was called
            retry_after: Retry-After header value if present
            
        Returns:
            Appropriate CoinAlyzeAPIError subclass instance
        """
        error_msg = CoinAlyzeErrorMapper.extract_error_message(response_body)

        if status_code == 400:
            return BadRequestError(
                f"Bad parameters for {endpoint}: {error_msg}",
                status_code=status_code,
                endpoint=endpoint
            )
        elif status_code == 401:
            return AuthenticationError(
                f"Invalid API key for {endpoint}: {error_msg}",
                status_code=status_code,
                endpoint=endpoint
            )
        elif status_code == 404:
            return NotFoundError(
                f"Resource not found for {endpoint}: {error_msg}",
                status_code=status_code,
                endpoint=endpoint
            )
        elif status_code == 429:
            retry_after_int = None
            if retry_after:
                try:
                    retry_after_int = int(retry_after)
                except ValueError:
                    pass

            return RateLimitError(
                f"Rate limit exceeded for {endpoint}: {error_msg}",
                retry_after=retry_after_int,
                status_code=status_code,
                endpoint=endpoint
            )
        elif status_code >= 500:
            return ServerError(
                f"Server error {status_code} for {endpoint}: {error_msg}",
                status_code=status_code,
                endpoint=endpoint
            )
        else:
            return CoinAlyzeAPIError(
                f"Unexpected error {status_code} for {endpoint}: {error_msg}",
                status_code=status_code,
                endpoint=endpoint
            )
</file>

<file path="ingestion/adapters/coinalyze_plugin/exceptions.py">
"""
CoinAlyze API Exception Hierarchy

Provides specific exception types for different CoinAlyze API error scenarios,
enabling proper error classification and handling downstream.
"""


class CoinAlyzeAPIError(Exception):
    """Base exception for all CoinAlyze API errors."""

    def __init__(self, message: str, status_code: int | None = None, endpoint: str | None = None):
        super().__init__(message)
        self.status_code = status_code
        self.endpoint = endpoint


class BadRequestError(CoinAlyzeAPIError):
    """400 - Bad parameter(s) in the request."""
    pass


class AuthenticationError(CoinAlyzeAPIError):
    """401 - Invalid or missing API key."""
    pass


class NotFoundError(CoinAlyzeAPIError):
    """404 - Resource not found (symbol, endpoint, or data unavailable)."""
    pass


class RateLimitError(CoinAlyzeAPIError):
    """429 - Too many requests, rate limit exceeded."""

    def __init__(self, message: str, retry_after: int | None = None, **kwargs):
        super().__init__(message, **kwargs)
        self.retry_after = retry_after


class ServerError(CoinAlyzeAPIError):
    """500+ - Server-side error."""
    pass


class ValidationError(CoinAlyzeAPIError):
    """Response validation failed."""
    pass
</file>

<file path="ingestion/adapters/coinalyze_plugin/key_rotator.py">
import asyncio
import time
from collections import deque

HEAD = "api_key"  # header / query name required by Coinalyze
MAX_PER_KEY = 39  # stay just under 40
WINDOW = 60


class KeyRotator:
    """
    Thread-safe API-key rotator for Coinalyze.
    Keeps ‚â§ N keys in a ring, tracks per-key calls in Redis (or memory),
    auto-evicts exhausted keys and refreshes the ring when every key is hot.
    Usage:
        rotator = KeyRotator(["key1", "key2", "key3"], redis)  # redis=None is OK
        headers = await rotator.get_headers()  # blocks until a key is free
    """

    def __init__(self, keys: list[str]):
        if not keys:
            raise RuntimeError("No API keys provided")
        self._keys: deque[str] = deque(keys)
        self._lock = asyncio.Lock()
        self._local: dict[str, deque[float]] = {}  # fallback if redis absent

    # ---------- public ----------

    async def get_headers(self) -> dict[str, str]:
        """Return headers dict with an available key; wait if necessary."""
        while True:
            key = await self._next_available_key()
            if key:
                await self._record_call(key)
                return {HEAD: key}
            await asyncio.sleep(0.5)  # spin politely

    # ---------- internal ----------

    async def _next_available_key(self) -> str | None:
        async with self._lock:
            now = time.time()
            for _ in range(len(self._keys)):
                key = self._keys[0]
                self._keys.rotate(-1)
                if await self._call_count(key, now) < MAX_PER_KEY:
                    return key
            # all keys exhausted ‚Äì refresh ring (could fetch new keys here)
            await self._refresh_keys()
            return None

    async def _call_count(self, key: str, now: float) -> int:
        """Number of calls made with `key` in the last 60 s."""      # memory fallback
        dq = self._local.setdefault(key, deque(maxlen=MAX_PER_KEY * 2))
        while dq and dq[0] < now - WINDOW:
            dq.popleft()
        return len(dq)

    async def _record_call(self, key: str) -> None:
        now = time.time()
        self._local.setdefault(key, deque(maxlen=MAX_PER_KEY * 2)).append(now)

    async def _refresh_keys(self) -> None:
        """
        Replenish the ring ‚Äì for now just re-queue the same keys
        (you could fetch new keys from a secret store here).
        """
        await asyncio.sleep(WINDOW / 2)  # wait half window
</file>

<file path="ingestion/adapters/coinalyze_plugin/mappers.py">
# quant_framework/ingestion/adapters/coinalyze_plugin/mappers.py

from quant_framework.shared.models.instruments import Instrument

from .symbol_registry import coin_alyze_registry

COINALYZE_INTERVAL_MAP = {
    "1m": "1min",
    "5m": "5min",
    "15m": "15min",
    "30m": "30min",
    "1h": "1hour",
    "4h": "4hour",
    "1d": "daily",
}


def get_coinalyze_interval(timeframe: str) -> str:
    """Map internal timeframe to CoinAlyze interval"""
    if timeframe not in COINALYZE_INTERVAL_MAP:
        raise ValueError(f"Unsupported timeframe {timeframe}")
    return COINALYZE_INTERVAL_MAP[timeframe]


def get_coinalyze_symbol(instrument: Instrument) -> str:
    """
    One-line wrapper around your proven registry.
    Takes Instrument ‚Üí CoinAlyze symbol format
    """
    # Map venue to exchange name
    exchange = instrument.venue.value.replace("_usdm", "").replace("_coinm", "")

    # Determine market type and settlement
    market_type = (
        "linear_perpetual" if not instrument.is_inverse else "inverse_perpetual"
    )
    settlement = (
        instrument.settlement_currency if not instrument.is_inverse else "NATIVE"
    )

    # Use your battle-tested registry
    return coin_alyze_registry.get_coinalyze_symbol(
        base_asset=instrument.base_asset,
        exchange=exchange,
        market_type=market_type,
        settlement_currency=settlement,
    )
</file>

<file path="ingestion/adapters/coinalyze_plugin/ohlcv_adapter.py">
"""CoinAlyze OHLCV Adapter - Fetches OHLCV data from CoinAlyze API."""

import logging
from collections.abc import Iterable
from datetime import datetime
from typing import Any

from quant_framework.ingestion.ports.data_ports import OHLCVPort

from .base import CoinalyzeAdapterBase
from .mappers import get_coinalyze_interval, get_coinalyze_symbol

logger = logging.getLogger(__name__)


class CoinalyzeOHLCVAdapter(CoinalyzeAdapterBase, OHLCVPort):
    """OHLCV data from CoinAlyze API."""

    capabilities = {OHLCVPort}

    async def fetch_ohlcv(
        self,
        instrument,
        timeframe: str,
        start: datetime,
        end: datetime,
        limit: int | None = None,
    ) -> Iterable[list[Any]]:
        """
        Fetch raw OHLCV data from CoinAlyze.

        Args:
            instrument: Instrument with venue, symbol, etc.
            timeframe: Timeframe string (e.g., "1h", "5m")
            start: Start datetime
            end: End datetime
            limit: Maximum number of records (optional)

        Returns:
            Raw data from CoinAlyze API (list of dicts with "history" key)
        """
        if not self.validate_instrument(instrument):
            logger.warning(f"Instrument validation failed: {instrument}")
            return []

        # Set venue for this fetch (logging/tracking purposes)
        self.venue = instrument.venue

        # Get CoinAlyze symbol (uses your proven registry)
        symbol = get_coinalyze_symbol(instrument)

        # Map timeframe
        interval = get_coinalyze_interval(timeframe)

        # Build request parameters
        params = {
            "symbols": symbol,
            "interval": interval,
            "from": int(start.timestamp()),
            "to": int(end.timestamp()),
            "convert_to_usd": "false",
        }

        logger.info(f"Fetching OHLCV for {instrument.instrument_id} ({symbol})")

        # Fetch via client (your existing client.py)
        return await self.client.fetch_data_async("ohlcv-history", params)
</file>

<file path="ingestion/adapters/coinalyze_plugin/open_interest_adapter.py">
"""CoinAlyze Open Interest Adapter - Fetches OI data from CoinAlyze API."""

import logging
from collections.abc import Iterable
from datetime import datetime
from typing import Any

from quant_framework.ingestion.ports.data_ports import OpenInterestPort

from .base import CoinalyzeAdapterBase
from .mappers import get_coinalyze_interval, get_coinalyze_symbol

logger = logging.getLogger(__name__)


class CoinalyzeOpenInterestAdapter(CoinalyzeAdapterBase, OpenInterestPort):
    """Open Interest data from CoinAlyze API."""

    capabilities = {OpenInterestPort}

    async def fetch_open_interest(
        self,
        instrument,
        timeframe: str,
        start: datetime,
        end: datetime,
        limit: int | None = None,
    ) -> Iterable[dict[str, Any]]:
        """
        Fetch raw Open Interest data from CoinAlyze.

        Args:
            instrument: Instrument with venue, symbol, etc.
            timeframe: Timeframe string (e.g., "1h", "5m")
            start: Start datetime
            end: End datetime
            limit: Maximum number of records (optional)

        Returns:
            Raw OI data from CoinAlyze API (list of dicts with "history" key)
        """
        if not self.validate_instrument(instrument):
            logger.warning(f"Instrument validation failed: {instrument}")
            return []

        self.venue = instrument.venue

        symbol = get_coinalyze_symbol(instrument)
        interval = get_coinalyze_interval(timeframe)

        params = {
            "symbols": symbol,
            "interval": interval,
            "from": int(start.timestamp()),
            "to": int(end.timestamp()),
            "convert_to_usd": "false",
        }

        logger.info(f"Fetching Open Interest for {instrument.instrument_id} ({symbol})")

        return await self.client.fetch_data_async("open-interest-history", params)
</file>

<file path="ingestion/adapters/coinalyze_plugin/response_validator.py">
"""
CoinAlyze Response Validator

Validates API response structure to ensure schema compliance before processing.
Helps catch data quality issues early.
"""

from typing import Any


class ResponseValidator:
    """Validates CoinAlyze API response structures."""

    @staticmethod
    def validate_ohlc_response(data: Any) -> tuple[bool, str]:
        """
        Validate OHLC response structure.
        
        Expected format:
        [
            {
                "symbol": "BTCUSDT_PERP.A",
                "history": [
                    {"t": timestamp, "o": open, "h": high, "l": low, "c": close, "v": volume},
                    ...
                ]
            },
            ...
        ]
        
        Args:
            data: Response data to validate
            
        Returns:
            Tuple of (is_valid, error_message)
        """
        if not isinstance(data, list):
            return False, f"Response must be a list, got {type(data).__name__}"

        if len(data) == 0:
            # Empty list is valid (no data for the range)
            return True, "Valid (empty)"

        for idx, item in enumerate(data):
            if not isinstance(item, dict):
                return False, f"Item {idx} must be a dict, got {type(item).__name__}"

            if 'symbol' not in item:
                return False, f"Item {idx} missing required 'symbol' field"

            if 'history' not in item:
                return False, f"Item {idx} missing required 'history' field"

            if not isinstance(item['history'], list):
                return False, f"Item {idx} 'history' must be a list, got {type(item['history']).__name__}"

            # Validate history entries (if not empty)
            if item['history']:
                first_entry = item['history'][0]
                required_fields = ['t', 'o', 'h', 'l', 'c']
                for field in required_fields:
                    if field not in first_entry:
                        return False, f"Item {idx} history entry missing required field '{field}'"

        return True, "Valid"

    @staticmethod
    def validate_oi_response(data: Any) -> tuple[bool, str]:
        """
        Validate Open Interest response structure.
        
        OI uses the same structure as OHLC (OHLC format where 'c' is the OI value).
        
        Args:
            data: Response data to validate
            
        Returns:
            Tuple of (is_valid, error_message)
        """
        # OI response has same structure as OHLC
        return ResponseValidator.validate_ohlc_response(data)

    @staticmethod
    def validate_response(endpoint: str, data: Any) -> tuple[bool, str]:
        """
        Dispatch validation based on endpoint.
        
        Args:
            endpoint: API endpoint name
            data: Response data to validate
            
        Returns:
            Tuple of (is_valid, error_message)
        """
        if endpoint == "ohlcv-history":
            return ResponseValidator.validate_ohlc_response(data)
        elif endpoint == "open-interest-history":
            return ResponseValidator.validate_oi_response(data)
        else:
            # Unknown endpoint, accept any data
            return True, "No validation defined for endpoint"
</file>

<file path="ingestion/adapters/coinalyze_plugin/retry_handler.py">
"""
CoinAlyze Retry Handler

Intelligent retry logic that distinguishes between retryable errors
(rate limits, server errors) and non-retryable errors (bad requests, not found).
"""


class RetryHandler:
    """Determines retry behavior for different error types."""

    # Status codes that should be retried (temporary failures)
    RETRYABLE_STATUS_CODES = (429, 500, 502, 503, 504)

    # Status codes that should NOT be retried (permanent failures)
    NON_RETRYABLE_STATUS_CODES = (400, 401, 404)

    @classmethod
    def should_retry(cls, status_code: int) -> bool:
        """
        Determine if an error should be retried.
        
        Args:
            status_code: HTTP status code
            
        Returns:
            True if error is retryable, False otherwise
        """
        # Explicit non-retryable errors fail immediately
        if status_code in cls.NON_RETRYABLE_STATUS_CODES:
            return False

        # Explicit retryable errors should be retried
        if status_code in cls.RETRYABLE_STATUS_CODES:
            return True

        # Unknown 5xx errors should be retried as server issues
        if status_code >= 500:
            return True

        # All other errors (2xx success, 3xx redirects, other 4xx) should not retry
        return False

    @classmethod
    def get_retry_delay(
        cls,
        attempt: int,
        status_code: int,
        retry_after: str | None = None
    ) -> float:
        """
        Calculate retry delay with exponential backoff.
        
        Args:
            attempt: Current retry attempt (0-indexed)
            status_code: HTTP status code
            retry_after: Retry-After header value if present
            
        Returns:
            Number of seconds to wait before retrying
        """
        # Honor Retry-After header for rate limits
        if status_code == 429 and retry_after:
            try:
                return int(retry_after)
            except ValueError:
                pass

        # Exponential backoff: 2^attempt
        base_delay = 2 ** attempt

        # Different max delays for different error types
        if status_code == 429:
            # Rate limits: allow up to 60s delay
            max_delay = 60
        else:
            # Server errors: max 30s delay
            max_delay = 30

        return min(base_delay, max_delay)
</file>

<file path="ingestion/adapters/coinalyze_plugin/symbol_registry.py">
import logging

logger = logging.getLogger(__name__)


class CoinAlyzeSymbolRegistry:
    """
    Exchange-specific symbol mapping for CoinAlyze API.

    Each exchange has different symbol format conventions:
    - Binance (A): BTCUSDT_PERP.A, BTCBUSD_PERP.A, BTCUSDC_PERP.A, BTCUSD_PERP.A (inverse)
    - Bybit (6): BTCUSDT.6 (no _PERP), BTCUSDC.6, BTCUSD.6 (inverse)
    - Gate.io (Y): BTC_USDT.Y (underscore separator), BTC_USD.Y (inverse)
    - Huobi (4): BTCUSDT_PERP.4, BTCUSD.4 (inverse)

    Settlement currency "NATIVE" is used for inverse perpetuals (COIN-M futures).
    """

    # Official CoinAlyze exchange codes from API
    # Source: curl https://api.coinalyze.net/v1/exchanges
    COINALYZE_EXCHANGE_CODES = {
        "binance": "A",
        "bybit": "6",
        "gateio": "Y",
        "huobi": "4",
        "okx": "3",
        "deribit": "2",
        "phemex": "7",
        "bitmex": "0",
        "dydx": "8",
        "bitfinex": "F",
        "woox": "W",
    }

    def __init__(self):
        logger.info(
            "Initialized CoinAlyzeSymbolRegistry with exchange-specific formatters"
        )

    def _format_binance_symbol(
        self, base_asset: str, market_type: str, settlement_currency: str | None
    ) -> str:
        """
        Binance symbol formats:
        - Linear USDT: BTCUSDT_PERP.A
        - Linear BUSD: BTCBUSD_PERP.A
        - Linear USDC: BTCUSDC_PERP.A
        - Inverse (NATIVE): BTCUSD_PERP.A (COIN-M futures)
        """
        base = base_asset.upper()

        if market_type == "linear_perpetual":
            if settlement_currency == "USDT":
                return f"{base}USDT_PERP.A"
            elif settlement_currency == "BUSD":
                return f"{base}BUSD_PERP.A"
            elif settlement_currency == "USDC":
                return f"{base}USDC_PERP.A"
            else:
                # Default to USDT
                return f"{base}USDT_PERP.A"
        elif market_type == "inverse_perpetual":
            # Based on pattern, try with _PERP suffix
            return f"{base}USD_PERP.A"
        else:
            raise ValueError(f"Unsupported market type for Binance: {market_type}")

    def _format_bybit_symbol(
        self, base_asset: str, market_type: str, settlement_currency: str | None
    ) -> str:
        """
        Bybit symbol formats (NO _PERP suffix):
        - Linear USDT: BTCUSDT.6 ‚úÖ (confirmed working)
        - Linear USDC: BTCUSDC.6
        - Inverse (NATIVE): BTCUSD.6 ‚úÖ (confirmed working)
        """
        base = base_asset.upper()

        if market_type == "linear_perpetual":
            if settlement_currency == "USDT":
                return f"{base}USDT.6"
            elif settlement_currency == "USDC":
                return f"{base}USDC.6"
            else:
                # Default to USDT
                return f"{base}USDT.6"
        elif market_type == "inverse_perpetual":
            return f"{base}USD.6"
        else:
            raise ValueError(f"Unsupported market type for Bybit: {market_type}")

    def _format_gateio_symbol(
        self, base_asset: str, market_type: str, settlement_currency: str | None
    ) -> str:
        """
        Gate.io symbol formats (underscore separator):
        - Linear USDT: BTC_USDT.Y ‚úÖ (confirmed working)
        - Linear USDC: BTC_USDC.Y (if available)
        - Inverse (NATIVE): BTC_USD.Y
        """
        base = base_asset.upper()

        if market_type == "linear_perpetual":
            if settlement_currency == "USDT":
                return f"{base}_USDT.Y"
            elif settlement_currency == "USDC":
                return f"{base}_USDC.Y"
            else:
                # Default to USDT
                return f"{base}_USDT.Y"
        elif market_type == "inverse_perpetual":
            return f"{base}_USD.Y"
        else:
            raise ValueError(f"Unsupported market type for Gate.io: {market_type}")

    def _format_huobi_symbol(
        self, base_asset: str, market_type: str, settlement_currency: str | None
    ) -> str:
        """
        Huobi symbol formats:
        - Linear USDT: BTCUSDT_PERP.4 ‚úÖ (confirmed working)
        - Linear USDC: BTCUSDC_PERP.4 (if available)
        - Inverse (NATIVE): BTCUSD.4
        """
        base = base_asset.upper()

        if market_type == "linear_perpetual":
            if settlement_currency == "USDT":
                return f"{base}USDT_PERP.4"
            elif settlement_currency == "USDC":
                return f"{base}USDC_PERP.4"
            else:
                # Default to USDT
                return f"{base}USDT_PERP.4"
        elif market_type == "inverse_perpetual":
            return f"{base}USD.4"
        else:
            raise ValueError(f"Unsupported market type for Huobi: {market_type}")

    def _format_okx_symbol(
        self, base_asset: str, market_type: str, settlement_currency: str | None
    ) -> str:
        """
        OKX symbol formats (needs testing):
        - Linear USDT: BTCUSDT_PERP.3 (assumed)
        - Inverse: BTCUSD.3 (assumed)
        """
        base = base_asset.upper()

        if market_type == "linear_perpetual":
            if settlement_currency == "USDT":
                return f"{base}USDT_PERP.3"
            elif settlement_currency == "USDC":
                return f"{base}USDC_PERP.3"
            else:
                return f"{base}USDT_PERP.3"
        elif market_type == "inverse_perpetual":
            return f"{base}USD.3"
        else:
            raise ValueError(f"Unsupported market type for OKX: {market_type}")

    def _format_deribit_symbol(
        self, base_asset: str, market_type: str, settlement_currency: str | None
    ) -> str:
        """
        Deribit symbol formats (needs testing):
        - Inverse: BTC-USD.2 or BTC_USD.2 (assumed)
        """
        base = base_asset.upper()

        if market_type == "inverse_perpetual":
            return f"{base}-USD.2"
        else:
            raise ValueError(f"Unsupported market type for Deribit: {market_type}")

    def get_coinalyze_symbol(
        self,
        base_asset: str,
        exchange: str,
        market_type: str,
        settlement_currency: str | None = None,
    ) -> str:
        """
        Generate CoinAlyze symbol using exchange-specific formatters.

        Args:
            base_asset: Base asset (e.g., "BTC")
            exchange: Exchange name (e.g., "binance")
            market_type: "linear_perpetual" or "inverse_perpetual"
            settlement_currency: "USDT", "USDC", "BUSD", or "NATIVE" for inverse

        Returns:
            CoinAlyze symbol string (e.g., "BTCUSDT_PERP.A")
        """
        exchange_lower = exchange.lower()

        # Route to exchange-specific formatter
        if exchange_lower == "binance":
            return self._format_binance_symbol(
                base_asset, market_type, settlement_currency
            )
        elif exchange_lower == "bybit":
            return self._format_bybit_symbol(
                base_asset, market_type, settlement_currency
            )
        elif exchange_lower == "gateio":
            return self._format_gateio_symbol(
                base_asset, market_type, settlement_currency
            )
        elif exchange_lower == "huobi":
            return self._format_huobi_symbol(
                base_asset, market_type, settlement_currency
            )
        elif exchange_lower == "okx":
            return self._format_okx_symbol(base_asset, market_type, settlement_currency)
        elif exchange_lower == "deribit":
            return self._format_deribit_symbol(
                base_asset, market_type, settlement_currency
            )
        else:
            raise ValueError(f"No CoinAlyze symbol formatter for exchange: {exchange}")

    def get_coinalyze_symbols_for_asset(
        self,
        base_asset: str,
        include_exchanges: list[str] | None = None,
        include_market_types: list[str] | None = None,
    ) -> list[str]:
        """Get all possible CoinAlyze symbols for a base asset"""
        symbols = []

        if include_exchanges is None:
            include_exchanges = ["binance", "bybit", "gateio", "huobi"]

        if include_market_types is None:
            include_market_types = ["linear_perpetual", "inverse_perpetual"]

        for exchange in include_exchanges:
            for market_type in include_market_types:
                try:
                    if market_type == "linear_perpetual":
                        # Try USDT
                        symbol_usdt = self.get_coinalyze_symbol(
                            base_asset, exchange, market_type, "USDT"
                        )
                        symbols.append(symbol_usdt)

                        # USDC for supported exchanges
                        if exchange in ["binance", "bybit"]:
                            symbol_usdc = self.get_coinalyze_symbol(
                                base_asset, exchange, market_type, "USDC"
                            )
                            symbols.append(symbol_usdc)

                        # BUSD for Binance
                        if exchange == "binance":
                            symbol_busd = self.get_coinalyze_symbol(
                                base_asset, exchange, market_type, "BUSD"
                            )
                            symbols.append(symbol_busd)

                    elif market_type == "inverse_perpetual":
                        symbol = self.get_coinalyze_symbol(
                            base_asset, exchange, market_type
                        )
                        symbols.append(symbol)

                except (ValueError, KeyError) as e:
                    logger.debug(
                        f"Skipping {exchange}/{market_type} for {base_asset}: {e}"
                    )
                    continue

        logger.info(f"Generated {len(symbols)} CoinAlyze symbols for {base_asset}")
        return symbols


# Global instance
coin_alyze_registry = CoinAlyzeSymbolRegistry()
</file>

<file path="ingestion/adapters/coinmetrics_plugin/__init__.py">

</file>

<file path="ingestion/adapters/coinmetrics_plugin/base.py">
## enherits (BaseAdapter)


venue = COINMETRICS
</file>

<file path="ingestion/adapters/coinmetrics_plugin/client.py">
l = "https://github.com/coinmetrics/data/blob/master/csv/{symbol}.csv"
symbol="btc"


def fetch_api_data(endpoint,params):
	handle http requests
</file>

<file path="ingestion/adapters/coinmetrics_plugin/onchain_adapter.py">
nenhirsts coinmetrics adapterbase

## inherits from coinmetricsaadapterbase and implements capability ports for coinmetrics onchain data.
</file>

<file path="ingestion/adapters/coinmetrics_plugin/repomix-output.xml">
This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
__init__.py
base.py
client.py
onchain_adapter.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="__init__.py">

</file>

<file path="base.py">
## enherits (BaseAdapter)


venue = COINMETRICS
</file>

<file path="client.py">
l = "https://github.com/coinmetrics/data/blob/master/csv/{symbol}.csv"
symbol="btc"


def fetch_api_data(endpoint,params):
	handle http requests
</file>

<file path="onchain_adapter.py">
nenhirsts coinmetrics adapterbase

## inherits from coinmetricsaadapterbase and implements capability ports for coinmetrics onchain data.
</file>

</files>
</file>

<file path="ingestion/adapters/__init__.py">
"""
Capability-based adapters package.
Exports core BaseAdapter and CCXT plugin capabilities.
"""

from quant_framework.ingestion.adapters.base import BaseAdapter
from quant_framework.ingestion.adapters.ccxt_plugin.ohlcv_adapter import (
    CCXTOHLCVAdapter,
)
from quant_framework.ingestion.adapters.ccxt_plugin.open_interest_adapter import (
    CCXTOpenInterestAdapter,
)

__all__ = [
    "BaseAdapter",
    "CCXTOHLCVAdapter",
    "CCXTOpenInterestAdapter",
]
</file>

<file path="ingestion/adapters/base.py">
"""
Enhanced base adapter with asset-agnostic, capability-based design.

Clarified data lineage through explicit venue + wrapper tracking.
"""

import logging
from abc import ABC, abstractmethod
from typing import TYPE_CHECKING, Any

from quant_framework.shared.models.enums import AssetClass
from quant_framework.ingestion.models.enums import (
    ClientType,
    ConnectionType,
)
from quant_framework.shared.models.enums import (
    DataVenue,
    WrapperImplementation,
)
from quant_framework.shared.models.instruments import Instrument

if TYPE_CHECKING:
    pass

logger = logging.getLogger(__name__)


class BaseAdapter(ABC):
    """
    Enhanced base adapter supporting all asset classes and capability-based design.

    Clarifies data lineage through explicit venue + wrapper tracking:
    - venue: WHERE the data originates (the actual exchange/source)
    - wrapper: HOW we access it (CCXT, CoinAlyze, Yahoo Finance, etc.)

    Key improvements over old BaseAdapter:
    1. Asset-agnostic: Works with any asset class via DataVenue enum
    2. Capability-based: Adapters declare which ports they implement
    3. Instrument-centric: Uses Instrument model instead of string symbols
    4. Metadata-rich: Exposes venue, wrapper, client_type, connection_type
    5. Layer separation: Returns raw data; no preprocessing/normalization
    6. Clear lineage: Explicit tracking of data source and access method

    Attributes:
        venue: Data venue (BINANCE_USDM, BYBIT, ICE_DATA, etc.) - WHERE data comes from
        wrapper: Wrapper implementation (COINALYZE, CCXT, YAHOO_FINANCE, NONE) - HOW we access it
        client_type: Type of client implementation (WRAPPER, NATIVE, etc.)
        connection_type: Network protocol (REST, WEBSOCKET, etc.)
        supported_asset_classes: Set of AssetClass values this adapter supports
        capabilities: Set of port classes this adapter implements

    Examples:
        # CoinAlyze adapter accessing Binance USD-M
        class CoinalyzeOHLCVAdapter(BaseAdapter):
            venue = DataVenue.BINANCE_USDM  # Can also be set per-instance
            wrapper = WrapperImplementation.COINALYZE
            client_type = ClientType.WRAPPER
            connection_type = ConnectionType.REST

        # CCXT adapter accessing Bybit
        class CCXTOHLCVAdapter(BaseAdapter):
            venue = DataVenue.BYBIT
            wrapper = WrapperImplementation.CCXT
            client_type = ClientType.WRAPPER
            connection_type = ConnectionType.REST
    """

    # ========== DATA LINEAGE (NEW: Clarified) ==========
    venue: DataVenue  # WHERE: The actual source
    wrapper: WrapperImplementation  # HOW: Access method

    # ========== TECHNICAL IMPLEMENTATION ==========
    client_type: ClientType
    connection_type: ConnectionType

    # ========== CAPABILITIES ==========
    supported_asset_classes: set[AssetClass] = set()
    capabilities: set[type] = set()

    def __init__(
        self,
        api_key: str | None = None,
        api_secret: str | None = None,
        passphrase: str | None = None,
        testnet: bool = False,
        config: dict[str, Any] | None = None,
    ):
        """Initialize adapter state and credentials."""
        self.api_key = api_key
        self.api_secret = api_secret
        self.passphrase = passphrase
        self.testnet = testnet
        self.config = config or {}
        self._connected = False

    @abstractmethod
    async def connect(self) -> None:
        """Establish connection to the data provider (idempotent)."""
        pass

    @abstractmethod
    async def close(self) -> None:
        """Close connection and clean up resources (idempotent)."""
        pass

    def supports_asset_class(self, asset_class: AssetClass) -> bool:
        """Check if this adapter supports the given asset class."""
        return asset_class in self.supported_asset_classes

    def supports_capability(self, port_type: type) -> bool:
        """Check if this adapter implements the given port."""
        return port_type in self.capabilities

    def validate_instrument(self, instrument: Instrument) -> bool:
        """Validate if instrument is supported by this adapter.

        Checks:
        1. Asset class support
        2. Venue match (with flexibility for AGGREGATED venues)
        3. Wrapper match (must match adapter's wrapper)
        4. Instrument is active
        """
        # Check asset class
        if not self.supports_asset_class(instrument.asset_class):
            logger.warning(
                f"{self.__class__.__name__} does not support asset class {instrument.asset_class}"
            )
            return False

        # Check venue match (with flexibility for AGGREGATED)
        # Adapters that access multiple venues (like CoinAlyze) should allow any venue
        if hasattr(self, "supports_multiple_venues") and self.supports_multiple_venues:
            # Multi-venue adapters (CoinAlyze, CCXT) - venue is validated elsewhere
            pass
        elif self.venue != DataVenue.AGGREGATED and instrument.venue != self.venue:
            logger.warning(
                f"Instrument venue {instrument.venue} does not match adapter venue {self.venue}"
            )
            return False

        # Check wrapper match (strict - wrapper must match)
        if instrument.wrapper != self.wrapper:
            logger.warning(
                f"Instrument wrapper {instrument.wrapper} does not match adapter wrapper {self.wrapper}"
            )
            return False

        # Check active status
        if not instrument.is_active:
            logger.warning(f"Instrument {instrument.instrument_id} is not active")
            return False

        return True

    async def __aenter__(self):
        await self.connect()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()

    def __repr__(self) -> str:
        return (
            f"{self.__class__.__name__}("
            f"venue={self.venue.value}, "
            f"wrapper={self.wrapper.value}, "
            f"client_type={self.client_type.value}, "
            f"authenticated={bool(self.api_key)}, "
            f"testnet={self.testnet})"
        )
</file>

<file path="ingestion/backfill/__init__.py">
"""Backfill package for historical data initialization."""

from .rate_limiter import IRateLimiter, CoinAlyzeRateLimiter
from .checkpoint_manager import (
    ICheckpointStore,
    BackfillCheckpoint,
    DatabaseCheckpointStore,
    CheckpointManager
)
from .coordinator import BackfillCoordinator, BackfillRequest, BackfillResult

__all__ = [
    'IRateLimiter',
    'CoinAlyzeRateLimiter',
    'ICheckpointStore',
    'BackfillCheckpoint',
    'DatabaseCheckpointStore',
    'CheckpointManager',
    'BackfillCoordinator',
    'BackfillRequest',
    'BackfillResult',
]
</file>

<file path="ingestion/backfill/checkpoint_manager.py">
"""
Checkpoint manager for tracking backfill progress.
Enables resumable backfills and progress monitoring.
"""

from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List, Optional, Protocol

import logging

logger = logging.getLogger(__name__)


@dataclass
class BackfillCheckpoint:
    """Represents a backfill progress checkpoint."""
    symbol: str
    timeframe: str
    data_type: str
    source: str
    last_completed_date: datetime
    total_chunks_completed: int
    total_records_written: int
    started_at: datetime
    updated_at: datetime
    status: str  # 'in_progress', 'completed', 'failed'
    error_message: Optional[str] = None


class ICheckpointStore(Protocol):
    """
    Protocol for checkpoint storage.
    Enables different backend implementations (database, file, cache).
    """

    async def save_checkpoint(self, checkpoint: BackfillCheckpoint) -> None:
        """
        Save or update checkpoint.
        
        Args:
            checkpoint: Checkpoint data to save
        """
        ...

    async def load_checkpoint(
        self,
        symbol: str,
        timeframe: str,
        data_type: str,
        source: str
    ) -> Optional[BackfillCheckpoint]:
        """
        Load checkpoint for specific combination.
        
        Args:
            symbol: Trading symbol
            timeframe: Data timeframe
            data_type: Type of data ('ohlc', 'oi')
            source: Data source name
            
        Returns:
            Checkpoint if exists, None otherwise
        """
        ...

    async def list_checkpoints(
        self,
        status: Optional[str] = None
    ) -> List[BackfillCheckpoint]:
        """
        List all checkpoints, optionally filtered by status.
        
        Args:
            status: Filter by status ('in_progress', 'completed', 'failed')
            
        Returns:
            List of checkpoints
        """
        ...

    async def delete_checkpoint(
        self,
        symbol: str,
        timeframe: str,
        data_type: str,
        source: str
    ) -> bool:
        """
        Delete checkpoint.
        
        Args:
            symbol: Trading symbol
            timeframe: Data timeframe
            data_type: Type of data
            source: Data source name
            
        Returns:
            True if deleted, False if not found
        """
        ...

    async def mark_completed(
        self,
        symbol: str,
        timeframe: str,
        data_type: str,
        source: str
    ) -> None:
        """
        Mark backfill as completed.
        
        Args:
            symbol: Trading symbol
            timeframe: Data timeframe
            data_type: Type of data
            source: Data source name
        """
        ...


class DatabaseCheckpointStore:
    """
    Database-backed checkpoint storage.
    Uses system.backfill_checkpoints table.
    """

    def __init__(self, db_adapter: 'IDatabaseAdapter'):
        """
        Initialize checkpoint store.
        
        Args:
            db_adapter: Database adapter instance
        """
        self.db = db_adapter
        self.table = "system.backfill_checkpoints"

    async def save_checkpoint(self, checkpoint: BackfillCheckpoint) -> None:
        """Save or update checkpoint in database."""
        query = f"""
            INSERT INTO {self.table} (
                symbol, timeframe, data_type, source,
                last_completed_date, total_chunks_completed, total_records_written,
                started_at, updated_at, status, error_message
            )
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
            ON CONFLICT (symbol, timeframe, data_type, source)
            DO UPDATE SET
                last_completed_date = EXCLUDED.last_completed_date,
                total_chunks_completed = EXCLUDED.total_chunks_completed,
                total_records_written = EXCLUDED.total_records_written,
                updated_at = EXCLUDED.updated_at,
                status = EXCLUDED.status,
                error_message = EXCLUDED.error_message
        """

        await self.db.execute_query(
            query,
            checkpoint.symbol,
            checkpoint.timeframe,
            checkpoint.data_type,
            checkpoint.source,
            checkpoint.last_completed_date,
            checkpoint.total_chunks_completed,
            checkpoint.total_records_written,
            checkpoint.started_at,
            checkpoint.updated_at,
            checkpoint.status,
            checkpoint.error_message
        )

        logger.debug(
            f"Saved checkpoint: {checkpoint.symbol}/{checkpoint.timeframe}/{checkpoint.data_type} "
            f"- {checkpoint.total_chunks_completed} chunks, {checkpoint.total_records_written} records"
        )

    async def load_checkpoint(
        self,
        symbol: str,
        timeframe: str,
        data_type: str,
        source: str
    ) -> Optional[BackfillCheckpoint]:
        """Load checkpoint from database."""
        query = f"""
            SELECT 
                symbol, timeframe, data_type, source,
                last_completed_date, total_chunks_completed, total_records_written,
                started_at, updated_at, status, error_message
            FROM {self.table}
            WHERE symbol = $1 
              AND timeframe = $2 
              AND data_type = $3 
              AND source = $4
        """

        result = await self.db.fetch_one(query, symbol, timeframe, data_type, source)

        if not result:
            return None

        return BackfillCheckpoint(
            symbol=result['symbol'],
            timeframe=result['timeframe'],
            data_type=result['data_type'],
            source=result['source'],
            last_completed_date=result['last_completed_date'],
            total_chunks_completed=result['total_chunks_completed'],
            total_records_written=result['total_records_written'],
            started_at=result['started_at'],
            updated_at=result['updated_at'],
            status=result['status'],
            error_message=result.get('error_message')
        )

    async def list_checkpoints(
        self,
        status: Optional[str] = None
    ) -> List[BackfillCheckpoint]:
        """List all checkpoints from database."""
        if status:
            query = f"""
                SELECT 
                    symbol, timeframe, data_type, source,
                    last_completed_date, total_chunks_completed, total_records_written,
                    started_at, updated_at, status, error_message
                FROM {self.table}
                WHERE status = $1
                ORDER BY updated_at DESC
            """
            results = await self.db.fetch_all(query, status)
        else:
            query = f"""
                SELECT 
                    symbol, timeframe, data_type, source,
                    last_completed_date, total_chunks_completed, total_records_written,
                    started_at, updated_at, status, error_message
                FROM {self.table}
                ORDER BY updated_at DESC
            """
            results = await self.db.fetch_all(query)

        return [
            BackfillCheckpoint(
                symbol=row['symbol'],
                timeframe=row['timeframe'],
                data_type=row['data_type'],
                source=row['source'],
                last_completed_date=row['last_completed_date'],
                total_chunks_completed=row['total_chunks_completed'],
                total_records_written=row['total_records_written'],
                started_at=row['started_at'],
                updated_at=row['updated_at'],
                status=row['status'],
                error_message=row.get('error_message')
            )
            for row in results
        ]

    async def delete_checkpoint(
        self,
        symbol: str,
        timeframe: str,
        data_type: str,
        source: str
    ) -> bool:
        """Delete checkpoint from database."""
        query = f"""
            DELETE FROM {self.table}
            WHERE symbol = $1 
              AND timeframe = $2 
              AND data_type = $3 
              AND source = $4
        """

        await self.db.execute_query(query, symbol, timeframe, data_type, source)
        logger.info(
            f"Deleted checkpoint: {symbol}/{timeframe}/{data_type}/{source}"
        )
        return True

    async def mark_completed(
        self,
        symbol: str,
        timeframe: str,
        data_type: str,
        source: str
    ) -> None:
        """Mark backfill as completed."""
        query = f"""
            UPDATE {self.table}
            SET status = 'completed',
                updated_at = NOW()
            WHERE symbol = $1 
              AND timeframe = $2 
              AND data_type = $3 
              AND source = $4
        """

        await self.db.execute_query(query, symbol, timeframe, data_type, source)
        logger.info(
            f"Marked completed: {symbol}/{timeframe}/{data_type}/{source}"
        )


class CheckpointManager:
    """
    High-level checkpoint management with automatic progress tracking.
    """

    def __init__(self, checkpoint_store: ICheckpointStore):
        """
        Initialize manager.
        
        Args:
            checkpoint_store: Checkpoint storage backend
        """
        self.store = checkpoint_store

    async def start_backfill(
        self,
        symbol: str,
        timeframe: str,
        data_type: str,
        source: str
    ) -> BackfillCheckpoint:
        """
        Start new backfill or resume existing one.
        
        Returns:
            Existing or new checkpoint
        """
        # Check for existing checkpoint
        existing = await self.store.load_checkpoint(
            symbol, timeframe, data_type, source
        )

        if existing and existing.status == 'in_progress':
            logger.info(
                f"Resuming backfill from checkpoint: {symbol}/{timeframe}/{data_type} "
                f"- Last completed: {existing.last_completed_date}"
            )
            return existing

        # Create new checkpoint
        now = datetime.utcnow()
        checkpoint = BackfillCheckpoint(
            symbol=symbol,
            timeframe=timeframe,
            data_type=data_type,
            source=source,
            last_completed_date=datetime.min,  # Will be updated on first chunk
            total_chunks_completed=0,
            total_records_written=0,
            started_at=now,
            updated_at=now,
            status='in_progress'
        )

        await self.store.save_checkpoint(checkpoint)
        logger.info(f"Started new backfill: {symbol}/{timeframe}/{data_type}")

        return checkpoint

    async def update_progress(
        self,
        checkpoint: BackfillCheckpoint,
        last_completed_date: datetime,
        records_written: int
    ) -> None:
        """
        Update checkpoint with progress.
        
        Args:
            checkpoint: Current checkpoint
            last_completed_date: Latest date completed
            records_written: Number of records written in this update
        """
        checkpoint.last_completed_date = last_completed_date
        checkpoint.total_chunks_completed += 1
        checkpoint.total_records_written += records_written
        checkpoint.updated_at = datetime.utcnow()

        await self.store.save_checkpoint(checkpoint)

        logger.debug(
            f"Updated progress: {checkpoint.symbol}/{checkpoint.timeframe} "
            f"- {checkpoint.total_chunks_completed} chunks, "
            f"{checkpoint.total_records_written} records"
        )

    async def complete_backfill(
        self,
        checkpoint: BackfillCheckpoint
    ) -> None:
        """
        Mark backfill as completed.
        
        Args:
            checkpoint: Checkpoint to complete
        """
        checkpoint.status = 'completed'
        checkpoint.updated_at = datetime.utcnow()
        await self.store.save_checkpoint(checkpoint)

        logger.info(
            f"‚úÖ Backfill completed: {checkpoint.symbol}/{checkpoint.timeframe}/{checkpoint.data_type} "
            f"- {checkpoint.total_chunks_completed} chunks, "
            f"{checkpoint.total_records_written} total records"
        )

    async def fail_backfill(
        self,
        checkpoint: BackfillCheckpoint,
        error_message: str
    ) -> None:
        """
        Mark backfill as failed.
        
        Args:
            checkpoint: Checkpoint to fail
            error_message: Error description
        """
        checkpoint.status = 'failed'
        checkpoint.error_message = error_message
        checkpoint.updated_at = datetime.utcnow()
        await self.store.save_checkpoint(checkpoint)

        logger.error(
            f"‚ùå Backfill failed: {checkpoint.symbol}/{checkpoint.timeframe}/{checkpoint.data_type} "
            f"- {error_message}"
        )

    async def get_progress_summary(self) -> Dict[str, any]:
        """
        Get overall progress summary.
        
        Returns:
            Dictionary with progress statistics
        """
        all_checkpoints = await self.store.list_checkpoints()

        in_progress = [c for c in all_checkpoints if c.status == 'in_progress']
        completed = [c for c in all_checkpoints if c.status == 'completed']
        failed = [c for c in all_checkpoints if c.status == 'failed']

        return {
            'total': len(all_checkpoints),
            'in_progress': len(in_progress),
            'completed': len(completed),
            'failed': len(failed),
            'total_records_written': sum(c.total_records_written for c in all_checkpoints),
            'checkpoints': {
                'in_progress': [
                    f"{c.symbol}/{c.timeframe}/{c.data_type}" 
                    for c in in_progress
                ],
                'completed': [
                    f"{c.symbol}/{c.timeframe}/{c.data_type}" 
                    for c in completed
                ],
                'failed': [
                    f"{c.symbol}/{c.timeframe}/{c.data_type}" 
                    for c in failed
                ]
            }
        }
</file>

<file path="ingestion/backfill/coordinator.py">
"""
Backfill Coordinator
Orchestrates historical data backfill with dependency injection.
"""

from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import logging
import yaml
from pathlib import Path

from quant_framework.ingestion.backfill.rate_limiter import IRateLimiter
from quant_framework.ingestion.backfill.checkpoint_manager import CheckpointManager, BackfillCheckpoint
from quant_framework.shared.models.instruments import Instrument

logger = logging.getLogger(__name__)


@dataclass
class BackfillRequest:
    """Specification for a backfill request."""
    symbols: List[str]
    timeframes: List[str]
    data_types: List[str]  # ['ohlc', 'oi']
    start_date: datetime
    end_date: datetime
    source: str = "coinalyze"
    exchanges: Optional[List[str]] = None  # For OI multi-exchange


@dataclass
class BackfillChunk:
    """Single chunk of backfill work."""
    symbol: str
    timeframe: str
    data_type: str
    exchange: str
    start_date: datetime
    end_date: datetime


@dataclass
class BackfillResult:
    """Result of a backfill operation."""
    symbol: str
    timeframe: str
    data_type: str
    exchange: str
    total_chunks: int
    successful_chunks: int
    total_records: int
    failed_chunks: List[Dict]
    duration_seconds: float
    status: str  # 'completed', 'partial', 'failed'


class BackfillCoordinator:
    """
    Coordinates historical data backfill with dependency injection.
    
    Responsibilities:
    - Generate optimal chunks based on rate limiting
    - Fetch data via adapter
    - Write to bronze layer via BronzeWriter
    - Track progress via CheckpointManager
    - Handle errors and retries
    """

    def __init__(
        self,
        data_adapter,  # CoinalyzeBackfillAdapter
        bronze_writer,  # BronzeWriter
        rate_limiter: IRateLimiter,
        checkpoint_manager: CheckpointManager,
        config_path: Optional[str] = None
    ):
        """
        Initialize coordinator with injected dependencies.
        
        Args:
            data_adapter: Adapter implementing fetch methods
            bronze_writer: Writer for MinIO/S3 bronze layer
            rate_limiter: Rate limiting strategy
            checkpoint_manager: Checkpoint tracking manager
            config_path: Path to backfill.yaml config
        """
        self.data_adapter = data_adapter
        self.bronze_writer = bronze_writer
        self.rate_limiter = rate_limiter
        self.checkpoint_manager = checkpoint_manager
        
        # Load configuration
        if config_path is None:
            project_root = Path(__file__).parent.parent.parent.parent.parent
            config_path = project_root / "config" / "backfill.yaml"
        
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # Extract config values
        exec_config = self.config.get('execution', {})
        self.max_retries = exec_config.get('max_retries', 3)
        self.retry_delay = exec_config.get('retry_delay_seconds', 5)
        self.checkpoint_interval = exec_config.get('checkpoint_interval_chunks', 10)
        
        self.global_floor_dates = self.config.get('global_floor_dates', {})
        
        logger.info("BackfillCoordinator initialized")

    async def execute_backfill(self, request: BackfillRequest) -> Dict[str, BackfillResult]:
        """
        Execute complete backfill request.
        
        Args:
            request: Backfill specification
            
        Returns:
            Dictionary mapping symbol/timeframe/data_type to results
        """
        logger.info(
            f"üöÄ Starting backfill: {len(request.symbols)} symbols, "
            f"{len(request.timeframes)} timeframes, {len(request.data_types)} data types"
        )
        
        results = {}
        
        # Process each combination
        for symbol in request.symbols:
            for timeframe in request.timeframes:
                for data_type in request.data_types:
                    # Determine exchanges to process
                    if data_type == 'oi' and request.exchanges:
                        exchanges = request.exchanges
                    else:
                        # OHLC typically from single exchange (e.g., binance)
                        exchanges = ['binance']
                    
                    for exchange in exchanges:
                        key = f"{symbol}/{timeframe}/{data_type}/{exchange}"
                        
                        try:
                            result = await self._execute_single_backfill(
                                symbol=symbol,
                                timeframe=timeframe,
                                data_type=data_type,
                                exchange=exchange,
                                start_date=request.start_date,
                                end_date=request.end_date,
                                source=request.source
                            )
                            results[key] = result
                            
                        except Exception as e:
                            logger.error(f"‚ùå Backfill failed for {key}: {e}")
                            results[key] = BackfillResult(
                                symbol=symbol,
                                timeframe=timeframe,
                                data_type=data_type,
                                exchange=exchange,
                                total_chunks=0,
                                successful_chunks=0,
                                total_records=0,
                                failed_chunks=[],
                                duration_seconds=0.0,
                                status='failed'
                            )
        
        # Log summary
        self._log_summary(results)
        
        return results

    async def _execute_single_backfill(
        self,
        symbol: str,
        timeframe: str,
        data_type: str,
        exchange: str,
        start_date: datetime,
        end_date: datetime,
        source: str
    ) -> BackfillResult:
        """
        Execute backfill for single symbol/timeframe/data_type combination.
        """
        start_time = datetime.utcnow()
        
        logger.info(
            f"üìä Backfilling {symbol}/{timeframe}/{data_type}/{exchange} "
            f"from {start_date.date()} to {end_date.date()}"
        )
        
        # Start or resume checkpoint
        checkpoint = await self.checkpoint_manager.start_backfill(
            symbol=symbol,
            timeframe=timeframe,
            data_type=data_type,
            source=source
        )
        
        # Adjust start date if resuming
        if checkpoint.last_completed_date > datetime.min:
            start_date = checkpoint.last_completed_date + timedelta(days=1)
            logger.info(f"üìç Resuming from checkpoint: {start_date.date()}")
        
        # Apply global floor date if exists
        if symbol in self.global_floor_dates:
            floor_date = datetime.fromisoformat(
                self.global_floor_dates[symbol]['date']
            )
            if start_date < floor_date:
                logger.info(
                    f"üìÖ Adjusting start date from {start_date.date()} "
                    f"to floor date {floor_date.date()}"
                )
                start_date = floor_date
        
        # Generate chunks
        chunks = self._generate_chunks(
            symbol=symbol,
            timeframe=timeframe,
            data_type=data_type,
            exchange=exchange,
            start_date=start_date,
            end_date=end_date
        )
        
        logger.info(f"üì¶ Generated {len(chunks)} chunks for processing")
        
        # Process chunks
        total_records = 0
        successful_chunks = 0
        failed_chunks = []
        
        for i, chunk in enumerate(chunks):
            try:
                records_written = await self._process_chunk(chunk, checkpoint)
                total_records += records_written
                successful_chunks += 1
                
                # Periodic checkpoint save
                if (i + 1) % self.checkpoint_interval == 0:
                    await self.checkpoint_manager.update_progress(
                        checkpoint=checkpoint,
                        last_completed_date=chunk.end_date,
                        records_written=0  # Already tracked in checkpoint
                    )
                    logger.info(
                        f"üíæ Checkpoint saved: {i + 1}/{len(chunks)} chunks processed"
                    )
                
            except Exception as e:
                logger.error(f"‚ùå Chunk failed: {chunk.start_date} - {chunk.end_date}: {e}")
                failed_chunks.append({
                    'chunk': chunk,
                    'error': str(e)
                })
        
        # Mark as completed
        if successful_chunks == len(chunks):
            await self.checkpoint_manager.complete_backfill(checkpoint)
            status = 'completed'
        elif successful_chunks > 0:
            status = 'partial'
        else:
            await self.checkpoint_manager.fail_backfill(
                checkpoint, f"All {len(chunks)} chunks failed"
            )
            status = 'failed'
        
        duration = (datetime.utcnow() - start_time).total_seconds()
        
        result = BackfillResult(
            symbol=symbol,
            timeframe=timeframe,
            data_type=data_type,
            exchange=exchange,
            total_chunks=len(chunks),
            successful_chunks=successful_chunks,
            total_records=total_records,
            failed_chunks=failed_chunks,
            duration_seconds=duration,
            status=status
        )
        
        logger.info(
            f"‚úÖ Backfill {status}: {symbol}/{timeframe}/{data_type} - "
            f"{successful_chunks}/{len(chunks)} chunks, "
            f"{total_records} records in {duration:.1f}s"
        )
        
        return result

    def _generate_chunks(
        self,
        symbol: str,
        timeframe: str,
        data_type: str,
        exchange: str,
        start_date: datetime,
        end_date: datetime
    ) -> List[BackfillChunk]:
        """
        Generate optimal chunks based on rate limiting strategy.
        """
        chunks = []
        chunk_size = self.rate_limiter.calculate_chunk_size(timeframe)
        
        current_start = start_date
        while current_start < end_date:
            chunk_end = min(current_start + chunk_size, end_date)
            
            chunks.append(BackfillChunk(
                symbol=symbol,
                timeframe=timeframe,
                data_type=data_type,
                exchange=exchange,
                start_date=current_start,
                end_date=chunk_end
            ))
            
            current_start = chunk_end
        
        return chunks

    async def _process_chunk(
        self,
        chunk: BackfillChunk,
        checkpoint: BackfillCheckpoint
    ) -> int:
        """
        Process single chunk: fetch data and write to bronze layer.
        
        Returns:
            Number of records written
        """
        logger.debug(
            f"Processing chunk: {chunk.symbol}/{chunk.timeframe}/{chunk.data_type} "
            f"{chunk.start_date.date()} to {chunk.end_date.date()}"
        )
        
        # Create instrument object
        from quant_framework.ingestion.models.enums import DataProvider
        
        instrument = Instrument(
            instrument_id=f"{chunk.exchange}:{chunk.symbol}",
            venue=chunk.exchange,
            symbol=chunk.symbol,
            provider=DataProvider.COINALYZE
        )
        
        # Fetch data
        if chunk.data_type == 'ohlc':
            records = await self.data_adapter.fetch_ohlcv_history(
                symbol=chunk.symbol,
                timeframe=chunk.timeframe,
                start_date=chunk.start_date,
                end_date=chunk.end_date,
                instrument=instrument
            )
        else:  # 'oi'
            records = await self.data_adapter.fetch_oi_history(
                symbol=chunk.symbol,
                timeframe=chunk.timeframe,
                start_date=chunk.start_date,
                end_date=chunk.end_date,
                instrument=instrument
            )
        
        if not records:
            logger.warning(f"No records returned for chunk {chunk.start_date.date()}")
            return 0
        
        # Group records by date for daily partitioning
        from collections import defaultdict
        records_by_date = defaultdict(list)
        
        for record in records:
            # Extract timestamp
            if isinstance(record, dict):
                ts = record.get('timestamp') or record.get('time')
            else:
                ts = getattr(record, 'timestamp', None) or getattr(record, 'time', None)
            
            if ts:
                if isinstance(ts, (int, float)):
                    ts = datetime.fromtimestamp(ts)
                elif isinstance(ts, str):
                    ts = datetime.fromisoformat(ts)
                
                date_key = ts.date()
                records_by_date[date_key].append(record)
        
        # Write each daily batch to bronze layer
        total_written = 0
        for date_key, daily_records in records_by_date.items():
            result = await self.bronze_writer.write_daily_batch(
                records=daily_records,
                symbol=chunk.symbol,
                timeframe=chunk.timeframe,
                data_type=chunk.data_type,
                date=datetime.combine(date_key, datetime.min.time())
            )
            
            if result.get('success'):
                written = result.get('records_written', 0)
                total_written += written
                logger.debug(
                    f"‚úÖ Wrote {written} records for {date_key} "
                    f"to {result.get('file_path')}"
                )
            else:
                logger.error(f"‚ùå Failed to write {date_key}: {result.get('error')}")
        
        # Update checkpoint
        await self.checkpoint_manager.update_progress(
            checkpoint=checkpoint,
            last_completed_date=chunk.end_date,
            records_written=total_written
        )
        
        return total_written

    def _log_summary(self, results: Dict[str, BackfillResult]) -> None:
        """Log summary of all backfill results."""
        logger.info("\n" + "=" * 80)
        logger.info("BACKFILL SUMMARY")
        logger.info("=" * 80)
        
        total_records = sum(r.total_records for r in results.values())
        total_duration = sum(r.duration_seconds for r in results.values())
        
        completed = sum(1 for r in results.values() if r.status == 'completed')
        partial = sum(1 for r in results.values() if r.status == 'partial')
        failed = sum(1 for r in results.values() if r.status == 'failed')
        
        logger.info(f"Total combinations: {len(results)}")
        logger.info(f"  ‚úÖ Completed: {completed}")
        logger.info(f"  ‚ö†Ô∏è  Partial: {partial}")
        logger.info(f"  ‚ùå Failed: {failed}")
        logger.info(f"Total records written: {total_records:,}")
        logger.info(f"Total duration: {total_duration:.1f}s")
        
        if failed > 0:
            logger.info("\nFailed backfills:")
            for key, result in results.items():
                if result.status == 'failed':
                    logger.info(f"  - {key}")
        
        logger.info("=" * 80 + "\n")
</file>

<file path="ingestion/backfill/rate_limiter.py">
"""
Rate limiting strategies for external data sources.
Prevents API throttling and data truncation.
"""

from abc import ABC, abstractmethod
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Protocol

import yaml
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class IRateLimiter(Protocol):
    """
    Protocol for rate limiting strategies.
    Enables different implementations per data source.
    """

    def normalize_timeframe(self, timeframe: str) -> str:
        """
        Normalize timeframe to standard format.
        
        Args:
            timeframe: Input timeframe (e.g., '1hour', '5min', '1h')
            
        Returns:
            Normalized timeframe in standard format (e.g., '1h', '5m')
        """
        ...

    def calculate_chunk_size(self, timeframe: str) -> timedelta:
        """
        Calculate optimal chunk size for timeframe.
        
        Args:
            timeframe: Standard timeframe format
            
        Returns:
            Time duration for safe chunk size
        """
        ...

    def should_reduce_chunk(self, points_received: int, timeframe: str) -> bool:
        """
        Determine if chunk size should be reduced based on response.
        
        Args:
            points_received: Number of data points in response
            timeframe: Timeframe being fetched
            
        Returns:
            True if approaching rate limit
        """
        ...

    def should_increase_chunk(self, points_received: int, timeframe: str) -> bool:
        """
        Determine if chunk size can be increased.
        
        Args:
            points_received: Number of data points in response
            timeframe: Timeframe being fetched
            
        Returns:
            True if safe to increase chunk size
        """
        ...

    def record_performance(
        self,
        timeframe: str,
        chunk_size_days: float,
        points_received: int,
        success: bool
    ) -> None:
        """
        Record performance metrics for adaptive chunking.
        
        Args:
            timeframe: Timeframe fetched
            chunk_size_days: Chunk size used (in days)
            points_received: Data points received
            success: Whether fetch succeeded
        """
        ...


class CoinAlyzeRateLimiter:
    """
    Rate limiter for CoinAlyze API with adaptive chunking.
    Prevents silent data truncation from API point limits.
    """

    def __init__(self, config_path: Optional[str] = None, verbose: bool = False):
        """
        Initialize rate limiter.
        
        Args:
            config_path: Path to backfill.yaml config file
            verbose: Enable verbose logging
        """
        self.verbose = verbose
        self.performance_stats: Dict[str, List[Dict]] = {}
        
        # Load configuration
        if config_path is None:
            # Default to config/backfill.yaml from project root
            project_root = Path(__file__).parent.parent.parent.parent.parent
            config_path = project_root / "config" / "backfill.yaml"
        
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        rate_config = config['rate_limiting']['coinalyze']
        self.max_points = rate_config['max_points_per_request']
        self.timeframe_aliases = rate_config['timeframe_aliases']
        self.timeframe_minutes = rate_config['timeframe_minutes']
        self.safe_chunk_days = rate_config['safe_chunk_days']
        
        adaptive_config = rate_config.get('adaptive_chunking', {})
        self.adaptive_enabled = adaptive_config.get('enabled', True)
        self.reduce_threshold = adaptive_config.get('reduce_threshold', 0.95)
        self.increase_threshold = adaptive_config.get('increase_threshold', 0.70)
        self.min_samples = adaptive_config.get('min_samples_for_adjustment', 3)
        
        logger.info(
            f"CoinAlyze rate limiter initialized: "
            f"max_points={self.max_points}, adaptive={self.adaptive_enabled}"
        )

    def normalize_timeframe(self, timeframe: str) -> str:
        """
        Normalize timeframe to standard CCXT format.
        
        Examples:
            '1hour' -> '1h'
            '5min' -> '5m'
            '1day' -> '1d'
            '1h' -> '1h' (already normalized)
        """
        normalized = self.timeframe_aliases.get(timeframe, timeframe)
        
        if normalized not in self.timeframe_minutes:
            logger.warning(
                f"Unknown timeframe '{timeframe}' (normalized to '{normalized}'). "
                f"Valid timeframes: {list(self.timeframe_minutes.keys())}"
            )
            # Default to 1h for unknown timeframes
            normalized = "1h"
        
        if self.verbose and normalized != timeframe:
            logger.debug(f"Normalized timeframe: {timeframe} -> {normalized}")
        
        return normalized

    def calculate_chunk_size(self, timeframe: str) -> timedelta:
        """
        Calculate optimal chunk size for timeframe.
        Uses safe defaults with optional adaptive adjustment.
        """
        normalized_tf = self.normalize_timeframe(timeframe)
        base_days = self.safe_chunk_days.get(normalized_tf, 36.0)
        
        # Apply adaptive adjustment if enabled and we have performance data
        if self.adaptive_enabled and normalized_tf in self.performance_stats:
            stats = self.performance_stats[normalized_tf]
            if len(stats) >= self.min_samples:
                avg_points = sum(s['points'] for s in stats) / len(stats)
                points_ratio = avg_points / self.max_points
                
                # Adjust chunk size based on historical performance
                if points_ratio > self.reduce_threshold:
                    # Getting too close to limit, reduce by 10%
                    adjusted_days = base_days * 0.9
                    logger.info(
                        f"Reducing chunk size for {normalized_tf}: "
                        f"{base_days:.1f} -> {adjusted_days:.1f} days "
                        f"(avg points: {avg_points:.0f})"
                    )
                    base_days = adjusted_days
                elif points_ratio < self.increase_threshold:
                    # Safe to increase, grow by 10%
                    adjusted_days = min(base_days * 1.1, self.safe_chunk_days[normalized_tf] * 1.5)
                    logger.info(
                        f"Increasing chunk size for {normalized_tf}: "
                        f"{base_days:.1f} -> {adjusted_days:.1f} days "
                        f"(avg points: {avg_points:.0f})"
                    )
                    base_days = adjusted_days
        
        return timedelta(days=base_days)

    def should_reduce_chunk(self, points_received: int, timeframe: str) -> bool:
        """
        Check if we're approaching API limits.
        Returns True if >= 95% of max points received.
        """
        threshold = self.max_points * self.reduce_threshold
        approaching_limit = points_received >= threshold
        
        if approaching_limit:
            logger.warning(
                f"Approaching rate limit for {timeframe}: "
                f"{points_received}/{self.max_points} points "
                f"({points_received/self.max_points*100:.1f}%)"
            )
        
        return approaching_limit

    def should_increase_chunk(self, points_received: int, timeframe: str) -> bool:
        """
        Check if safe to increase chunk size.
        Returns True if < 70% of max points received.
        """
        threshold = self.max_points * self.increase_threshold
        safe_to_increase = points_received < threshold
        
        if safe_to_increase and self.verbose:
            logger.debug(
                f"Safe to increase chunk for {timeframe}: "
                f"{points_received}/{self.max_points} points "
                f"({points_received/self.max_points*100:.1f}%)"
            )
        
        return safe_to_increase

    def record_performance(
        self,
        timeframe: str,
        chunk_size_days: float,
        points_received: int,
        success: bool
    ) -> None:
        """
        Record performance metrics for adaptive learning.
        """
        normalized_tf = self.normalize_timeframe(timeframe)
        
        if normalized_tf not in self.performance_stats:
            self.performance_stats[normalized_tf] = []
        
        self.performance_stats[normalized_tf].append({
            'chunk_size_days': chunk_size_days,
            'points': points_received,
            'success': success,
            'timestamp': datetime.utcnow()
        })
        
        # Keep only recent history (last 100 samples)
        if len(self.performance_stats[normalized_tf]) > 100:
            self.performance_stats[normalized_tf] = self.performance_stats[normalized_tf][-100:]
        
        if self.verbose:
            logger.debug(
                f"Recorded performance: {normalized_tf}, "
                f"chunk={chunk_size_days:.1f}d, points={points_received}, "
                f"success={success}"
            )

    def validate_chunk_response(
        self,
        points_received: int,
        timeframe: str,
        expected_range: Optional[tuple] = None
    ) -> Dict[str, any]:
        """
        Validate API response for truncation or issues.
        
        Args:
            points_received: Number of points in response
            timeframe: Timeframe of request
            expected_range: Optional (min, max) expected point range
            
        Returns:
            Validation result dictionary with warnings/errors
        """
        result = {
            'valid': True,
            'warnings': [],
            'truncated': False
        }
        
        # Check for API truncation
        if points_received >= self.max_points:
            result['valid'] = False
            result['truncated'] = True
            result['warnings'].append(
                f"TRUNCATION DETECTED: Received {points_received} points "
                f"(>= max {self.max_points}). Data may be incomplete!"
            )
        
        # Check if approaching limit
        elif self.should_reduce_chunk(points_received, timeframe):
            result['warnings'].append(
                f"Approaching limit: {points_received}/{self.max_points} points. "
                f"Consider reducing chunk size."
            )
        
        # Check expected range if provided
        if expected_range:
            min_expected, max_expected = expected_range
            if points_received < min_expected:
                result['warnings'].append(
                    f"Fewer points than expected: {points_received} < {min_expected}"
                )
            elif points_received > max_expected:
                result['warnings'].append(
                    f"More points than expected: {points_received} > {max_expected}"
                )
        
        return result

    def get_performance_summary(self) -> Dict[str, Dict]:
        """
        Get performance statistics summary for all timeframes.
        """
        summary = {}
        
        for tf, stats in self.performance_stats.items():
            if not stats:
                continue
            
            successful = [s for s in stats if s['success']]
            summary[tf] = {
                'total_fetches': len(stats),
                'successful_fetches': len(successful),
                'avg_points': sum(s['points'] for s in successful) / len(successful) if successful else 0,
                'max_points': max(s['points'] for s in successful) if successful else 0,
                'avg_chunk_size': sum(s['chunk_size_days'] for s in successful) / len(successful) if successful else 0,
                'success_rate': len(successful) / len(stats) * 100 if stats else 0
            }
        
        return summary
</file>

<file path="ingestion/connectors/__init__.py">
"""
Connectors for market data providers.
"""
</file>

<file path="ingestion/connectors/rest.py">
"""
REST connectors for market data ingestion.
Shared CCXT utilities and HTTP client wrappers.
"""

import logging

import aiohttp

logger = logging.getLogger(__name__)


class RestConnector:
    """
    REST connector for HTTP-based data fetching.
    Currently a placeholder for future abstraction of CCXT HTTP client.
    """

    def __init__(self, timeout: int = 30):
        """Initialize REST connector."""
        self.timeout = timeout
        self.session: aiohttp.ClientSession | None = None

    async def __aenter__(self):
        """Async context manager entry."""
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        if self.session:
            await self.session.close()

    async def get(
        self,
        url: str,
        params: dict | None = None,
        headers: dict | None = None,
    ) -> dict:
        """Perform GET request."""
        if not self.session:
            raise RuntimeError("Connector not initialized (use async context manager)")

        async with self.session.get(
            url, params=params, headers=headers, timeout=self.timeout
        ) as response:
            if response.status == 200:
                return await response.json()
            else:
                raise Exception(f"HTTP {response.status}: {await response.text()}")
</file>

<file path="ingestion/factories/adapter_factory.py">
"""
Adapter factory for capability-based ingestion.

Creates adapters based on data provider and desired class.
"""

from __future__ import annotations

from collections.abc import Callable
from typing import Any

from quant_framework.ingestion.adapters.base import BaseAdapter
from quant_framework.ingestion.models.enums import DataProvider

AdapterBuilder = Callable[..., BaseAdapter]


class AdapterFactory:
    """Registry-driven factory for adapters."""

    def __init__(self) -> None:
        self._registry: dict[DataProvider, AdapterBuilder] = {}

    def register(self, provider: DataProvider, builder: AdapterBuilder) -> None:
        self._registry[provider] = builder

    def create(self, provider: DataProvider, *args: Any, **kwargs: Any) -> BaseAdapter:
        if provider not in self._registry:
            raise ValueError(f"No adapter registered for provider {provider}")
        return self._registry[provider](*args, **kwargs)

    def available_providers(self) -> list[DataProvider]:
        return list(self._registry.keys())
</file>

<file path="ingestion/factories/preprocessor_factory.py">
"""
Preprocessor factory for provider-specific quirks.
"""

from __future__ import annotations

from collections.abc import Callable
from typing import Any

from quant_framework.ingestion.models.enums import DataProvider
from quant_framework.ingestion.preprocessing.base import DataPreprocessor

PreprocessorBuilder = Callable[..., DataPreprocessor]


class PreprocessorFactory:
    """Registry-driven factory for preprocessors."""

    def __init__(self) -> None:
        self._registry: dict[DataProvider, PreprocessorBuilder] = {}

    def register(self, provider: DataProvider, builder: PreprocessorBuilder) -> None:
        self._registry[provider] = builder

    def create(self, provider: DataProvider, *args: Any, **kwargs: Any) -> DataPreprocessor:
        if provider not in self._registry:
            raise ValueError(f"No preprocessor registered for provider {provider}")
        return self._registry[provider](*args, **kwargs)

    def available_providers(self) -> list[DataProvider]:
        return list(self._registry.keys())
</file>

<file path="ingestion/models/__init__.py">
"""Ingestion data models."""
</file>

<file path="ingestion/models/enums.py">
"""
Foundational enums for multi-asset, pluggable ingestion framework.

These enums support asset-agnostic design and decouple the framework from specific
data providers (CCXT, Bloomberg, etc.).
"""
from enum import Enum


class DataProvider(str, Enum):
    """
    Multi-asset data provider enum.
    
    Replaces the crypto-specific Exchange enum to support equities, commodities,
    forex, and other asset classes beyond cryptocurrency.
    """
    # Cryptocurrency Exchanges
    BINANCE = "binance"
    BYBIT = "bybit"
    GATEIO = "gateio"
    HUOBI = "huobi"
    BITGET = "bitget"
    OKX = "okx"
    KRAKEN = "kraken"
    COINBASE = "coinbase"

    # Traditional Market Data Providers
    ICE = "ice"  # ICE Data Services
    BLOOMBERG = "bloomberg"
    REFINITIV = "refinitiv"
    REUTERS = "reuters"
    FACTSET = "factset"

    # Forex Providers
    OANDA = "oanda"
    FXCM = "fxcm"

    # Commodity Providers
    CME = "cme"

    # Generic
    INTERNAL = "internal"  # Internal data generation
    FILE = "file"  # File-based data


class ClientType(str, Enum):
    """
    Type of client implementation used by adapter.
    
    Distinguishes between different integration patterns:
    - WRAPPER: Uses third-party wrapper (e.g., CCXT)
    - NATIVE: Direct integration with provider's API
    - VENDOR_SDK: Uses provider's official SDK
    - FILE_PARSER: Reads from files (CSV, Parquet, etc.)
    """
    WRAPPER = "wrapper"  # e.g., CCXT wrapping exchange APIs
    NATIVE = "native"  # Direct REST/WebSocket integration
    VENDOR_SDK = "vendor_sdk"  # Official SDK (e.g., Bloomberg Terminal API)
    FILE_PARSER = "file_parser"  # File-based ingestion


class ConnectionType(str, Enum):
    """
    Network connection type used by adapter.
    
    Specifies the protocol/mechanism for data retrieval.
    """
    REST = "rest"
    WEBSOCKET = "websocket"
    FIX = "fix"  # Financial Information eXchange protocol
    GRPC = "grpc"
    SFTP = "sftp"
    FILE = "file"  # Local file system
    DATABASE = "database"  # Direct database connection


class PipelineMode(str, Enum):
    """
    Execution mode for data ingestion pipeline.
    
    Used by orchestration layer to determine behavior:
    - BACKFILL: Historical data fetch with pagination
    - INCREMENTAL: Recent data fetch (e.g., last N candles)
    - STREAMING: Real-time continuous data feed
    """
    BACKFILL = "backfill"
    INCREMENTAL = "incremental"
    STREAMING = "streaming"


class IngestionStatus(str, Enum):
    """Status of data ingestion operation."""
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    PARTIAL = "partial"  # Some data retrieved, some failed
    SKIPPED = "skipped"
</file>

<file path="ingestion/orchestration/data_fetcher.py">
"""
DataFetchOrchestrator coordinates adapter ‚Üí preprocessor ‚Üí normalizer pipeline.
"""

from __future__ import annotations

from collections.abc import Iterable
from typing import Protocol

from quant_framework.ingestion.adapters.base import BaseAdapter
from quant_framework.ingestion.preprocessing.base import DataPreprocessor
from quant_framework.shared.models.instruments import Instrument


class Normalizer(Protocol):
    def __call__(
        self, instrument: Instrument, row: dict, timeframe: str
    ):  # pragma: no cover - structural
        ...


class DataFetchOrchestrator:
    """Coordinates fetching raw data, preprocessing quirks, and normalizing to domain models."""

    def __init__(
        self,
        adapter: BaseAdapter,
        preprocessor: DataPreprocessor,
        ohlcv_normalizer: Normalizer,
        open_interest_normalizer: Normalizer,
    ) -> None:
        self.adapter = adapter
        self.preprocessor = preprocessor
        self.ohlcv_normalizer = ohlcv_normalizer
        self.open_interest_normalizer = open_interest_normalizer

    async def fetch_ohlcv(
        self,
        instrument: Instrument,
        timeframe: str,
        start=None,
        end=None,
        limit: int | None = None,
    ) -> Iterable[object]:
        raw = await self.adapter.fetch_ohlcv(instrument, timeframe, start, end, limit)
        preprocessed = self.preprocessor.preprocess_ohlcv(instrument, raw)
        return [
            self.ohlcv_normalizer(instrument, row, timeframe) for row in preprocessed
        ]

    async def fetch_open_interest(
        self,
        instrument: Instrument,
        timeframe: str,
        start=None,
        end=None,
        limit: int | None = None,
    ) -> Iterable[object]:
        raw = await self.adapter.fetch_open_interest(
            instrument, timeframe, start, end, limit
        )
        preprocessed = self.preprocessor.preprocess_open_interest(instrument, raw)
        return [
            self.open_interest_normalizer(instrument, row, timeframe)
            for row in preprocessed
        ]
</file>

<file path="ingestion/pipelines/__init__.py">
"""
Orchestration pipelines for market data ingestion.
Placeholder for future pipeline implementations.
"""
</file>

<file path="ingestion/ports/__init__.py">
"""Data ports for capability-based adapter design."""
</file>

<file path="ingestion/ports/data_ports.py">
"""
Data port protocols for capability-based adapters.

Ports define the raw data contracts that adapters must satisfy before
preprocessing/normalization. This enables layered architecture and
provider-agnostic orchestration.
"""

from __future__ import annotations

from collections.abc import Iterable
from datetime import datetime
from typing import Any, Protocol

from quant_framework.shared.models.instruments import Instrument


class OHLCVPort(Protocol):
    """Raw OHLCV data port."""

    async def fetch_ohlcv(
        self,
        instrument: Instrument,
        timeframe: str,
        start: datetime | None = None,
        end: datetime | None = None,
        limit: int | None = None,
    ) -> Iterable[dict[str, Any]]:
        """Fetch raw OHLCV candles for the given instrument."""


class OpenInterestPort(Protocol):
    """Raw open interest data port."""

    async def fetch_open_interest(
        self,
        instrument: Instrument,
        timeframe: str,
        start: datetime | None = None,
        end: datetime | None = None,
        limit: int | None = None,
    ) -> Iterable[dict[str, Any]]:
        """Fetch raw open interest snapshots for the given instrument."""
</file>

<file path="ingestion/preprocessing/base.py">
"""
Preprocessing layer interface for provider-specific quirks.

Preprocessors convert raw adapter payloads into normalized intermediary
records before domain normalization. They handle:
- Parameter adjustments (e.g., timeframe mapping, pagination hints)
- Field renaming/conversions (e.g., timestamps in seconds vs ms)
- Provider quirks (e.g., COIN-M openInterestValue vs openInterestAmount)
"""

from __future__ import annotations

from abc import ABC, abstractmethod
from collections.abc import Iterable
from typing import Any

from quant_framework.shared.models.instruments import Instrument


class DataPreprocessor(ABC):
    """Abstract preprocessor for provider-specific quirks."""

    @abstractmethod
    def preprocess_ohlcv(
        self,
        instrument: Instrument,
        raw: Iterable[dict[str, Any]],
    ) -> Iterable[dict[str, Any]]:
        """Transform raw OHLCV payloads into normalized intermediary rows."""

    @abstractmethod
    def preprocess_open_interest(
        self,
        instrument: Instrument,
        raw: Iterable[dict[str, Any]],
    ) -> Iterable[dict[str, Any]]:
        """Transform raw open interest payloads into normalized intermediary rows."""
</file>

<file path="ingestion/preprocessing/providers.py">
"""
Exchange-specific preprocessors extracting common fields and harmonizing payloads.
"""

from __future__ import annotations

import logging
from collections.abc import Iterable
from typing import Any

from quant_framework.ingestion.preprocessing.base import DataPreprocessor
from quant_framework.shared.models.instruments import Instrument


def _to_ms(value: Any) -> Any:
    try:
        # Some exchanges return seconds, others ms
        return int(value) if int(value) > 10_000_000_000 else int(value) * 1000
    except Exception:
        return value


class BinancePreprocessor(DataPreprocessor):
    def preprocess_ohlcv(
        self,
        instrument: Instrument,
        raw: Iterable[dict[str, Any]],
    ) -> Iterable[dict[str, Any]]:
        for row in raw:
            ts = row[0] if isinstance(row, (list, tuple)) else row.get("timestamp")
            yield {
                "timestamp": _to_ms(ts),
                "open": row[1] if isinstance(row, (list, tuple)) else row.get("open"),
                "high": row[2] if isinstance(row, (list, tuple)) else row.get("high"),
                "low": row[3] if isinstance(row, (list, tuple)) else row.get("low"),
                "close": row[4] if isinstance(row, (list, tuple)) else row.get("close"),
                "volume": row[5]
                if isinstance(row, (list, tuple))
                else row.get("volume"),
            }

    def preprocess_open_interest(
        self,
        instrument: Instrument,
        raw: Iterable[dict[str, Any]],
    ) -> Iterable[dict[str, Any]]:
        for row in raw:
            amount = row.get("openInterestAmount") or row.get("sumOpenInterest")
            value = row.get("openInterestValue")
            yield {
                "timestamp": _to_ms(row.get("timestamp") or row.get("ts")),
                "open_interest_amount": amount,
                "open_interest_value": value,
                "settlement_currency": row.get("settlementCurrency"),
            }


class BybitPreprocessor(DataPreprocessor):
    def preprocess_ohlcv(
        self, instrument: Instrument, raw: Iterable[dict[str, Any]]
    ) -> Iterable[dict[str, Any]]:
        for row in raw:
            ts = row[0] if isinstance(row, (list, tuple)) else row.get("timestamp")
            yield {
                "timestamp": _to_ms(ts),
                "open": row[1] if isinstance(row, (list, tuple)) else row.get("open"),
                "high": row[2] if isinstance(row, (list, tuple)) else row.get("high"),
                "low": row[3] if isinstance(row, (list, tuple)) else row.get("low"),
                "close": row[4] if isinstance(row, (list, tuple)) else row.get("close"),
                "volume": row[5]
                if isinstance(row, (list, tuple))
                else row.get("volume"),
            }

    def preprocess_open_interest(
        self, instrument: Instrument, raw: Iterable[dict[str, Any]]
    ) -> Iterable[dict[str, Any]]:
        for row in raw:
            amount = row.get("openInterest")
            if amount is None:
                continue
            yield {
                "timestamp": _to_ms(row.get("timestamp") or row.get("ts")),
                "open_interest_amount": amount,
                "open_interest_value": row.get("openInterestValue"),
                "settlement_currency": row.get("settlementCurrency"),
            }


class GateIOPreprocessor(DataPreprocessor):
    def preprocess_ohlcv(
        self, instrument: Instrument, raw: Iterable[dict[str, Any]]
    ) -> Iterable[dict[str, Any]]:
        for row in raw:
            ts = row[0] if isinstance(row, (list, tuple)) else row.get("timestamp")
            yield {
                "timestamp": _to_ms(ts),
                "open": row[1] if isinstance(row, (list, tuple)) else row.get("open"),
                "high": row[2] if isinstance(row, (list, tuple)) else row.get("high"),
                "low": row[3] if isinstance(row, (list, tuple)) else row.get("low"),
                "close": row[4] if isinstance(row, (list, tuple)) else row.get("close"),
                "volume": row[5]
                if isinstance(row, (list, tuple))
                else row.get("volume"),
            }

    def preprocess_open_interest(
        self, instrument: Instrument, raw: Iterable[dict[str, Any]]
    ) -> Iterable[dict[str, Any]]:
        for row in raw:
            yield {
                "timestamp": _to_ms(row.get("t") or row.get("timestamp")),
                "open_interest_amount": row.get("v") or row.get("openInterest"),
                "open_interest_value": row.get("openInterestValue"),
                "settlement_currency": row.get("settlementCurrency"),
            }


class HuobiPreprocessor(DataPreprocessor):
    def preprocess_ohlcv(
        self, instrument: Instrument, raw: Iterable[dict[str, Any]]
    ) -> Iterable[dict[str, Any]]:
        for row in raw:
            ts = (
                row[0]
                if isinstance(row, (list, tuple))
                else row.get("id") or row.get("timestamp")
            )
            yield {
                "timestamp": _to_ms(ts),
                "open": row[1] if isinstance(row, (list, tuple)) else row.get("open"),
                "high": row[2] if isinstance(row, (list, tuple)) else row.get("high"),
                "low": row[3] if isinstance(row, (list, tuple)) else row.get("low"),
                "close": row[4] if isinstance(row, (list, tuple)) else row.get("close"),
                "volume": row[5] if isinstance(row, (list, tuple)) else row.get("vol"),
            }

    def preprocess_open_interest(
        self, instrument: Instrument, raw: Iterable[dict[str, Any]]
    ) -> Iterable[dict[str, Any]]:
        for row in raw:
            yield {
                "timestamp": _to_ms(row.get("ts") or row.get("timestamp")),
                "open_interest_amount": row.get("volume") or row.get("openInterest"),
                "open_interest_value": row.get("openInterestValue"),
                "settlement_currency": row.get("settlementCurrency"),
            }


class BitgetPreprocessor(DataPreprocessor):
    def preprocess_ohlcv(
        self, instrument: Instrument, raw: Iterable[dict[str, Any]]
    ) -> Iterable[dict[str, Any]]:
        for row in raw:
            ts = row[0] if isinstance(row, (list, tuple)) else row.get("timestamp")
            yield {
                "timestamp": _to_ms(ts),
                "open": row[1] if isinstance(row, (list, tuple)) else row.get("open"),
                "high": row[2] if isinstance(row, (list, tuple)) else row.get("high"),
                "low": row[3] if isinstance(row, (list, tuple)) else row.get("low"),
                "close": row[4] if isinstance(row, (list, tuple)) else row.get("close"),
                "volume": row[5]
                if isinstance(row, (list, tuple))
                else row.get("volume"),
            }

    def preprocess_open_interest(
        self, instrument: Instrument, raw: Iterable[dict[str, Any]]
    ) -> Iterable[dict[str, Any]]:
        for row in raw:
            yield {
                "timestamp": _to_ms(row.get("timestamp") or row.get("ts")),
                "open_interest_amount": row.get("size") or row.get("openInterest"),
                "open_interest_value": row.get("openInterestValue"),
                "settlement_currency": row.get("marginCoin")
                or row.get("settlementCurrency"),
            }


"""CoinAlyze preprocessor for transforming raw API responses."""


logger = logging.getLogger(__name__)


class CoinalyzePreprocessor(DataPreprocessor):
    """
    Transform CoinAlyze API responses into normalized format.

    Handles exchange-specific response structures and field extraction.
    """

    def preprocess_ohlcv(
        self,
        instrument: Instrument,
        raw: list[dict[str, Any]],
    ) -> Iterable[dict[str, Any]]:
        """
        Transform CoinAlyze OHLCV response structure:

        Input format:
        [
            {
                "symbol": "BTCUSDT_PERP.A",
                "history": [
                    {"t": timestamp, "o": open, "h": high, "l": low, "c": close, "v": volume},
                    ...
                ]
            }
        ]

        Output format:
        Iterator of: {
            "timestamp": ms,
            "open": float,
            "high": float,
            "low": float,
            "close": float,
            "volume": float
        }
        """
        for symbol_data in raw:
            history = symbol_data.get("history", [])
            for item in history:
                yield {
                    "timestamp": self._to_ms(item["t"]),
                    "open": float(item["o"]),
                    "high": float(item["h"]),
                    "low": float(item["l"]),
                    "close": float(item["c"]),
                    "volume": float(item.get("v", 0)),
                }

    def preprocess_open_interest(
        self,
        instrument: Instrument,
        raw: list[dict[str, Any]],
    ) -> Iterable[dict[str, Any]]:
        """
        Transform CoinAlyze Open Interest response.

        CoinAlyze returns OI in OHLC format - we use 'c' (close) as the OI value.
        """
        for symbol_data in raw:
            history = symbol_data.get("history", [])
            for item in history:
                yield {
                    "timestamp": self._to_ms(item["t"]),
                    "open_interest_amount": float(item["c"]),
                    "open_interest_value": float(item["c"]),
                    "settlement_currency": instrument.settlement_currency,
                }

    def _to_ms(self, timestamp: int) -> int:
        """Convert timestamp to milliseconds if needed"""
        return timestamp * 1000 if timestamp < 10_000_000_000 else timestamp
</file>

<file path="ingestion/symbol_resolution/resolver.py">
"""
SymbolResolver bridges legacy symbol registry to the Instrument domain model.
"""

from __future__ import annotations

from typing import Protocol

from quant_framework.shared.models.instruments import Instrument


class SymbolRegistry(Protocol):
    def resolve(self, symbol: str) -> dict:
        ...


class SymbolResolver:
    """Converts legacy registry output into Instrument instances."""

    def __init__(self, registry: SymbolRegistry):
        self.registry = registry

    def resolve(self, symbol: str) -> Instrument:
        data = self.registry.resolve(symbol)
        return Instrument(**data)
</file>

<file path="ingestion/__init__.py">
"""
Ingestion module for market data collection.
Clean architecture with adapters, connectors, and pipelines.
"""
</file>

<file path="ingestion/service.py">
"""
IngestionService: public entrypoint for capability-based ingestion.
"""

from __future__ import annotations

from typing import Any

from quant_framework.ingestion.factories.adapter_factory import AdapterFactory
from quant_framework.ingestion.factories.preprocessor_factory import PreprocessorFactory
from quant_framework.ingestion.models.enums import DataProvider
from quant_framework.ingestion.orchestration.data_fetcher import DataFetchOrchestrator
from quant_framework.shared.models.instruments import Instrument


class IngestionService:
    """Composes adapter, preprocessor, and normalizers to serve ingestion requests."""

    def __init__(
        self,
        adapter_factory: AdapterFactory,
        preprocessor_factory: PreprocessorFactory,
        ohlcv_normalizer,
        open_interest_normalizer,
    ) -> None:
        self.adapter_factory = adapter_factory
        self.preprocessor_factory = preprocessor_factory
        self.ohlcv_normalizer = ohlcv_normalizer
        self.open_interest_normalizer = open_interest_normalizer

    def _build_orchestrator(self, provider: DataProvider, *args: Any, **kwargs: Any) -> DataFetchOrchestrator:
        adapter = self.adapter_factory.create(provider, *args, **kwargs)
        preprocessor = self.preprocessor_factory.create(provider)
        return DataFetchOrchestrator(adapter, preprocessor, self.ohlcv_normalizer, self.open_interest_normalizer)

    async def fetch_ohlcv(
        self,
        provider: DataProvider,
        instrument: Instrument,
        timeframe: str,
        start=None,
        end=None,
        limit: int | None = None,
        **adapter_kwargs: Any,
    ):
        orchestrator = self._build_orchestrator(provider, **adapter_kwargs)
        return await orchestrator.fetch_ohlcv(instrument, timeframe, start, end, limit)

    async def fetch_open_interest(
        self,
        provider: DataProvider,
        instrument: Instrument,
        timeframe: str,
        start=None,
        end=None,
        limit: int | None = None,
        **adapter_kwargs: Any,
    ):
        orchestrator = self._build_orchestrator(provider, **adapter_kwargs)
        return await orchestrator.fetch_open_interest(instrument, timeframe, start, end, limit)
</file>

<file path="shared/enums/__init__.py">
"""
Shared enumerations for market data ingestion.
Re-exports from mnemo_quant.models.enums for compatibility.
"""

from mnemo_quant.models.enums import (
    AssetClass,
    Exchange,
    MarketDataType,
    MarketType,
)

__all__ = ["AssetClass", "Exchange", "MarketDataType", "MarketType"]
</file>

<file path="shared/models/__init__.py">
"""Shared domain models."""

from quant_framework.shared.models.enums import (
    AssetClass,
    CandleType,
    ContractType,
    DataProvider,
    Exchange,
    InstrumentType,
    MarketDataType,
    MarketType,
    OrderType,
    PriceType,
)
from quant_framework.shared.models.instruments import Instrument

__all__ = [
    # Enums
    "AssetClass",
    "ContractType",
    "CandleType",
    "PriceType",
    "InstrumentType",
    "MarketType",
    "OrderType",
    "MarketDataType",
    "Exchange",
    "DataProvider",
    # Models
    "Instrument",
]
</file>

<file path="shared/models/enums.py">
"""
Shared enumerations for the Quant Framework.

Refactored to separate data venues (WHERE) from wrappers (HOW).
Khraisha ¬ß2 p 44-54: Multi-asset, multi-venue support requires clear type definitions.
"""

import enum


# ============================================================================
# ASSET & INSTRUMENT CLASSIFICATION
# ============================================================================
class AssetClass(str, enum.Enum):
    """Layer 1-4: Fundamental asset classification."""

    CRYPTO = "crypto"
    EQUITY = "equity"
    FUTURE = "future"
    OPTION = "option"
    FOREX = "forex"
    COMMODITY = "commodity"
    RATES = "rates"
    INDEX = "index"
    OTHER = "other"


class ContractType(str, enum.Enum):
    """Layer 1-4: Contract settlement type - needed for ingestion!
    Khraisha: Preserve ingestion context in canonical forms."""

    LINEAR = "linear"
    INVERSE = "inverse"


class CandleType(str, enum.Enum):
    """Layer 1-4: Price feed types - needed during ingestion
    to distinguish what we're pulling from exchanges."""

    SPOT = "spot"
    FUTURES = "futures"
    MARK = "mark"
    INDEX = "index"
    FUNDING_RATE = "funding_rate"


class PriceType(str, enum.Enum):
    """Layer 1-4: Price reference types - exchanges provide different prices."""

    LAST = "last"
    MARK = "mark"
    INDEX = "index"


class InstrumentType(str, enum.Enum):
    """Layer 1-4: Instrument type classification."""

    SPOT = "spot"
    PERPETUAL = "perpetual"
    FUTURE = "future"
    CALL = "call"
    PUT = "put"
    STOCK = "stock"
    PAIR = "pair"


class MarketType(str, enum.Enum):
    """
    Market type classification that maps to exchange market structures.
    Used for routing and configuration.
    """

    SPOT = "spot"
    LINEAR_PERPETUAL = "linear_perpetual"  # USD-margined perpetuals
    INVERSE_PERPETUAL = "inverse_perpetual"  # Coin-margined perpetuals
    LINEAR_FUTURE = "linear_future"  # Dated USD-margined futures
    INVERSE_FUTURE = "inverse_future"  # Dated coin-margined futures
    OPTION = "option"


class OrderType(str, enum.Enum):
    """Layer 1-4: Order type - fundamental market structure."""

    LIMIT = "limit"
    MARKET = "market"
    STOP_LOSS = "stop_loss"
    STOP_LOSS_LIMIT = "stop_loss_limit"
    TAKE_PROFIT = "take_profit"
    TAKE_PROFIT_LIMIT = "take_profit_limit"


class MarketDataType(str, enum.Enum):
    """Types of market data streams."""

    OHLC = "ohlc"
    OHLCV = "ohlcv"
    OPEN_INTEREST = "open_interest"
    FUNDING_RATE = "funding_rate"
    LIQUIDATION = "liquidation"
    ORDERBOOK = "orderbook"
    TRADES = "trades"
    TICKER = "ticker"


# ============================================================================
# DATA LINEAGE: WHERE (Venue) + HOW (Wrapper)
# ============================================================================
class DataVenue(str, enum.Enum):
    """The actual source where data originates.

    This represents the legal entity/trading venue that generates the data.
    Used to track data provenance and lineage.

    Examples:
    - BINANCE_USDM: Binance USD-M Futures exchange
    - ICE_DATA: ICE Data Services
    - NYSE: New York Stock Exchange
    """

    # ========== CRYPTO EXCHANGES ==========
    # Binance (separate venues for different market types)
    BINANCE = "binance"  # Spot
    BINANCE_USDM = "binance_usdm"  # USD-M Futures (linear perpetual/futures)
    BINANCE_COINM = "binance_coinm"  # COIN-M Futures (inverse perpetual/futures)

    # Other major crypto exchanges
    BYBIT = "bybit"
    GATEIO = "gateio"
    HUOBI = "huobi"
    BITGET = "bitget"
    OKX = "okx"
    KRAKEN = "kraken"
    COINBASE = "coinbase"
    DERIBIT = "deribit"

    # ========== TRADITIONAL EXCHANGES ==========
    NYSE = "nyse"
    NASDAQ = "nasdaq"
    LSE = "lse"  # London Stock Exchange

    # ========== FUTURES & DERIVATIVES EXCHANGES ==========
    CME = "cme"  # Chicago Mercantile Exchange
    CBOE = "cboe"  # Chicago Board Options Exchange
    ICE_FUTURES = "ice_futures"  # ICE Futures (as trading venue)
    EUREX = "eurex"

    # ========== DATA VENDORS (Terminal/Feed Providers) ==========
    BLOOMBERG = "bloomberg"  # Bloomberg Terminal
    REFINITIV = "refinitiv"  # Refinitiv Eikon/Workspace
    ICE_DATA = "ice_data"  # ICE Data Services (as data provider)
    FACTSET = "factset"

    # ========== FOREX VENUES ==========
    OANDA = "oanda"
    FXCM = "fxcm"

    # ========== SPECIAL CASES ==========
    AGGREGATED = "aggregated"  # When wrapper combines multiple venues
    INTERNAL = "internal"  # Internal data generation
    FILE = "file"  # File-based data
    UNKNOWN = "unknown"  # Unknown source


class WrapperImplementation(str, enum.Enum):
    """How we access the data (wrapper/client type).

    This identifies the specific library/API/service used to fetch data.
    Separates the access method from the data source.

    Examples:
    - CCXT: Accessing Binance via CCXT library
    - COINALYZE: Accessing multiple exchanges via CoinAlyze API
    - YAHOO_FINANCE: Accessing various venues via Yahoo Finance
    - NONE: Direct native API access
    """

    # ========== CRYPTO WRAPPERS ==========
    CCXT = "ccxt"  # CCXT library (multi-exchange)
    COINALYZE = "coinalyze"  # CoinAlyze API (multi-exchange aggregator)

    # ========== MULTI-ASSET WRAPPERS ==========
    YAHOO_FINANCE = "yahoo_finance"  # Yahoo Finance (stocks, crypto, forex, futures)
    ALPHA_VANTAGE = "alpha_vantage"  # Alpha Vantage API
    POLYGON = "polygon"  # Polygon.io (stocks, forex, crypto)
    TWELVE_DATA = "twelve_data"  # Twelve Data API
    FINNHUB = "finnhub"  # Finnhub API

    # ========== BLOCKCHAIN DATA WRAPPERS ==========
    ETHERSCAN = "etherscan"  # Etherscan API
    ALCHEMY = "alchemy"  # Alchemy API
    INFURA = "infura"  # Infura API
    THEGRAPH = "thegraph"  # The Graph Protocol

    # ========== TRADITIONAL FINANCE WRAPPERS ==========
    IB_INSYNC = "ib_insync"  # Interactive Brokers wrapper
    ALPACA = "alpaca"  # Alpaca trading API
    ROBINHOOD = "robinhood"  # Robinhood API

    # ========== VENDOR SDKs ==========
    BLOOMBERG_SDK = "bloomberg_sdk"  # Bloomberg Terminal API
    REFINITIV_SDK = "refinitiv_sdk"  # Refinitiv SDK

    # ========== NATIVE (No Wrapper) ==========
    NONE = "none"  # Direct API access, no wrapper


class ClientType(str, enum.Enum):
    """Type of client implementation architecture.

    Describes the technical implementation pattern.
    """

    WRAPPER = "wrapper"  # Third-party wrapper library
    NATIVE = "native"  # Direct native API integration
    VENDOR_SDK = "vendor_sdk"  # Official vendor SDK
    FILE_PARSER = "file_parser"  # File-based ingestion


class ConnectionType(str, enum.Enum):
    """Network connection type used by adapter."""

    REST = "rest"
    WEBSOCKET = "websocket"
    FIX = "fix"  # Financial Information eXchange protocol
    GRPC = "grpc"
    SFTP = "sftp"
    FILE = "file"  # Local file system
    DATABASE = "database"  # Direct database connection


# ============================================================================
# PIPELINE & INGESTION
# ============================================================================
class PipelineMode(str, enum.Enum):
    """Execution mode for data ingestion pipeline."""

    BACKFILL = "backfill"
    INCREMENTAL = "incremental"
    STREAMING = "streaming"


class IngestionStatus(str, enum.Enum):
    """Status of data ingestion operation."""

    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    PARTIAL = "partial"  # Some data retrieved, some failed
    SKIPPED = "skipped"


# ============================================================================
# BACKWARD COMPATIBILITY ALIASES (Deprecated)
# ============================================================================
# Note: DataProvider and Exchange are deprecated in favor of DataVenue + WrapperImplementation
# Keep for migration period only


class Exchange(str, enum.Enum):
    """Trading venue identifiers (DEPRECATED - use DataVenue instead)."""

    BINANCE = "binance"
    BINANCEUSDM = "binance_usdm"
    BINANCECOINM = "binance_coinm"
    BYBIT = "bybit"
    GATEIO = "gateio"
    OKX = "okx"
    DERIBIT = "deribit"
    HUOBI = "huobi"
    BITGET = "bitget"

    @classmethod
    def _missing_(cls, value):
        if isinstance(value, str):
            value_lower = value.lower()
            for member in cls:
                if member.value == value_lower:
                    return member
        return None


# Alias for migration - maps old DataProvider to new DataVenue
DataProvider = DataVenue  # type: ignore
</file>

<file path="shared/models/instruments.py">
# quant_framework/shared/models/instruments.py

from datetime import datetime
from decimal import Decimal
from typing import Any

from pydantic import BaseModel, Field, root_validator, validator

from quant_framework.shared.models.enums import (
    AssetClass,
    DataVenue,
    MarketType,
    WrapperImplementation,
)


class Instrument(BaseModel):
    """
    Universal instrument model supporting all asset classes.

    Pydantic version provides robust validation and automatic field calculation.
    """

    # ========== CORE IDENTIFIERS (Required) ==========
    instrument_id: str = Field(..., description="Unique identifier")
    asset_class: AssetClass
    market_type: MarketType
    venue: DataVenue

    # ========== ASSET COMPOSITION (Required) ==========
    base_asset: str
    quote_asset: str

    # ========== DATA LINEAGE (Optional) ==========
    wrapper: WrapperImplementation = Field(
        default=WrapperImplementation.NONE,
        description="How we access the data (wrapper)",
    )

    # ========== CONTRACT SPECIFICATIONS (Optional) ==========
    contract_type: str | None = Field(default=None)
    contract_size: Decimal | None = Field(default=None)
    tick_size: Decimal | None = Field(default=None)
    lot_size: Decimal | None = Field(default=None)

    # ========== LIFECYCLE (Optional) ==========
    listing_date: datetime | None = Field(default=None)
    expiry_date: datetime | None = Field(default=None)
    is_active: bool = Field(default=True)

    # ========== SPECIAL FLAGS (Optional) ==========
    is_inverse: bool = Field(default=False)

    # ========== VENUE/WRAPPER MAPPING (Optional) ==========
    raw_symbol: str = Field(default="")
    metadata: dict[str, Any] | None = Field(default_factory=dict)

    # ========== CALCULATED FIELD (Not in __init__) ==========
    settlement_currency: str | None = Field(default=None)

    # ==================== VALIDATORS ====================

    @validator("raw_symbol", pre=True, always=True)
    def set_raw_symbol(cls, v, values):
        """Set raw_symbol to instrument_id if not provided"""
        if not v:
            return values.get("instrument_id", "")
        return v

    @validator("metadata", pre=True, always=True)
    def set_metadata(cls, v):
        """Initialize metadata as empty dict if None"""
        return v or {}

    @root_validator(skip_on_failure=True)
    def calculate_settlement_currency(cls, values):
        """Auto-calculate settlement currency based on contract type"""
        settlement = values.get("settlement_currency")

        # Only calculate if settlement_currency is None
        if settlement is None:
            is_inverse = values.get("is_inverse", False)
            base_asset = values.get("base_asset")
            quote_asset = values.get("quote_asset")

            if is_inverse:
                values["settlement_currency"] = base_asset
            else:
                values["settlement_currency"] = quote_asset

        return values

    # ==================== PROPERTIES ====================

    @property
    def symbol(self) -> str:
        """Alias for instrument_id for backward compatibility"""
        return self.instrument_id

    @property
    def full_symbol(self) -> str:
        """Full symbol with venue prefix (e.g., 'binance_usdm:BTCUSDT')"""
        return f"{self.venue.value}:{self.instrument_id}"

    @property
    def data_lineage(self) -> str:
        """Human-readable data lineage string"""
        if self.wrapper == WrapperImplementation.NONE:
            return f"{self.venue.value} (native)"
        return f"{self.venue.value} via {self.wrapper.value}"

    @property
    def is_derivative(self) -> bool:
        """Check if instrument is a derivative"""
        return self.market_type in [
            MarketType.LINEAR_PERPETUAL,
            MarketType.INVERSE_PERPETUAL,
            MarketType.LINEAR_FUTURE,
            MarketType.INVERSE_FUTURE,
            MarketType.OPTION,
        ]

    @property
    def is_perpetual(self) -> bool:
        """Check if instrument is a perpetual swap"""
        return self.market_type in [
            MarketType.LINEAR_PERPETUAL,
            MarketType.INVERSE_PERPETUAL,
        ]

    # ==================== SERIALIZATION ====================

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary (Pydantic does this natively)"""
        return self.dict(by_alias=True)

    def to_json(self) -> str:
        """Convert to JSON string"""
        return self.json(by_alias=True, indent=2)

    class Config:
        """Pydantic configuration"""

        validate_assignment = True  # Re-validate on attribute change
        arbitrary_types_allowed = True  # Allow Decimal, datetime


# ==================== TESTING ====================
</file>

<file path="shared/__init__.py">
"""
Shared infrastructure modules.
"""
</file>

<file path="__init__.py">
"""
Quantitative framework for systematic trading.
Modular architecture with clean separation of concerns.

Modules:
- ingestion: Market data collection and normalization
- shared: Common models, enums, utilities
- infrastructure: Config, database, logging
"""
</file>

</files>
